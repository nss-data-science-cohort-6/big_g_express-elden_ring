{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1931c612",
   "metadata": {},
   "source": [
    "# Creating a machine learning model from the data prep in BigG_AC \n",
    "\n",
    "**Two metholodogies will be implemented in the following models, one taking in consideration the model per truck, or one where the model takes into consideration the timestamps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e7eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1735e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diagnostics_5246 = pd.read_csv('data/Diagnostics_5246.csv')\n",
    "Diagnostics_1569 =  pd.read_csv('data/Diagnostics_1569.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1f1ed",
   "metadata": {},
   "source": [
    "- Dropping the columns that will not be considered in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebfaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diagnostics_5246 = Diagnostics_5246.drop(columns = ['RecordID', 'EquipmentID','Unnamed: 0', 'ecuSoftwareVersion', 'ecuSerialNumber', 'ecuModel', 'ecuMake',\n",
    " #      'ecuSource', 'eventDescription', 'EventTimeStamp_DateOnly', 'LocationTimeStamp', 'LocationTimeStamp_DateOnly''active', 'FaultId','spn', 'fmi'])\n",
    "#Diagnostics_1569 = Diagnostics_1569.drop(columns = ['Unnamed: 0','ecuSoftwareVersion', 'ecuSerialNumber', 'ecuModel', 'ecuMake',\n",
    "  #     'ecuSource'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872317fb",
   "metadata": {},
   "source": [
    "Splitting the trucks between derate and non derate. For Diagnostics_5246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2aace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trucks = Diagnostics_5246['EquipmentID'].unique()\n",
    "derate_trucks = Diagnostics_5246.loc[Diagnostics_5246['spn'] == 5246]['EquipmentID'].unique()\n",
    "no_derate_trucks = all_trucks[np.isin(all_trucks, derate_trucks, invert=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2add3",
   "metadata": {},
   "source": [
    "- put the two lists together (marking 1 for trucks with derate, 0 with non):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dcbc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks_df = pd.concat([\n",
    "            pd.DataFrame({'EquipmentID': derate_trucks, 'derate': 1}),\n",
    "            pd.DataFrame({'EquipmentID': no_derate_trucks, 'derate': 0}) \n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591b68e",
   "metadata": {},
   "source": [
    "Using trest_train_split and “stratify” to ensure the proportions of derate/non-derate stay same in the samples:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493437f",
   "metadata": {},
   "source": [
    "- And then you’d use something like below to extract and train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5eb175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split_data_custom(Diagnostics_5246, spn, percentage):\n",
    "    all_trucks = Diagnostics_5246['EquipmentID'].unique()\n",
    "    derate_trucks = Diagnostics_5246.loc[Diagnostics_5246['spn'] == spn]['EquipmentID'].unique()\n",
    "    no_derate_trucks = all_trucks[np.isin(all_trucks, derate_trucks, invert=True)]\n",
    "\n",
    "\n",
    "    #shuffle(sklearn) the array so that we get ramdom sequence in sample split\n",
    "    all_Equip_s = shuffle(all_trucks, random_state=42)\n",
    "    derate_Equip_s = shuffle(derate_trucks, random_state=42)\n",
    "    no_derate_Equip_s = shuffle(no_derate_trucks, random_state=42)\n",
    "\n",
    "\n",
    "    print(len(all_Equip_s))\n",
    "    print(len(derate_Equip_s))\n",
    "    print(len(no_derate_Equip_s))\n",
    "\n",
    "    #convert to dataframe to locate rows\n",
    "    df_all_Equip = pd.DataFrame(all_Equip_s, columns = ['EquipmentID'])\n",
    "    df_derate_Equip = pd.DataFrame(derate_Equip_s, columns = ['EquipmentID'])\n",
    "    df_no_derate_Equip = pd.DataFrame(no_derate_Equip_s, columns = ['EquipmentID'])\n",
    "\n",
    "\n",
    "    #get the equipments % based on the passed percentage\n",
    "    #keep_rows = int(total_rows * percentage)\n",
    "    keep_rows_nde = int(len(no_derate_Equip_s) * percentage)\n",
    "    print(keep_rows_nde)\n",
    "\n",
    "    #============================================\n",
    "    #STEP- 4 Get X1 for no_derate_Equip\n",
    "    #============================================\n",
    "    df_nde_pct_X1_train = df_no_derate_Equip.iloc[:keep_rows_nde, :]\n",
    "    print(df_nde_pct_X1_train)\n",
    "\n",
    "    #get rest of the percentage by doing not isin  lookup\n",
    "    df_nde_pct_X1_test = df_no_derate_Equip[np.isin(df_no_derate_Equip,df_nde_pct_X1_train, invert=True)]\n",
    "    print(df_nde_pct_X1_test) \n",
    "\n",
    "    #Make sure data is correct by checking we dont have equip id in both dataframes\n",
    "    should_be_zero_nde= df_nde_pct_X1_train[np.isin(df_nde_pct_X1_train,df_nde_pct_X1_test)]\n",
    "    print(\"should_be_zero nde = \"  + str(should_be_zero_nde.size)) \n",
    "\n",
    "    #============================================\n",
    "    #STEP 5 get X2 or train and Test\n",
    "    #============================================\n",
    "    keep_rows_de = int(len(derate_Equip_s) * percentage)\n",
    "    print(keep_rows_de)\n",
    "\n",
    "    df_de_pct_X2_train = df_derate_Equip.iloc[:keep_rows_de, :]\n",
    "    print(df_de_pct_X2_train)\n",
    "\n",
    "    #get rest of the percentage by doing not isin  lookup\n",
    "    df_de_pct_X2_test = df_derate_Equip[np.isin(df_derate_Equip,df_de_pct_X2_train, invert=True)]\n",
    "    print(df_de_pct_X2_test) \n",
    "\n",
    "    #Make sure data is correct by checking we dont have equip id in both dataframes\n",
    "    should_be_zero_de= df_de_pct_X2_train[np.isin(df_de_pct_X2_train,df_de_pct_X2_test)]\n",
    "    print(\"should_be_zero derate = \"  + str(should_be_zero_de.size)) \n",
    "\n",
    "\n",
    "    # now we have the id ready for trian and test from both derate and no derate data frames. \n",
    "    # combine df_nde_pct_X1_train & df_de_pct_X2_train, combine df_nde_pct_X1_test and df_de_pct_X2_test\n",
    "    combined_df_X_train = pd.concat([df_nde_pct_X1_train, df_de_pct_X2_train], ignore_index=True)\n",
    "    print(combined_df_X_train)\n",
    "\n",
    "\n",
    "    # combine df_nde_pct_X1_test & df_de_pct_X2_test, combine df_de_pct_X2_test and df_de_pct_X2_test\n",
    "    combined_df_X_test = pd.concat([df_nde_pct_X1_test, df_de_pct_X2_test], ignore_index=True)\n",
    "    print(combined_df_X_test)\n",
    "\n",
    "\n",
    "\n",
    "    #STEP 6 - Get X_train, y_train and X_test. y_test \n",
    "    # Next filter the data from original dataframe for X, y\n",
    "\n",
    "    df_train = Diagnostics_5246[Diagnostics_5246['EquipmentID'].isin(combined_df_X_train['EquipmentID'].tolist())]\n",
    "    X_train= df_train.drop(columns=['target'])\n",
    "    y_train = df_train['target']\n",
    "\n",
    "    df_test = Diagnostics_5246[Diagnostics_5246['EquipmentID'].isin(combined_df_X_test['EquipmentID'].tolist())]\n",
    "    X_test =df_test.drop(columns=['target'])\n",
    "    y_test = df_test['target']\n",
    "\n",
    "    # x_train, x_test, y_train, y_test\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fa119",
   "metadata": {},
   "source": [
    "- Spliting the data to train and test to try to get a classifier model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf94be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diagnostics_5246['spn_fmi'] = ['_'.join(i) for i in zip(Diagnostics_5246['spn'].astype(str), Diagnostics_5246['fmi'].astype(str))]\n",
    "\n",
    "Diagnostics_5246 = pd.get_dummies(Diagnostics_5246, columns=['spn_fmi'], prefix='spn_fmi')\n",
    "\n",
    "#Diagnostics_5246 = Diagnostics_5246.sort_values(by=['EquipmentID', 'EventTimeStamp'])\n",
    "\n",
    "# to obtain the one hot encoded columns since there are so many\n",
    "#faults_cols = ['EventTimeStamp'] + [col for col in Diagnostics_5246.columns if 'spn_fmi' in col] \n",
    "\n",
    "##diagnostics_cols = ['EventTimeStamp', 'activeTransitionCount', 'AcceleratorPedal',\n",
    "#         'BarometricPressure', 'CruiseControlSetSpeed', 'DistanceLtd',\n",
    "#         'EngineCoolantTemperature', 'EngineLoad', 'EngineOilPressure', \n",
    "#        'EngineOilTemperature', 'EngineRpm', 'EngineTimeLtd', 'FuelLevel', 'FuelLtd', \n",
    "#        'FuelRate', 'FuelTemperature', 'IntakeManifoldTemperature', 'LampStatus',\n",
    "#        'Speed', 'SwitchedBatteryVoltage', 'Throttle', 'TurboBoostPressure', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f279807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Unnamed: 0','RecordID', 'EquipmentID','EventTimeStamp', \n",
    "                'EventTimeStamp_DateOnly',  'LocationTimeStamp_DateOnly', 'LocationTimeStamp',\n",
    "               'active', 'FaultId','spn', 'fmi', 'eventDescription',\n",
    "       'eventDescription', 'ecuSoftwareVersion', 'ecuSerialNumber', 'ecuModel',\n",
    "       'ecuMake', 'ecuSource']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5128bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faults_1 = (\n",
    "#    Diagnostics_5246\n",
    "#    .groupby('EquipmentID')[faults_cols]\n",
    " #   .max()\n",
    "  #  )\n",
    "\n",
    "#faults_2 = (\n",
    "#     Diagnostics_5246\n",
    "#    .groupby('EquipmentID')[diagnostics_cols]\n",
    "#    .mean()\n",
    "#    )\n",
    "\n",
    "#faults_1 = faults_1.reset_index()\n",
    "#faults_2 = faults_2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd16b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faults_merged = pd.merge(Diagnostics_5246['RecordID'], #[['RecordID'] + diagnostics_cols]\n",
    " #                         faults_1,\n",
    "  #                        left_index= True,\n",
    "   #                       right_on = 'EquipmentID').drop(columns='EquipmentID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd508d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faults_merged_2 = pd.merge(Diagnostics_5246['RecordID'], #[['RecordID'] + diagnostics_cols]\n",
    " #                         faults_2,\n",
    "  #                        left_index= True,\n",
    "   #                       right_on = 'EquipmentID').drop(columns='EquipmentID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9850aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faults_diagnostics =  faults_merged.merge(faults_merged_2, on = 'RecordID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80d70525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#faults_diagnostics = Diagnostics_5246.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13bf58c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039\n",
      "491\n",
      "548\n",
      "438\n",
      "     EquipmentID\n",
      "0           2093\n",
      "1           1695\n",
      "2           2052\n",
      "3           2188\n",
      "4           2302\n",
      "..           ...\n",
      "433         2327\n",
      "434         1770\n",
      "435         1760\n",
      "436         1672\n",
      "437         1767\n",
      "\n",
      "[438 rows x 1 columns]\n",
      "     EquipmentID\n",
      "438         2371\n",
      "439         1674\n",
      "440         2169\n",
      "441         2046\n",
      "442         1412\n",
      "..           ...\n",
      "543         1336\n",
      "544         1518\n",
      "545         2135\n",
      "546         2248\n",
      "547         1333\n",
      "\n",
      "[110 rows x 1 columns]\n",
      "should_be_zero nde = 0\n",
      "392\n",
      "     EquipmentID\n",
      "0           1944\n",
      "1           1517\n",
      "2           1921\n",
      "3           1949\n",
      "4           1763\n",
      "..           ...\n",
      "387         1888\n",
      "388         1718\n",
      "389         1744\n",
      "390         1971\n",
      "391         1561\n",
      "\n",
      "[392 rows x 1 columns]\n",
      "     EquipmentID\n",
      "392         1887\n",
      "393         1557\n",
      "394         1812\n",
      "395         1758\n",
      "396         1883\n",
      "..           ...\n",
      "486         1450\n",
      "487         1615\n",
      "488         1593\n",
      "489         1891\n",
      "490         1815\n",
      "\n",
      "[99 rows x 1 columns]\n",
      "should_be_zero derate = 0\n",
      "     EquipmentID\n",
      "0           2093\n",
      "1           1695\n",
      "2           2052\n",
      "3           2188\n",
      "4           2302\n",
      "..           ...\n",
      "825         1888\n",
      "826         1718\n",
      "827         1744\n",
      "828         1971\n",
      "829         1561\n",
      "\n",
      "[830 rows x 1 columns]\n",
      "     EquipmentID\n",
      "0           2371\n",
      "1           1674\n",
      "2           2169\n",
      "3           2046\n",
      "4           1412\n",
      "..           ...\n",
      "204         1450\n",
      "205         1615\n",
      "206         1593\n",
      "207         1891\n",
      "208         1815\n",
      "\n",
      "[209 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = test_split_data_custom(Diagnostics_5246, 1569, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26484941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    429488\n",
       "1    429488\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversampler = SMOTE(k_neighbors=5, random_state=42)\n",
    "X_smote, y_smote = oversampler.fit_resample(X_train.drop(columns=cols_to_drop), y_train.drop(columns=cols_to_drop))\n",
    "y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0dc8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the model\n",
    "gbm = GradientBoostingClassifier(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a07e7be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2015-02-21 11:35:33'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model to the training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:429\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_state()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Check input\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# Since check_array converts both X and y to the same dtype, but the\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# trees use different types for X and y, checking them separately.\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    431\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m sample_weight_is_none \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    435\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:810\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[0;32m--> 810\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[1;32m    812\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:6240\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6233\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   6235\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m   6236\u001b[0m     ]\n\u001b[1;32m   6238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6239\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6240\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6243\u001b[0m \u001b[38;5;66;03m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:448\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastype\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, dtype, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py:526\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 526\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    529\u001b[0m newb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/astype.py:299\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/astype.py:230\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    227\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/astype.py:170\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(arr\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(dtype):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2015-02-21 11:35:33'"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test data\n",
    "y_pred = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a decision tree classifier on the training set\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c248a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(gbm.feature_importances_, index=X_test.columns)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f82755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0078b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the testing set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance of the classifier using accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = classifier.feature_importances_\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852998ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = pd.Series(clf.feature_importances_, index=X_test.columns)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74baaf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_sorted = importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "importances_sorted.plot(kind='barh')\n",
    "plt.title('Feature importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "text_representation = tree.export_text(classifier)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee97578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='.0f')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c93f406",
   "metadata": {},
   "source": [
    "- Tring to improve the model with different classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00619668",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    steps = [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('nn', MLPClassifier(hidden_layer_sizes = (2,),\n",
    "                             activation = 'tanh',\n",
    "                             max_iter = 10000))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5637d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_2 = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#oversampler = SMOTE(k_neighbors=5, random_state=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda52805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_smote, y_smote = oversampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f4316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c442e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = lr.coef_\n",
    "\n",
    "feature_names = X_test.columns\n",
    "\n",
    "coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients[0]})\n",
    "coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6dfd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the confusion matrix\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='.0f')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5479657",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
