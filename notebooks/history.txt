 1/1:
import pandas as pd
import numpy as np
 1/2:
#this is the DS6: Data question1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
 1/3:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("/c/Users/upadh/Documents/NSS_Projects/un_data_question-smitamisra/data")
 1/4:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("/c/Users/upadh/Documents/NSS_Projects/un_data_question-smitamisra/data/gdp_per_capita.csv")
 1/5:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("C:\Users\upadh\Documents\NSS_Projects\un_data_question-smitamisra\data\gdp_per_capita.csv")
 1/6:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
 1/7:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
print(gdp_df.head(5))
 1/8:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
 1/9:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
1/10: gdp_df.tail(5)
1/11:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
1/12: gdp_df.info()
1/13: gdp_df.shape()
1/14: gdp_df.shape
1/15: gdp_df_nan = gdp_df.dropna(subset=['Country or Area'])
1/16: gdp_df_nan.shape
1/17: gdp_df_nan.tail(5)
1/18: gdp_df_nan = gdp_df.dropna(subset=['Country or Area', 'Value']) #(6870, 4)
1/19: gdp_df_nan.shape
1/20: gdp_df_nan.tail(5)
1/21:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_A = gdp_df_nan.drop["ValueFootnotes"]
1/22:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_A = gdp_df_nan.drop(columns = ["ValueFootnotes"])
1/23:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_A = gdp_df_nan.drop(columns = ["Value Footnotes"])
1/24:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_A = gdp_df_nan.drop(columns = ["Value Footnotes"])
print(gdp_df_A.tail(5))
1/25:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_A = gdp_df_nan.drop(columns = ["Value Footnotes"])
print(gdp_df_A.tail(5))
gdp_df_A.column
1/26:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_A = gdp_df_nan.drop(columns = ["Value Footnotes"])
print(gdp_df_A.tail(5))
gdp_df_A.column()
1/27:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_A = gdp_df_nan.drop(columns = ["Value Footnotes"])
print(gdp_df_A.tail(5))
gdp_df_A.columns()
1/28:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_A = gdp_df_nan.drop(columns = ["Value Footnotes"])
print(gdp_df_A.tail(5))
gdp_df_A.columns
1/29:
gdp_df_B = gdp_df_A.rename(columns = ['Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'])
print(gdp_df_B.head(5))
1/30:
gdp_df_B = gdp_df_A.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_B.head(5))
1/31:
gdp_df_B.dtypes
gdp_df_B.shape
1/32:
gdp_df_B.dtypes
#gdp_df_B.shape  #(6868, 3)
1/33: gdp_df_C = gdp_df_B.convert.dtypes()
1/34: gdp_df_C = gdp_df_B.convert_dtypes()
1/35: gdp_df_C.dtypes
1/36:
#this is the DS6: Data question1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
1/37:
grade = 80

if the "grade" >= 60
print (pass)
else print (fail)
1/38:
grade = 80

if the grade >= 60
print (pass)
else print (fail)
1/39:
grade = 80

if the grade >= 60:
    print (pass),
elif print (fail)
1/40:
grade = 80

if the grade >= 60:
    print (pass),
elif print (fail)
1/41:
grade = 80

if the grade >= 60:
    print ("pass"),
elif print ("fail")
1/42:
grade = 80

if the 'grade' >= 60:
    print ("pass"),
elif print ("fail")
1/43:
a = 200
b = 33
if b > a:
  print("b is greater than a")
elif a == b:
  print("a and b are equal")
else:
  print("a is greater than b")
1/44:
grade = 80

if the 'grade' >= 60:
    print ("pass"),
else:
    print ("fail")
1/45:
grade = 80

if grade >= 60:
    print ("pass"),
else:
    print ("fail")
1/46:
grade = 40

if grade >= 60:
    print ("pass"),
else:
    print ("fail")
1/47:
grade = 40

if grade >= 60:
    print ("pass")
elif grade < 60:
    print("fail")    
else:
    print ("fail")
1/48:
a = 74

if a >= 90:
  print("A")
if b >= 80:
  print("B")

else:
  print("fail")
1/49:
a = 80

if a >= 90:
  print("A")
if b >= 80:
  print("B")

else:
  print("fail")
1/50:
a = 

if a >= 90:
  print("A")
if b >= 80:
  print("B")

else:
  print("fail")
1/51:
a = 90

if a >= 90:
  print("A")
if b >= 80:
  print("B")

else:
  print("fail")
1/52:
a = 90

if a >= 90:
  print("A")
if b >= 80:
  print("B")
else:
  print("fail")
1/53:
a = 70

if a >= 90:
  print("A")
if b >= 80:
  print("B")
else:
  print("fail")
1/54:
a = 82

if a >= 90:
  print("A")
if b >= 80:
  print("B")
else:
  print("fail")
1/55:
a = 82

if a >= 90:
  print("A")
elif b >= 80:
  print("B")
else:
  print("fail")
1/56:
a = 82

if a >= 90:
  print("A")
elif b < 90 & >= 80:
  print("B")
else:
  print("fail")
1/57:
a = 82

if a >= 90:
  print("A")
if a >= 80:
  print("B")
if a >= 70:
  print("C")
if a >= 60:
  print("D")
else:
  print("fail")
1/58:
a = 82

if a >= 90:
  print("A")
elif a >= 80:
  print("B")
elif a >= 70:
  print("C")
elif a >= 60:
  print("D")
else:
  print("fail")
 2/1:
grade = 40

if grade >= 60:
    print ("pass")
elif grade < 60:
    print("fail")    
else:
    print ("fail")
 2/2:
a = 82

if a >= 90:
  print("A")
elif a >= 80:
  print("B")
elif a >= 70:
  print("C")
elif a >= 60:
  print("D")
else:
  print("fail")
 2/3:
a = 82

if a >= 90:
  print("A")
elif a >= 80:
  print("B")
elif a >= 70:
  print("C")
elif a >= 60:
  print("D")
else:
  print("fail")
 3/1:
#this is the DS6: Data question1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
 3/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)

# there are 4 coulmns Country or Area, Year, Value, Value Footnotes. Value Footnotes has NaN values
 3/3: gdp_df.tail(5)  #Yes there are NaN values in the country column
 3/4: gdp_df.info()
 3/5: gdp_df.shape
 3/6: gdp_df_nan = gdp_df.dropna(subset=['Country or Area', 'Value']) #(6870, 4) with just the country
 3/7: gdp_df_nan.shape
 3/8: gdp_df_nan.tail(5)
 3/9:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_A = gdp_df_nan.drop(columns = ["Value Footnotes"])
print(gdp_df_A.tail(5))
gdp_df_A.columns
3/10:
gdp_df_B = gdp_df_A.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_B.head(5))
3/11:
#How many rows and columns does gdp_df have? What are the data types of its columns? If any of the columns are not the expected types, figure out why and fix it.

gdp_df_B.dtypes
#gdp_df_B.shape  #(6868, 3)
3/12: gdp_df_C = gdp_df_B.convert_dtypes()
3/13: gdp_df_C.dtypes
 2/4:
x = [1, 2, 3]
x**2
 2/5:
x = [1, 2, 3] #is a list thus the mathematical operations don't work
#x**2

#use the array to do mathematical operation
y = ([1, 2, 3])
y**2
 2/6:
x = [1, 2, 3] #is a list thus the mathematical operations don't work
#x**2

#use the array to do mathematical operation
import numpy as np
y = np.array([1, 2, 3])
y**2
3/14:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.dtypes

# there are 4 coulmns Country or Area, Year, Value, Value Footnotes. Value Footnotes has NaN values
3/15: print(gdp_df_nan.tail(5))
3/16: gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
3/17:
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.dtypes
3/18: gdp_df_B["Year"] = gdp_df_B["Year"].astype(int)
3/19: gdp_df_B.dtypes
 4/1:
#this is the DS6: Data question1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
 4/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)

# there are 4 coulmns Country or Area, Year, Value, Value Footnotes. Value Footnotes has NaN values
 4/3: gdp_df.tail(5)  #Yes there are NaN values in the country column and Year. The world Footnote in the Year column changes the Year column type to object, rather than keeping it to integer
 4/4: gdp_df.info()
 4/5: gdp_df.shape
 4/6: gdp_df_nan = gdp_df.dropna(subset=['Country or Area', 'Value']) #(6870, 4) with just the country
 4/7: gdp_df_nan.shape
 4/8: gdp_df_nan.tail(5)
 4/9:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_A = gdp_df_nan.drop(columns = ["Value Footnotes"])
print(gdp_df_A.tail(5))
gdp_df_A.columns
4/10:
gdp_df_B = gdp_df_A.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_B.head(5))
4/11:
#How many rows and columns does gdp_df have? What are the data types of its columns? If any of the columns are not the expected types, figure out why and fix it.

gdp_df_B.dtypes
#gdp_df_B.shape  #(6868, 3)
4/12: gdp_df_B["Year"] = gdp_df_B["Year"].astype(int)
4/13: gdp_df_B.dtypes
4/14:
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.dtypes
4/15:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)

# there are 4 coulmns Country or Area, Year, Value, Value Footnotes. Value Footnotes has NaN values
4/16:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
4/17:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_nan.tail(5)
gdp_df_nan.info()
4/18:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
#gdp_df_A.columns
4/19:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
4/20:
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
#print(gdp_df_B.head(5))
4/21:
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
4/22:
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info()
4/23:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
4/24:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
4/25:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
4/26:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
4/27: gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
4/28:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
4/29:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
4/30:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
4/31: gdp_df_for_analysis.info()
4/32:
 How many rows and columns does gdp_df have? What are the data types of its columns? 
    If any of the columns are not the expected types, figure out why and fix it.
4/33: gdp_df_for_analysis['Year'].value_counts()
 5/1:
x = 1
while x <10:
    x = x*2
print(x)
 5/2:
x = 1
while x > 10:
    x = x*2
print(x)
 5/3:
x = 1
while x < 10:
    x = x*2
print(x)
 5/4:
x = 1
while x < 10:
    x = x*2
    print(x)
 5/5:
x = 1
while x < 10:
    x = x*2
    print(x)
 5/6:
x = 1
while x < 10:
    x = x*2
    print(x)
 5/7:
x = 1
while x > 10:
    x = x*2
    print(x)
 5/8:
x = 1
while x > 10:
    x = x*2
print(x)
 5/9:
result = []

for x in [1, 2, 3, 4,5]
    result.append(x**2)
print(result[3])
5/10:
result = []

for x in [1, 2, 3, 4, 5]
    result.append(x**2)
print(result[3])
5/11:
result = []

for x in [1, 2, 3, 4, 5]:
    result.append(x**2)
print(result[3])
5/12:
result = []

for x in [1, 2, 3, 4, 5]:
    result.append(x**2)
    print(result[3])
5/13:
result = []

for x in [1, 2, 3, 4, 5]:
    result.append(x**2)
print(result[3])
5/14:
for x in ['a', 'b', 'c']
    print(x)
5/15:
for x in ['a', 'b', 'c']:
    print(x)
5/16:
for x in enumerate (['a', 'b', 'c']):
    print(x)
5/17:
for i in x enumerate (['a', 'b', 'c']):
    print(x)
5/18:
for i x in enumerate (['a', 'b', 'c']):
    print(x)
5/19:
for i in x in enumerate (['a', 'b', 'c']):
    print(x)
5/20:
1 for i in x in enumerate (['a', 'b', 'c']):
    print(i)
    print(x)
5/21:
1 for i in x in enumerate (['a', 'b', 'c']):
    print(i)
    print(x)
5/22:
for i in x in enumerate (['a', 'b', 'c']):
    print(i)
    print(x)
5/23:
for i, x in enumerate (['a', 'b', 'c']):
    print(i)
    print(x)
 6/1: gdp_df_for_analysis['Year'].value_counts().plot()
 6/2: gdp_df_for_analysis['Year'].value_counts().plots()
 7/1:
#this is the DS6: Data question1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
 7/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)

# there are 4 coulmns Country or Area, Year, Value, Value Footnotes. Value Footnotes has NaN values
 7/3:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
 7/4:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_nan.tail(5)
gdp_df_nan.info()
 7/5:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_nan.tail(5)
gdp_df_nan.info()
 7/6:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
 7/7:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
 7/8:
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
 7/9:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
7/10:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
7/11:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
7/12: gdp_df_for_analysis.info()
7/13: gdp_df_for_analysis['Year'].value_counts().plots()
7/14: gdp_df_for_analysis['Year'].value_counts()
7/15: gdp_df_for_analysis['Year'].value_counts().plot()
7/16: gdp_df_for_analysis['Year'].value_counts().plot(kind = "bar')
7/17: gdp_df_for_analysis['Year'].value_counts().plot(kind = 'bar')
7/18:
gdp_df_for_analysis['Year'].value_counts()
#.plot(kind = 'bar')
7/19: The data ranges from Year 1990 to 2019 with lowest values for 1990
7/20: gdp_df_for_analysis['Country'].value_counts()
7/21:
gdp_df_for_analysis['Country'].value_counts()
gdp_df_for_analysis.head()
7/22:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis.head()
7/23:
gdp_df_for_analysis['Country'].value_counts().plot()
#gdp_df_for_analysis.head()
7/24:
gdp_df_for_analysis['Country'].value_counts().plot(kind='bar')
#gdp_df_for_analysis.head()
7/25:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis.head()
7/26: gdp_df_for_analysis('Country')['Year'].agg([min, max, sum])
7/27: gdp_df_for_analysis.groupby('Country')['Year'].agg([min, max, sum])
7/28: gdp_df_for_analysis.groupby('Country')['Year'].agg([min, max])
7/29: gdp_df_for_analysis.groupby('Country')['Year'].agg([min, max]).value_counts()
7/30: gdp_df_for_analysis.groupby('Country')['Year'].value_counts()
7/31: gdp_df_for_analysis.groupby('Year')['Country'].value_counts()
7/32:
gdp_df_for_analysis['Country'].value_counts()
gdp_df_for_analysis.head()
7/33: gdp_df_for_analysis['Year'].value_counts()
7/34: gdp_df_for_analysis['Year'].value_counts().sort_index()
7/35: gdp_df_for_analysis['Country'].value_counts()
7/36: gdp_df_for_analysis['Country'].value_counts().tail(20)
7/37: gdp_df_for_analysis.groupby('Country')['Year'].value_counts().tail(20)
7/38: gdp_df_for_analysis['Country'] == "Djibouti"
7/39: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
7/40:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis.head()
7/41: gdp_df_for_analysis['Year'].value_counts().sort_index()
7/42:
gdp_df_for_analysis['Year'].value_counts()
.sort_index()
7/43:
gdp_df_for_analysis['Year'].value_counts()
    .sort_index()
7/44: gdp_df_for_analysis['Year'].value_counts().sort_index()
7/45:
gdp_df_for_analysis['Year'].value_counts().sort_index(sort = True)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(sort = Trye)
#.plot(kind = 'bar')
7/46:
gdp_df_for_analysis['Year'].value_counts().sort_index(descendng)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(sort = Trye)
#.plot(kind = 'bar')
7/47:
gdp_df_for_analysis['Year'].value_counts().sort_index(desendng)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(sort = Trye)
#.plot(kind = 'bar')
7/48:
gdp_df_for_analysis['Year'].value_counts().sort_index(desending)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(sort = Trye)
#.plot(kind = 'bar')
7/49:
gdp_df_for_analysis['Year'].value_counts().sort_index(descending)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(sort = Trye)
#.plot(kind = 'bar')
7/50:
gdp_df_for_analysis['Year'].value_counts().sort_index()
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(sort = Trye)
#.plot(kind = 'bar')
7/51:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=Fase)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(sort = Trye)
#.plot(kind = 'bar')
7/52:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(sort = Trye)
#.plot(kind = 'bar')
7/53:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
7/54:
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
7/55:
gdp_df_for_analysis['Country'].value_counts()
gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
7/56:
gdp_df_for_analysis['Country'].value_counts()
gdp_df_for_analysis['Country'].value_counts().head(20)
#gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
7/57:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
7/58: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
7/59: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == "Djibouti1990"]
7/60: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == "1990"]
7/61: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990]
7/62: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts()
7/63: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "India"]
7/64: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
7/65: gdp_df_group_country = gdp_df_for_analysis.loc[['Country']]
7/66: gdp_df_group_country = gdp_df_for_analysis.groupby[['Country']]
7/67: gdp_df_group_country = gdp_df_for_analysis.groupby['Country']
7/68: gdp_df_group_country = gdp_df_for_analysis.groupby['Country']
7/69: gdp_df_group_country = gdp_df_for_analysis.groupby(['Country'])
7/70:
gdp_df_group_country = gdp_df_for_analysis.groupby(['Country'])
gdp_df_group_country.head(20)
7/71:
gdp_df_group_country = gdp_df_for_analysis.groupby('Country')
gdp_df_group_country.head(20)
7/72:
gdp_df_for_analysis.groupby('Country').value_counts()
gdp_df_group_country.head(20)
7/73: gdp_df_for_analysis.groupby('Country').value_counts()
7/74: type(gdp_df_for_analysis.groupby('Country').value_counts())
7/75:
#type(gdp_df_for_analysis.groupby('Country').value_counts())
gdp_df_for_analysis.groupby(['Country'])['Country'].count()
7/76:
#type(gdp_df_for_analysis.groupby('Country').value_counts())
gdp_df_for_analysis.groupby(['Country'])['Country'].count().tail(20)
7/77:
type(gdp_df_for_analysis[gdp_df_for_analysis.groupby('Country').value_counts()])
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
7/78:
gdp_df_for_analysis.groupby('Country').value_counts()
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
7/79:
gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
7/80:
gdp_groupby_Country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
7/81:
gdp_groupby_Country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_df_group_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
7/82:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_group_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
7/83:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
7/84: gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis.loc['Year'] == 2014]
7/85: gdp_df_for_analysis.loc[gdp_df_for_analysis.loc['Year'] == 2014]
7/86: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
7/87: gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
7/88:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014.head(20)
7/89: gdp_2014.describe()
7/90: gdp_2014['GDP_Per_Capita'].describe()
7/91: gdp_2014.sort_values('GDP_Per_Capita')
7/92: gdp_2014.sort_values('GDP_Per_Capita').head(5)
7/93: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
7/94:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014.set_index('Year')
gdp_2014.head(20)
7/95:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014.set_index('Year')
#gdp_2014.head(20)
7/96:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014.set_index('Year')
gdp_2014.head(20)
7/97:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
7/98: gdp_2014.sort_values('GDP_Per_Capita').head(5)
7/99: gdp_2014['GDP_Per_Capita'].describe()
7/100: plt.his(gdp_2014['GDP_Per_Capita'])
7/101: plt.hist(gdp_2014['GDP_Per_Capita'])
7/102:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], height, edgecolor="red", bins=20)
7/103:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color=green, edgecolor="red", bins=20)
7/104:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=20)
7/105:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
7/106:
table = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year'])
7/107: table.head()
7/108:
table = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=(['Year'] == 1990, 2017))
7/109:
table = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=(gdp_df_for_analysis.loc['Year'] == 1990, 2017))
7/110:
table = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=(gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990, 2017]))
7/111:
table = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=[gdp_df_for_analysis['Year'] == 1990, 2017])
7/112:
table = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=[['Year'] == 1990, 2017])
7/113:
table = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year'])
7/114: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year']==1990, 2017]
7/115: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
7/116:
table = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])])
7/117: table.head()
7/118:
table = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year'])
7/119: table.head()
7/120: gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
7/121:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.head()
7/122:
table_X = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year'])
7/123: tableX.head()
7/124: table_X.head()
7/125:
table = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year'])
7/126: table.head()
7/127:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year'])
7/128: gdp_pivoted.head()
7/129: gdp_pivoted.describe().shape
7/130: gdp_pivoted.describe()
7/131:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.head().shape
7/132:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.head()
7/133: table_X.shape
7/134:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.head()
gdp_1990_2017.shape
7/135:
gdp_pivoted.head()
gdp_pivoted.shape
7/136: gdp_pivoted.info()
7/137: gdp_pivoted.head()
7/138: gdp_pivoted.shape
7/139: gdp_pivoted_dropnan = gdp_pivoted.dropna()
7/140:
gdp_pivoted_dropnan = gdp_pivoted.dropna()
gdp_pivoted_dropnan.head()
7/141:
gdp_pivoted_dropnan = gdp_pivoted.dropna()
gdp_pivoted_dropnan.head().shape
7/142:
gdp_pivoted_dropnan = gdp_pivoted.dropna()
gdp_pivoted_dropnan.shape
7/143:
table_X = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()
7/144: table_X.shape
7/145:
table_X = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year'])
7/146: table_X.shape
7/147:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.head()
gdp_1990_2017.shape
7/148:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.head()
#gdp_1990_2017.shape
7/149:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()
7/150: gdp_pivoted.head()
7/151: gdp_pivoted.shape
7/152: gdp_pivoted.describe()
7/153:
#table_X = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year'])
#table_X.shape
7/154:
#table_X = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    #columns=['Year'])
#table_X.shape
7/155:
#gdp_pivoted_dropnan = gdp_pivoted.dropna()
#gdp_pivoted_dropnan.shape
7/156: 100 * (gdp_pivoted['2017'] - gdp_pivoted['1990'])/ gdp_pivoted['1990']
7/157:
gdp_pivoted.shape
gdp_pivoted.columns()
7/158:
gdp_pivoted.shape
gdp_pivoted.column()
7/159:
gdp_pivoted.shape
gdp_pivoted.columns
7/160:
gdp_pivoted.shape
gdp_pivoted.columns
type(gdp_pivoted)
7/161:
gdp_pivoted.shape
gdp_pivoted.columns
7/162: 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
7/163: gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
7/164:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
7/165:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head(50)
7/166: gdp_pivoted.info()
7/167: gdp_pivoted.describe()
7/168: gdp_pivoted['Percent_Change'].value_counts()
7/169: gdp_pivoted['Percent_Change'].value_counts(sort=True)
7/170: gdp_pivoted['Percent_Change'].value_counts(sort=False)
7/171: gdp_pivoted['Percent_Change'].value_counts()
7/172: gdp_pivoted.sort_values("Percent_Change")
7/173: gdp_pivoted.sort_values("Percent_Change").head(50)
7/174: gdp_pivoted.sort_values("Percent_Change").tail(50)
7/175: sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
7/176:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count
sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
7/177: gdp_pivoted.sort_values("Percent_Change").head(10)
7/178: gdp_pivoted.sort_values("Percent_Change").tail(10)
7/179:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
7/180:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Contry'] == "Equatorial Guinea"].plot(x=gdp_df_for_analysis["Year"], y=gdp_df_for_analysis['GDP_Per_Capita'])
7/181:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(x=gdp_df_for_analysis["Year"], y=gdp_df_for_analysis['GDP_Per_Capita'])
7/182:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(x = gdp_df_for_analysis['Year'], y = gdp_df_for_analysis['GDP_Per_Capita'])
7/183:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot()
7/184:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')
7/185: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin["Equatorial Guinea", "China"].plot(y='GDP_Per_Capita')
7/186: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"]).plot(y='GDP_Per_Capita')
7/187: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"]).plot()
7/188:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plt.plot(gdp_df_for_analysis['Year'], gdp_df_for_analysis['GDP_Per_Capita'])

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/189:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/190:
#using plt.plot function
#finding row number for the 
gdp_pivoted.index[gdp_pivotedivoted['Country']=="Equatorial Guinea"].tolist()
7/191:
#using plt.plot function
#finding row number for the 
gdp_pivoted.index[gdp_pivoted['Country']=="Equatorial Guinea"].tolist()
7/192:
#using plt.plot function
#finding row number for the 
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
7/193:
#using plt.plot function
#finding row number for the 
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
gdp_df_for_analysis.iloc[1754:1783].plot(y='GDP_Per_Capita')
7/194:
#using plt.plot function
#finding row number for the 
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')
7/195:
#using plt.plot function
#finding row number for the 
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita')

#here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/196:
#using plt.plot function
#finding row number for the 
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita')


#here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/197: gdp_df_for_analysis.index[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"]).tolist()
7/198: gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=["Equatorial Guinea", "China"].tolist()
7/199: gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=["China"].tolist()
7/200: gdp_df_for_analysis.index[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"]).tolist()
7/201: gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=='China"]).tolist()
7/202: gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=='China"].tolist()
7/203: gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=='China".tolist()
7/204: gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()
7/205:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
gdp_df_for_analysis.iloc[1754:1783 & 1162:1191].plot(x='Year', y='GDP_Per_Capita')

#gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita')


#here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/206:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/207:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
gdp_df_for_analysis.iloc[1754:1783 & 1162:1191].plot(x='Year', y='GDP_Per_Capita')

#gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita')


#here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/208:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

#gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita')


#here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/209:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita')
7/210:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[[1162:1191], [1754:1783]].plot(x='Year', y='GDP_Per_Capita')
7/211:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita')
5/24:
   import pandas as pd
   df = pd.read_csv("../data/qb")
5/25:
   import pandas as pd
   df = pd.read_csv("../data/qbs.csv")
5/26:
import pandas as pd
df = pd.read_csv("../data/qbs.csv")
5/27:
import pandas as pd
df = pd.read_csv("../qbs.csv")
5/28:
import pandas as pd
df = pd.read_csv("qbs.csv")
5/29: df.head
5/30: df.head()
5/31: df.loc[df['TD'] >= 4]
5/32:
import pandas as pd
qbs = pd.read_csv("qbs.csv")
5/33: qbs.head()
5/34: qbs.loc[df['TD'] >= 4]
5/35: qbs.loc[qbs['TD']>=4 ['Cmp', 'TD']]
5/36: qbs.loc[qbs['TD'] >=4 ['Cmp', 'TD']]
5/37: qbs.loc[qbs['TD'] >=4, ['Cmp', 'TD']]
5/38: qbs.loc[df['TD'] >= 4 & df['INT'] <3]
5/39: qbs.loc[df['INT'] <3]
5/40: qbs.loc[df['TD','INT'] >=4 & <3]
5/41: qbs.loc[df['TD','INT'] >=4 & <=3]
5/42: qbs.loc[[qbs['TD'] >=4 & qbs['INT']<=3]]
5/43: qbs.loc[(qbs['TD'] >=4 ) & (qbs['INT']<=30]
5/44: qbs.loc[(qbs['TD'] >=4 ) & (qbs['INT']<=30)]
5/45: qbs.loc[(qbs['TD'] >=4 ) & (qbs['INT']<=3)]
5/46: qbs.loc[(qbs['TD'] >=4 ) & (qbs['INT']<3)]
 9/1: import pandas as pd
 9/2: unemployment = pd.read_csv('data/tn_unemployment.csv')
 9/3: unemployment.head()
 9/4: population = pd.read_csv('data/tn_population.csv')
 9/5: population.head()
 9/6: population['County Name'].str.upper()
 9/7: unemployment['Period'].str.replace('21', '2021')
 9/8: unemployment['Name'].str.split(',')
 9/9: unemployment['Name'].str.split(',', expand = True)
9/10: unemployment['Name'].str.split(',', expand = True)[0]
9/11: unemployment['Name'] = unemployment['Name'].str.split(',', expand = True)[0]
9/12: unemployment.head()
9/13:
pd.merge(left = population, 
         right = unemployment, 
         left_on = 'County Name', 
         right_on = 'Name')
9/14:
pd.merge(left = population,
         right = unemployment.rename(columns = {'Name': 'County Name'}))
9/15:
pd.merge(left = population,
         right = unemployment[['Name', 'unemployment_rate']].rename(columns = {'Name': 'County Name'}))
9/16:
tn_counties = pd.merge(left = population,
         right = unemployment[['Name', 'unemployment_rate']].rename(columns = {'Name': 'County Name'}))
9/17: grand_divisions = pd.read_csv('data/tn_divisions.csv')
9/18: grand_divisions.head()
9/19: # Your Code Here
7/212:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita')


#here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/213:
#Read in continents.csv contained in the `data` folder into a new dataframe called `continents`. We will be using this dataframe to add a new column to our dataset.
continents = pd.read_csv("../data/continents.csv")
7/214:
#Read in continents.csv contained in the `data` folder into a new dataframe called `continents`. We will be using this dataframe to add a new column to our dataset.
continents = pd.read_csv("../data/continents.csv")
continents.head()
7/215:
#Read in continents.csv contained in the `data` folder into a new dataframe called `continents`. We will be using this dataframe to add a new column to our dataset.
continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
7/216:
#Read in continents.csv contained in the `data` folder into a new dataframe called `continents`. We will be using this dataframe to add a new column to our dataset.
continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns()
7/217:
#Read in continents.csv contained in the `data` folder into a new dataframe called `continents`. We will be using this dataframe to add a new column to our dataset.
continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
7/218: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])].plot()
7/219: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
7/220: gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
7/221:
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.shape
7/222:
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.shape
df_EG_China = gdp_EG_China.set_index['Country']
7/223:
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.shape
gdp_EG_China.set_index['Country']
7/224:
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
#gdp_EG_China.set_index['Country']
7/225:
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
gdp_EG_China.set_index['Country']
7/226:
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index['Country']
7/227:
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
gdp_EG_China.set_index["Country"]
7/228:
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
gdp_EG_China.set_index("Country")
7/229: gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
7/230: EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
7/231:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita')
df.plot(color=df.columns, figsize=(5, 3))
7/232:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita')
7/233:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(color=df.columns, figsize=(5, 3))
7/234:
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]

gdp_EG_China.columns
gdp_EG_China.set_index("Country")
gdp_EG_China.head()
7/235:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
EG_China.plot(color=df.columns, figsize=(5, 3))
7/236:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
EG_China.plot(color=EG_China.columns, figsize=(5, 3))
7/237:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
EG_China.plot()
7/238:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita')


#here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/239:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
EG_China.plot(title:"Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")

#plt.title("Sports Watch Data", fontdict = font1)
#.xlabel("Average Pulse", fontdict = font2)
#plt.ylabel("Calorie Burnage", fontdict = font2)
7/240:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")

#plt.title("Sports Watch Data", fontdict = font1)
#.xlabel("Average Pulse", fontdict = font2)
#plt.ylabel("Calorie Burnage", fontdict = font2)
7/241:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot()
#plt.title(""Countries with hhighest GDP_Growth, fontdict = font1)
#.xlabel("Year", fontdict = font2)
#plt.ylabel("GDP_Per_Capita", fontdict = font2)
7/242:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot()
plt.title(""Countries with hhighest GDP_Growth, fontdict = font1)
#.xlabel("Year", fontdict = font2)
#plt.ylabel("GDP_Per_Capita", fontdict = font2)
7/243:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot()
plt.title(Countries with hhighest GDP_Growth, fontdict = font1)
#.xlabel("Year", fontdict = font2)
#plt.ylabel("GDP_Per_Capita", fontdict = font2)
7/244:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot()
plt.title("Countries with highest GDP_Growth", fontdict = font1)
#.xlabel("Year", fontdict = font2)
#plt.ylabel("GDP_Per_Capita", fontdict = font2)
7/245:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot()
plt.title("Countries with highest GDP_Growth")
#.xlabel("Year")
#plt.ylabel("GDP_Per_Capita")
7/246:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot()
plt.title("Countries with highest GDP_Growth")
plt.xlabel("Year")
#plt.ylabel("GDP_Per_Capita")
7/247:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot()
plt.title("Countries with highest GDP_Growth")
plt.xlabel("Year")
plt.ylabel("GDP_Per_Capita")
7/248:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]

gdp_EG_China.columns
gdp_EG_China.set_index("Country")
gdp_EG_China.head()
7/249:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
gdp_EG_China.set_index("Country")
7/250:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
7/251:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
7/252:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot()
plt.title("Countries with highest GDP_Growth")
plt.xlabel("Year")
plt.ylabel("GDP_Per_Capita")
7/253:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
7/254:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014")
plt.xlable()
plt.ylable("GDP_Per_Capita")
7/255:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014")
#plt.xlable()
plt.ylable("GDP_Per_Capita")
7/256:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014")
#plt.xlable()
plt.ylabel("GDP_Per_Capita")
7/257:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
#plt.xlable()
plt.ylabel("GDP_Per_Capita")
7/258:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel()
7/259:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
7/260:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
7/261:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
7/262: gdp_df.head()
7/263:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
continents.head()
7/264:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
#continents.columns
#continents.head()
7/265:
pd.merge(left = gdp_df, 
         right = continents, 
         #left_on = 'County Name', 
         #right_on = 'Name')
7/266:
pd.merge(left = gdp_df, 
         right = continents) 
         #left_on = 'County Name', 
         #right_on = 'Name')
7/267:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
7/268:
pd.merge(left = gdp_df, 
         right = continents, 
         left_on = 'Country or Area', 
         right_on = 'Country')
7/269:
pd.merge(left = gdp_df_for_analysis, 
         right = continents, 
         left_on = 'Country or Area', 
         right_on = 'Country')
7/270: gdp_df_for_analysis.head()
7/271:
pd.merge(left = gdp_df_for_analysis, 
         right = continents, 
         left_on = 'Country', 
         right_on = 'Country')
7/272:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, 
         left_on = 'Country', 
         right_on = 'Country')
7/273:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, 
         left_on = 'Country', 
         right_on = 'Country')
gdp_df_merged.info()
7/274:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, 
         left_on = 'Country', 
         right_on = 'Country')
gdp_df_merged.info()
gdp_df_merged.shape
7/275:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, 
         left_on = 'Country', 
         right_on = 'Country', how = 'left')
gdp_df_merged.info()
gdp_df_merged.shape
7/276:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, 
         left_on = 'Country', 
         right_on = 'Country', how = 'inner')
gdp_df_merged.info()
gdp_df_merged.shape
7/277:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, 
         left_on = 'Country', 
         right_on = 'Country', 
        how = 'inner')
gdp_df_merged.info()
gdp_df_merged.shape
7/278:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents) 
         
gdp_df_merged.info()
gdp_df_merged.shape
7/279:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
7/280: gdp_df_for_analysis.shape
7/281:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
7/282:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
7/283:
#19. Determine the number of countries per continent. Create a bar chart showing this.
gdp_df_merged['Continent'].value_counts()
7/284:
#19. Determine the number of countries per continent. Create a bar chart showing this.
gdp_df_merged['Continent', 'Country'].value_counts()
7/285:
#19. Determine the number of countries per continent. Create a bar chart showing this.
gdp_df_merged['Continent'].value_counts()
7/286:
#19. Determine the number of countries per continent. Create a bar chart showing this.
plt.bar(gdp_df_merged['Continent'].value_counts()0
7/287:
#19. Determine the number of countries per continent. Create a bar chart showing this.
plt.bar(gdp_df_merged['Continent'].value_counts())
7/288:
#19. Determine the number of countries per continent. Create a bar chart showing this.
gdp_df_merged['Continent'].value_counts()
7/289:
#19. Determine the number of countries per continent. Create a bar chart showing this.
gdp_df_merged['Continent'].value_counts().plot(kind="bar")
7/290: gdp_df_merged['Continent'].unique()
7/291: gdp_df_merged['Continent'].inunique()
7/292: gdp_df_merged['Continent'].nunique()
7/293: gdp_df_merged.groupby('Continent').value_counts()
7/294: gdp_df_merged.groupby('Continent')['Country'].value_counts()
7/295: gdp_df_merged.groupby('Continent')['Country'].nunique()
7/296: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar")
7/297: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', ylabel='Number of Countries')
7/298: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
7/299: #19. Determine the number of countries per continent. Create a bar chart showing this.
7/300:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

sns.catplot(data=(gdp_df_merged.groupby('Continent')['Country'].nunique()), kind="bar", x="Continent", y="GDP_Per_Capita", hue="smoker")
7/301:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

sns.catplot(data=(gdp_df_merged.groupby('Continent')['Country'].nunique()), kind="bar")
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
7/302:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

sns.barplot(data=(gdp_df_merged.groupby('Continent')['Country'].nunique()), kind="bar")
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
7/303:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

sns.boxplot(gdp_df_merged.groupby('Continent')['Country'].nunique())
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
7/304:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
7/305: gdp_df_merged.groupby(['Continent'] & gdp_df_merged['Year']==2014)['GDP_Per_Capita']
7/306:
gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()

dataframe.loc[(dataframe['Age'] == 22) & 
              dataframe['Stream'].isin(options)]
7/307: gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
7/308:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/309:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
gdp_df_merged.loc[gdp_df_merged['Year']== 2014].groupby('Continents')
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/310:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
gdp_df_merged.loc[gdp_df_merged['Year']== 2014].groupby(gdp_df_merged['Continents'])
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/311:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
.groupby(gdp_df_merged['Continents'])
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/312:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.groupby('Continents')['GDP_Per_Capita'].plot(kind='bar')
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/313:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.groupby(['Continents'])['GDP_Per_Capita'].plot(kind='bar')
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/314:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.groupby(['Continents'])
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/315:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/316:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()
    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/317:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/318:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

df_merged_2014.groupby('Continent')
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/319:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

df_merged_2014.groupby('Continent')['GDP_Per_Capita']
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/320:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

sns.boxplot(df_merged_2014.groupby('Continent')['GDP_Per_Capita'], x='congtinents', y='GDP_Per_capita')
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/321:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

sns.boxplot(df_merged_2014.groupby('Continent')['GDP_Per_Capita'], x='continents', y='GDP_Per_capita')
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/322:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

sns.boxplot(df = df_merged_2014, x = 'continents', y = 'GDP_Per_capita')
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/323:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

sns.boxplot(df = df_merged_2014, x = 'Continents', y = 'GDP_Per_capita')
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/324:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

sns.boxplot(df = df_merged_2014, x = df_merged_2014['Continents'], y = df_merged_2014['GDP_Per_capita'])
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/325:
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

#sns.boxplot(df = df_merged_2014, x = df_merged_2014['Continents'], y = df_merged_2014['GDP_Per_capita'])
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/326:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

#sns.boxplot(df = df_merged_2014, x = df_merged_2014['Continents'], y = df_merged_2014['GDP_Per_capita'])
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/327:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

sns.boxplot(df = df_merged_2014, x = df_merged_2014['Continents'], y = df_merged_2014['GDP_Per_capita'])
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/328:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")
#['GDP_Per_Capita']

#.nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')

    #groupby(['Continents']).value_counts()
#).groupby('Continent')['GDP_Per_Capita'].plot()
7/329:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
7/330:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
type(df_merged_2014)

sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
7/331:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
type(df_merged_2014)

sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
7/332:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
type(df_merged_2014)

#sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
7/333:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014())

#sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
7/334:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014()

#sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
7/335:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.head()

#sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
7/336:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

#sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
7/337:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

sns.boxplt(gdp_df_merged.groupby('Continent')['GDP_Per_Capita'])
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
7/338:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

sns.boxplot(gdp_df_merged.groupby('Continent')['GDP_Per_Capita'])
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
7/339:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

gdp_df_merged.groupby('Continent')['GDP_Per_Capita']
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
7/340:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot()
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
7/341:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
7/342:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape.head()

#sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
7/343:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

#sns.boxplot(df = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
7/344:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

#sns.boxplot(data = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
7/345:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x = "Continents", y = "GDP_Per_capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
7/346:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x = "Continents", y = "GDP_Per_Capita")

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
7/347:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014[['GDP_Per_Capita']])

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
7/348:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014[['Continent','GDP_Per_Capita']])

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
7/349:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014[['GDP_Per_Capita']], x=df_merged_2014['Continent'])

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
11/1: import pandas as pd
11/2: animals = pd.read_csv('data/animals.csv')
11/3: animals.head()
13/1: import pandas as pd
13/2: cars = pd.read_csv('data/auto-mpg.csv')
13/3: cars.head()
11/4: animals.head()
11/5: animals['Type'].value_counts()
14/1:
#this is the DS6: Data question1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
14/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)

# there are 4 coulmns Country or Area, Year, Value, Value Footnotes. Value Footnotes has NaN values
14/3:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
14/4:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
14/5:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
14/6:
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
14/7:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
14/8:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
14/9:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
14/10: gdp_df_for_analysis.info()
14/11:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
14/12:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
14/13: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
14/14: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
14/15: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
14/16:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
14/17:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
14/18: gdp_2014['GDP_Per_Capita'].describe()
14/19:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
14/20: gdp_2014.sort_values('GDP_Per_Capita').head(5)
14/21: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
14/22:
#table_X = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    #columns=['Year'])
#table_X.shape
14/23:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.shape
gdp_1990_2017.head()
14/24:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()

#Create the gdp_pivoted df from the .loc df with 1990 and 2017 year and using the .dropna() method to delete the NaN row.
14/25: gdp_pivoted.head()
14/26: gdp_pivoted.describe()
14/27:
gdp_pivoted.shape
gdp_pivoted.columns
14/28: gdp_pivoted.info()
14/29:
#gdp_pivoted_dropnan = gdp_pivoted.dropna()
#gdp_pivoted_dropnan.shape
14/30:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
14/31: gdp_pivoted.sort_values("Percent_Change").head(10)
14/32: gdp_pivoted.sort_values("Percent_Change").tail(10)
14/33:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count
sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
14/34:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
14/35:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
14/36:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
14/37:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
14/38:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
14/39:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
14/40:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
14/41:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
14/42: #gdp_df_merged['Continent'].nunique()
14/43: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
14/44:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
14/45:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014[['GDP_Per_Capita']], x=df_merged_2014['Continent'])

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
11/6: animals.loc[['Type']]['Days in Shelter'].mean()
11/7: animals.loc[['Type']['Days in Shelter']].mean()
11/8: df.loc['Type'].(animals['Days in Shelter'].mean())
11/9: animals.loc['Type'].(animals['Days in Shelter'].mean())
11/10: animals.loc[['Type']].(animals['Days in Shelter'].mean())
11/11: animals.loc['Type'].(animals['Days in Shelter'].mean())
11/12: animals(animals.loc['Type'].(animals['Days in Shelter'].mean())
11/13: animal.groupby('Type')['Days in Sheter'].mean()
11/14: animals.groupby('Type')['Days in Sheter'].mean()
11/15: animals.groupby('Type')['Days in Shelter'].mean()
11/16: animals(animals.loc['Type'].(animals['Days in Shelter'].mean())
11/17: animals.loc[animals['Type'].isin(['CAT', 'DOG'])].groupby('Type')['Days in Shelter'].mean()
11/18: animals.groupby('Type')['Days in Shelter'].mean().loc[['CAT', 'DOG']]
11/19: animals[animals.loc['Type']== 'DOG']['Outcome Type'].describe()
11/20: animals.loc[animals['Type']== 'DOG']['Outcome Type'].describe()
11/21: animals.loc[animals['Type']== 'DOG']['Outcome Type'].value_counts()
11/22: animals.loc[animals['Type'].isin[['DOG', 'CAT']]['Outcome Type'].value_counts()
11/23: animals.groupby['Type']['Outcome Type'].describe()
11/24: animals.groupby('Type')['Outcome Type'].describe()
11/25: animals.loc[animals['Type'].isin(['DOG', 'CAT'])['Outcome Type'].value_counts()
11/26: animals.loc[animals['Type'].isin(['DOG', 'CAT'])]['Outcome Type'].value_counts()
11/27: animals.groupby(['Type'])['Outcome Type'].agg(pd.Series.mode)
14/46:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

gdp_df_merged.groupby('Continent')[['GDP_Per_Capita']].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
14/47:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?

gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
11/28: animals.groupby('Type')['Days in Shelter'].mean().plot(kind = 'bar');
14/48:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?


gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar');
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
14/49:
#20. Create a seaborn boxplot showing GDP per capita in 2014 split out by continent. What do you notice?


gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
11/29: import seaborn as sns
11/30: sns.boxplot(data = animals.head(20), y='Days in Shelter')
11/31: sns.boxplot(data = animals.head(20), x='Type', y='Days in Shelter');
14/50:
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');
#sns.boxplot(data = df_merged_2014[['GDP_Per_Capita']], x=df_merged_2014['Continent'])

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
14/51:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
14/52:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita', title='GDP_Per_Capita across Continents in 2014');
14/53:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
14/54: ### 21. Download the full csv containing Life expectancy at birth, total (years) from [https://data.worldbank.org/indicator/SP.DYN.LE00.IN?name_desc=false](https://data.worldbank.org/indicator/SP.DYN.LE00.IN?name_desc=false). Read this data into a DataFrame named `life_expectancy`.
14/55: life_expectancy = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv')
14/56: life_expectancy = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', skiprows = 4)
14/57:
life_expectancy = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy.head()
14/58:
life_expectancy = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy.shape.head()
14/59:
life_expectancy = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy.shape
14/60:
life_expectancy = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)


life_expectancy.shape #(266, 67)

life_expectancy.columns()
14/61:
life_expectancy = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)


life_expectancy.shape #(266, 67)

life_expectancy.columns
14/62: life_expectancy.drop['Country Code', 'Indicator Name', 'Indicator Code']
14/63: life_expectancy.drop(['Country Code', 'Indicator Name', 'Indicator Code'])
14/64: life_expectancy.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code'])
14/65: le = life_expectancy.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code'])
14/66:
le = life_expectancy.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code'])
le.head(5)
14/67:
le = life_expectancy.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code'])
le.head(2)
14/68:
lemelt = pd.melt(le, id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
14/69: lemelt.head(5)
14/70:
lemelt = pd.melt(life_expectancy.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
14/71:
lemelt = pd.melt(life_expectancy.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
14/72: lemelt.head()
14/73:

emelt = pd.melt(life_expectancy.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
14/74:
lemelt = pd.melt(life_expectancy.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
14/75: lemelt.head()
14/76: lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")
14/77:
life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)


life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
14/78:
#Scratch work
le = life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code'])
le.head(2)
14/79: life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")
14/80: life_expectancy.head(2)
14/81:
life_expectancy.head(2)
life_expectancy.tail(2)
14/82:
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy.describe()
14/83:
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy.value_counts()
#life_expectancy.describe()
14/84:
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts()
#life_expectancy.describe()
14/85:
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts()
#life_expectancy.describe()
14/86:
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
14/87:
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape
14/88: life_expectancy.head(2)
14/89:
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
14/90:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
14/91:
df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80]
14/92:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80]
14/93:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year')
14/94:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').top(10)
14/95:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').head(10)
14/96: #### 24. Merge `gdp_per_capita` and `life_expectancy`, keeping all countries and years that appear in both DataFrames. Save the result to a new DataFrame named `gdp_le`. If you get any errors in doing this, read them carefully and correct them. Look at the first five rows of your new data frame to confirm it merged correctly. Also, check the last five rows to make sure the data is clean and as expected.
14/97:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= 'Country')
14/98:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
14/99:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)
14/100:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)
gdp_le.shape
14/101:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)
gdp_le.shape #395302, 5
gdp_df_for_analysis.shape
14/102:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)
gdp_le.shape #395302, 5
gdp_df_for_analysis.shape #6868,3
life_expectancy.shape
14/103:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)
gdp_le.shape #395302, 5
gdp_df_for_analysis.shape #6868,3
life_expectancy.shape #16492, 3
14/104:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)

gdp_df_for_analysis.shape #6868,3
life_expectancy.shape #16492, 3
gdp_le.shape #395302, 5
14/105:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)

gdp_df_for_analysis.shape #6868,3
life_expectancy.shape #16492, 3
gdp_le.shape #395302, 5

gdp_le.head(5)
14/106:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'left', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)

gdp_df_for_analysis.shape #6868,3
life_expectancy.shape #16492, 3
gdp_le.shape #395302, 5

gdp_le.head(5)
14/107:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)

gdp_df_for_analysis.shape #6868,3
life_expectancy.shape #16492, 3
gdp_le.shape #395302, 5

gdp_le.head(5)
14/108:
gdp_le = pd.merge(left = gdp_df_for_analysis, 
         right = life_expectancy, how = 'outer', on= ['Year','Country']) #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)

gdp_df_for_analysis.shape #6868,3
life_expectancy.shape #16492, 3
gdp_le.shape #395302, 5

gdp_le.head(5)
14/109:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)

gdp_df_for_analysis.shape #6868,3
life_expectancy.shape #16492, 3
gdp_le.shape #395302, 5

gdp_le.head(5)
14/110:
pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
14/111: gdp_df_merged.info
14/112: gdp_df_merged.info()
14/113:
gdp_df_merged.info()
life_expectancy.info()
14/114:
gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'].astype(int)
14/115:
gdp_df_merged.info()
life_expectancy.info()
life_expectancy = life_expectancy['Year'].astype(int)
14/116:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= 'Country') #how =outer keeps all rows
gdp_le.head(5)
gdp_le.tail(5)

#gdp_df_merged.shape #6868,3
life_expectancy.shape #16492, 3
gdp_le.shape #395302, 5

gdp_le.head(5)
14/117:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
14/118:
gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
14/119:
gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
14/120:
pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
14/121:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita')

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
14/122:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita');

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
14/123:
gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
14/124:
pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
14/125:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
14/126:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
gdp_le.head(5)
14/127:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
gdp_le.head(5)
gdp_le_tail()
14/128:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
gdp_le.head(5)
gdp_le.tail()
14/129:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
gdp_le.head(5)
gdp_le.tail()
dgp_le.shape
14/130:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape
14/131:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape #16821, 5
gdp_le.info()
14/132: gdp_le.loc[gdp_le['Year'] == 2019]
14/133: gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
14/134:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape
14/135:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80]
14/136:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].value_counts('Country')
14/137:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].counts('Country')
14/138:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
14/139:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info().head()
14/140:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
14/141:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80]
14/142:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
14/143: #### 43 countries have the expectany equall to or above 80.
14/144: #### 43 countries have the expectany equall to or above 80 in 2019
14/145: gdp_le_2019.sort_values('GDP_Per_Capita', asceding = False)
14/146: gdp_le_2019.sort_values('GDP_Per_Capita')
14/147: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=True)
14/148: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=False)
14/149: gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))]
14/150:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
plt.show()
14/151:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy").add_legend()
# show the object
plt.show()
14/152:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
plt.show()
14/153: sns.scatterplot(data=gdp_le_2019, x="Life_Expectance", y="GDP_Per_Capita")
14/154: sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita")
14/155: sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita");
14/156:
sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", 
               plt.xlabel("Effect of GDP_Per_Capita growth on Life expectancy"));
14/157:
sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita"); 
               plt.xlabel("Effect of GDP_Per_Capita growth on Life expectancy")
14/158:
sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita"); 
               
#plt.xlabel("Effect of GDP_Per_Capita growth on Life expectancy")
14/159:
sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita"); 
               
plt.xlabel("Effect of GDP_Per_Capita growth on Life expectancy")
14/160:
sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", plt.xlabel("Effect of GDP_Per_Capita growth on Life expectancy")); 
               
plt.xlabel("Effect of GDP_Per_Capita growth on Life expectancy")
14/161:
sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita"); 
               
plt.xlabel("Effect of GDP_Per_Capita growth on Life expectancy")
14/162:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita")
               
plt.xlabel("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
14/163:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita")
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
14/164:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", ci=none)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
14/165:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
14/166: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
14/167:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
14/168: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
14/169:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
14/170: gdp_le_2019['log_GDP'] = np.log(gdp_le_2019['GDP_Per_Capita'])
14/171: gdp_le_2019.head(5)
14/172: gdp_le_2019['log_GDP'].corr(gdp_le_2019['Life_Expectancy'])
14/173:
fig2 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig2)
14/174:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
14/175:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
14/176:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent');
plt.title("Regression plot of the log_GDP growth on Life expectancy")
14/177:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
plt.title("Regression plot of the log_GDP growth on Life expectancy")
15/1:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="r").add_legend()
# show the object
plt.show()
15/2:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
plt.show()
15/3:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
plt.show()
16/1:
#this is the DS6: Data question1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
16/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)

# there are 4 coulmns Country or Area, Year, Value, Value Footnotes. Value Footnotes has NaN values
16/3:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
16/4:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
16/5:
#Drop the 'Value Footnotes' column, and rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
16/6:
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
16/7:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
16/8:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
16/9:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
16/10: gdp_df_for_analysis.info()
16/11:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
16/12:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
16/13: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
16/14: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
16/15: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
16/16:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
16/17:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
16/18: gdp_2014['GDP_Per_Capita'].describe()
16/19:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
16/20: gdp_2014.sort_values('GDP_Per_Capita').head(5)
16/21: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
16/22:
#table_X = gdp_df_for_analysis.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    #columns=['Year'])
#table_X.shape
16/23:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.shape
gdp_1990_2017.head()
16/24:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()

#Create the gdp_pivoted df from the .loc df with 1990 and 2017 year and using the .dropna() method to delete the NaN row.
16/25: gdp_pivoted.head()
16/26: gdp_pivoted.describe()
16/27:
gdp_pivoted.shape
gdp_pivoted.columns
16/28: gdp_pivoted.info()
16/29:
#gdp_pivoted_dropnan = gdp_pivoted.dropna()
#gdp_pivoted_dropnan.shape
16/30:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
16/31: gdp_pivoted.sort_values("Percent_Change").head(10)
16/32: gdp_pivoted.sort_values("Percent_Change").tail(10)
16/33:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count
sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
16/34:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
16/35:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
16/36:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
16/37:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
16/38:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
16/39:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
16/40:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
16/41:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
16/42: #gdp_df_merged['Continent'].nunique()
16/43: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
16/44:
#scratch work
gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
16/45:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita');

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
16/46:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
16/47:
life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
16/48:
#Scratch work to drop the columns
le = life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code'])
le.head(2)
16/49:
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
16/50:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
16/51:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').head(10)
16/52:
gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
16/53:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year'])
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape #16821, 5
gdp_le.info()
16/54:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
16/55: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=False)
16/56:
#Scratch work
#Subsetting the df for top three countries
gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))]
16/57:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
plt.show()
16/58:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
16/59:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
16/60: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
16/61: gdp_le_2019['log_GDP'] = np.log(gdp_le_2019['GDP_Per_Capita'])
16/62: gdp_le_2019.head(5)
16/63: gdp_le_2019['log_GDP'].corr(gdp_le_2019['Life_Expectancy'])
16/64:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
plt.title("Regression plot of the log_GDP growth on Life expectancy")
16/65:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
16/66:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
16/67:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
#gdp_df.tail(5)

# there are 4 coulmns Country or Area, Year, Value, Value Footnotes. Value Footnotes has NaN values
16/68: ### there are 4 coulmns Country or Area, Year, Value, Value Footnotes. The Value Footnotes has NaN values
16/69:
##After droping Value Footnote column rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
16/70:
gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
16/71: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
16/72:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
gdp_2014.shape
16/73:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
#gdp_2014.shape #(238, 2)
16/74:
### gdp_2014 is created by for year 2014.
**There are 238 row representing 238 countries in the dataset for the Year 2014**
16/75: gdp_pivoted.sort_values("Percent_Change").head(10)
16/76: gdp_pivoted.sort_values("Percent_Change", ascending=False).head(10)
16/77:
**I notice that African GDP is the poorest of all and Europe is the best. For Europe GDP_is is well distributed, as an seen by the almost centre placement of the median line.
Europ has eqaul distribution of rich and poor countries as can be seen by the mean line. 
While in Asia most of the Countries are in the lowerside and have poor GDP_Per_capita.
The same uneen distribution is true for North America.**
16/78:
life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
16/79:
*#df of the life expectancy data, looking at the file in CSV it show that there are few top rows which are note, that is why I skiped first 4 rows at the time of reading the file.As it was showing error upon reading the csv file**

life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
16/80:
#df of the life expectancy data, looking at the file in CSV it show that there are few top rows which are note, that is why I skiped first 4 rows at the time of reading the file.As it was showing error upon reading the csv file**

life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
16/81:
#droping the columns and making the table transpose. Column to rows.
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
17/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
17/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
17/3:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
17/4:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
17/5:
#Drop the 'Value Footnotes' column, 

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
17/6:
##After droping Value Footnote column rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
17/7:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
17/8:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
17/9:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
17/10: gdp_df_for_analysis.info()
17/11:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
17/12:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
17/13: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
17/14: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
17/15: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
17/16:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
17/17:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
#gdp_2014.shape #(238, 2)
17/18: gdp_2014['GDP_Per_Capita'].describe()
17/19:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
17/20: gdp_2014.sort_values('GDP_Per_Capita').head(5)
17/21: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
17/22:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.shape
gdp_1990_2017.head()
17/23:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()

#Create the gdp_pivoted df from the .loc df with 1990 and 2017 year and using the .dropna() method to delete the NaN row.
17/24: gdp_pivoted.head()
17/25: gdp_pivoted.describe()
17/26:
gdp_pivoted.shape
gdp_pivoted.columns
17/27: gdp_pivoted.info()
17/28:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
17/29: gdp_pivoted.sort_values("Percent_Change").head(10)
17/30: gdp_pivoted.sort_values("Percent_Change").tail(10)
17/31:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count

sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
17/32: gdp_pivoted.sort_values("Percent_Change", ascending=False).head(10)
17/33:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
17/34:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
17/35:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
17/36:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
17/37:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
17/38:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
17/39:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
17/40:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
17/41: #gdp_df_merged['Continent'].nunique()
17/42: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
17/43:
#scratch work
gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
17/44:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita');

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
17/45:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
17/46:
#df of the life expectancy data, looking at the file in CSV it show that there are few top rows which are note, that is why I skiped first 4 rows at the time of reading the file.As it was showing error upon reading the csv file**

life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
17/47:
#droping the columns and making the table transpose. Column to rows.
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
17/48:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
17/49:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').head(10)
17/50:
#gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
17/51:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year']) 
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape #16821, 5
gdp_le.info()
17/52:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
17/53: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=False)
17/54:
#Scratch work
#Subsetting the df for top three countries
gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))]
17/55:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
17/56:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
17/57:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
17/58: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
17/59: gdp_le_2019['log_GDP'] = np.log(gdp_le_2019['GDP_Per_Capita'])
17/60: gdp_le_2019.head(5)
17/61: gdp_le_2019['log_GDP'].corr(gdp_le_2019['Life_Expectancy'])
17/62:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
plt.title("Regression plot of the log_GDP growth on Life expectancy")
18/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
18/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
18/3:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele_df.head(10)
tele_df.tail(5)
18/4:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(5)
18/5:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(5)
tele1_df.dtyepe()
18/6:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(5)
tele1_df.dtype()
18/7:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(5)
tele1_df.info()
18/8:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
#tele1_df.tail(5)
#tele1_df.info()
18/9:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
#tele1_df.tail(5)
tele1_df.info()
18/10:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(5)
tele1_df.info()
18/11:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(5)
#tele1_df.info()
18/12:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(20)
#tele1_df.info()
18/13:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
#tele1_df.tail(20)
#tele1_df.info()
18/14:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
#tele1_df.tail(20)
tele1_df.info()
18/15:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
#tele1_df.tail(20)
tele1_df.info()
tele1_df.columns
18/16:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
#tele1_df.tail(20)
tele1_df.info()
tele1_df.columns
18/17:
tele2 = tele1_df.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.head()
18/18:
tele2 = tele1_df.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.tail()
18/19:
tele2 = tele1_df.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.tail()
tele2.info()
18/20: tele2['Year', 'Value'].astype(int)
18/21: tele2[columns =['Year', 'Value']].astype(int)
18/22: tele2[columns['Year', 'Value']].astype(int)
18/23: tele2[['Year', 'Value']].astype(int)
18/24: tele2['Year'].astype(int)
18/25: tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value'])
18/26:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value']) 
tele_dropnan.info
18/27:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value']) 
tele_dropnan.info()
18/28: tele_dropnan['Year'].astype(int)
18/29:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
18/30:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape
18/31:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns[Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
18/32:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns[Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape
18/33:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape
18/34:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape
18/35:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape #(4496, 6)
tele2.info
18/36:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape #(4496, 6)
tele2.info()
18/37:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape #(4496, 6)
tele2.info()
tele2["Year"]astype(int)
18/38:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape #(4496, 6)
tele2.info()
tele2["Year"].astype(int)
18/39:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape #(4496, 6)
tele2.info()
tele2.tail(20)
18/40: tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrow=4494)
18/41: tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
18/42:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
18/43:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
18/44:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
18/45:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
18/46:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
18/47:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
18/48:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc2.dropna(columns=['Country or Area', 'Year'])
18/49:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc2.dropna['Country or Area', 'Year']
18/50:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc2.dropna[['Country or Area', 'Year']]
18/51:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc2.dropna["Country or Area"]
18/52:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc2["Country or Area", "Year"].dropna()
18/53:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc2["Country or Area"].dropna()
18/54:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
ttc3 = tc2["Country or Area"].dropna()
18/55:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2["Country or Area"].dropna()
tc3 = tc3.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
18/56:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2["Country or Area"].dropna()
tc4 = tc3.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
18/57:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2["Country or Area"].dropna()
tc3.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
19/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
19/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(20)
tele1_df.info()
tele1_df.columns
19/3:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape #(4496, 6)
tele2.info()
tele2.tail(20)
19/4:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2["Country or Area"].dropna()
tc3.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
19/5:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2["Country or Area"].dropna()
tc3.rename(column = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
19/6:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(column2 = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
19/7:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
19/8:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
19/9: tc3['Year'].value_counts()
19/10: tc3['Year'].value_counts().sort_index(ascending=False)
19/11:
tc3['Year'].value_counts().sort_index(ascending=False)
tc3['Country'].value_counts().sort_index(ascending=False)
19/12:
tc3['Year'].value_counts().sort_index(ascending=False)
tc3['Country'].value_counts().sort_index(ascending=False).tail(5)
19/13:
tc3['Year'].value_counts().sort_index(ascending=False)
tc3['Country'].value_counts() #.sort_index(ascending=False).tail(5)
19/14:
tc3['Year'].value_counts().sort_index(ascending=False)
tc3['Country'].value_counts().head() #.sort_index(ascending=False).tail(5)
19/15:
tc3['Year'].value_counts().sort_index(ascending=False)
tc3['Country'].value_counts().tail() #.sort_index(ascending=False).tail(5)
19/16:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
19/17:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
17/63:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
#tc.head()
#tc.tal(i)
#tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
tc3.info()
17/64:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
#tc.head()
#tc.tal(i)
#tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
tc3.info()
tc3.describe()
17/65:
gdp_df_merged_tele = pd.merge(left = gdp_df_merged, 
         right = tc3, how = 'inner', on= 'Country') 
         
gdp_df_merged_tele.info()
gdp_df_merged_tele.shape
gdp_df_merged_tele.head()
17/66:
gdp_df_merged_tele = pd.merge(left = gdp_df_merged, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_df_merged_tele.info()
gdp_df_merged_tele.shape
gdp_df_merged_tele.head()
17/67:
#INTERUSER IN 2014
sns.boxplot(data = gdp_df_merged_tele.loc[gdp_df_merged_tele['Year']== 2014], 
            x='Continent', y='Percentage_Internet_User');
17/68:
gdp_df_merged_tele = pd.merge(left = gdp_df_merged, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_df_merged_tele.info()
gdp_df_merged_tele.shape
gdp_df_merged_tele.head()
17/69: gdp_df_merged_tele.sort_values("Percentage_Internet_use", ascending=False).head(10)
17/70: gdp_df_merged_tele.sort_values("Percentage_Internet_User", ascending=False).head(10)
17/71:
gdp_df_merged_tele.sort_values("Percentage_Internet_User", ascending=False).head(10)

#Ireland and Bermida has the highest interntusers
17/72:
gdp_le_tele = pd.merge(left = gdp_le, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_le_tele.info()
gdp_le_tele.shape
gdp_le_tele.head()
17/73:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
17/74:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
17/75:
fig = sns.scatterplot(data=gdp_le_tele, x="GDP_Per_Capita", y="Percentage_Internet_User", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.show(fig)
17/76:
fig = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.show(fig)
17/77:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.show(figx)
17/78:
figy = sns.lmplot(data=gdp_le_2019, x="Percentage_Internet_Usage", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
17/79:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_Usage", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
17/80:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
17/81:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
17/82:
gdp_le_tele_2019 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2019]
gdp_le_tele_2019.shape #276,5
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
17/83:
gdp_le_tele_2019 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2019.shape #276,5
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
17/84:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2019.shape #179, 6
gdp_le
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
17/85:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2014.shape #179, 6
gdp_le
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
17/86:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2014.shape #179, 6

#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
17/87:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2014.shape #179, 6
gdp_le_tele_2014.head(10)
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
17/88:
figy = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth in 2014")
plt.show(figy)
17/89:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth in 2014")
plt.show(figz)
17/90:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="Life_Expectancy", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on Life_Expectancy in 2014")
plt.show(figz)
17/91: gdp_le_tele_2014['log_GDP'].corr(gdp_le_tele_2014['Life_Expectancy'])
17/92: gdp_le_tele_2014['GDP_Per_Capita'].corr(gdp_le_tele_2014['Life_Expectancy'])
17/93: gdp_le_tele_2014['Percentage_Internet_Usage'].corr(gdp_le_tele_2014['Life_Expectancy'])
17/94: gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
17/95: gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
17/96: a= gdp_le_tele_2014['GDP_Per_Capita'].corr(gdp_le_tele_2014['Life_Expectancy'])
17/97: b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
17/98: c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
17/99: print("The increased internet usage is directly correlate to GDP growth by", c ,"and life expectancy by", b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014".)
17/100: print("The increased internet usage is directly correlate to GDP growth by", c ,"and life expectancy by", b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/101: print("The increased internet usage is directly correlate to GDP growth by" c "and life expectancy by" b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
21/1:
import pandas as pd
import matplotlib.pyplot as plt
21/2: animals = pd.read_csv('data/animals.csv')
21/3: animals.head()
21/4:
(
    animals
    .loc[animals['Breed'] == 'ROTTWEILER']
    ['Outcome Type']
    .value_counts()
    .plot(kind = 'barh')
)
plt.title('Outcomes for Rottweilers');
21/5:
def outcome_plot(breed):
    # Fill in the function to produce the plot
17/102: print("The increased internet usage is directly correlate to GDP growth by", c "and life expectancy by", b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/103:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
print("The increased internet usage is directly correlate to GDP growth by", c "and life expectancy by", b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/104:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
print(a)
#print("The increased internet usage is directly correlate to GDP growth by", c "and life expectancy by", b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/105:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
print(b)
#print("The increased internet usage is directly correlate to GDP growth by", c "and life expectancy by", b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/106:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
print(b)
#print("The increased internet usage is directly correlate to GDP growth by",
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita']), "and life expectancy by", b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy']), "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/107:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by {c} "and life expectancy by {b} for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/108:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by {c} and life expectancy by {b} for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/109:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by, c, and life expectancy by {b} for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
21/6:
def outcome_plot(breed):
    (
    animals
    .loc[animals['Breed'] == 'breed']
    ['Outcome Type']
    .value_counts()
    .sort_values()
    .plot(kind = 'barh')
)
plt.title(f'Outcomes for {breed.capitalize()}s');
    
    
    
    
    
    # Fill in the function to produce the plot
21/7:
def outcome_plot(breed):
    (
    animals
    .loc[animals['Breed'] == breed]
    ['Outcome Type']
    .value_counts()
    .sort_values()
    .plot(kind = 'barh')
)
plt.title(f'Outcomes for {breed.capitalize()}s');
    
    
    
    
    
    # Fill in the function to produce the plot
21/8:
def outcome_plot(breed):
    (
    animals
    .loc[animals['Breed'] == breed]
    ['Outcome Type']
    .value_counts()
    .sort_values()
    .plot(kind = 'barh')
    )
plt.title(f'Outcomes for {breed.capitalize()}s');
    
    
    
    
    
    # Fill in the function to produce the plot
21/9:
def outcome_plot(breed):
    (
    animals
    .loc[animals['Breed'] == breed]
    ['Outcome Type']
    .value_counts()
    .sort_values()
    .plot(kind = 'barh')
    )
#plt.title(f'Outcomes for {breed.capitalize()}s');
    
    
    
    
    
    # Fill in the function to produce the plot
21/10:
def outcome_plot(breed):
    (
    animals
    .loc[animals['Breed'] == breed]
    ['Outcome Type']
    .value_counts()
    .sort_values()
    .plot(kind = 'barh')
    )
    plt.title(f'Outcomes for {breed.capitalize()}s');
    
    
    
    
    
    # Fill in the function to produce the plot
21/11:
def outcome_plot(breed):
    (
    animals
    .loc[animals['Breed'] == breed]
    ['Outcome Type']
    .value_counts()
    .sort_values()
    .plot(kind = 'barh')
    )
    plt.title(f'Outcomes for {breed.capitalize()}s');
    
    
    
    
    
    # Fill in the function to produce the plot
21/12: outcome_plot(breed = "PIT BULL")
17/110:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + str(b) + str(c)) #, and life expectancy by {b} for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/111:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + b + c) #, and life expectancy by {b} for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/112:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + b ) #, and life expectancy by {b} for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/113:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + b + "and life expectancy by", + b + "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/114:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + b + "and life expectancy by", + b, "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/115:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + b + "and life expectancy by", + b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/116:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + b  "and life expectancy by", + b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/117:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + b) "and life expectancy by", + b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/118:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + b) # "and life expectancy by", + b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/119:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + b )"and life expectancy by", + b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/120:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by" + b )"and life expectancy by", + b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/121:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", + b )#"and life expectancy by", + b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/122:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", b )#"and life expectancy by", + b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/123:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", c )#"and life expectancy by", + b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/124:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", c, "and life expectancy by", + b "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
17/125:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", c, "and life expectancy by", b, "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
19/18:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(20)
#tele1_df.info()
#tele1_df.columns
19/19:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(20)
#tele1_df.info()
#tele1_df.columns
tele1_df.loc[tele1_df['Value'].isna()]
17/126:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth in 2014")
plt.show(figz)
23/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
23/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
23/3:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
23/4:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
23/5:
#Drop the 'Value Footnotes' column, 

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
23/6:
##After droping Value Footnote column rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
23/7:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
23/8:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
23/9:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
23/10: gdp_df_for_analysis.info()
23/11:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
23/12:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
23/13: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
23/14: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
23/15: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
23/16:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
23/17:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
#gdp_2014.shape #(238, 2)
23/18: gdp_2014['GDP_Per_Capita'].describe()
23/19:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
23/20: gdp_2014.sort_values('GDP_Per_Capita').head(5)
23/21: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
23/22:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.shape
gdp_1990_2017.head()
23/23:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()

#Create the gdp_pivoted df from the .loc df with 1990 and 2017 year and using the .dropna() method to delete the NaN row.
23/24: gdp_pivoted.head()
23/25: gdp_pivoted.describe()
23/26:
gdp_pivoted.shape
gdp_pivoted.columns
23/27: gdp_pivoted.info()
23/28:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
23/29: gdp_pivoted.sort_values("Percent_Change").head(10)
23/30: gdp_pivoted.sort_values("Percent_Change").tail(10)
23/31:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count

sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
23/32: gdp_pivoted.sort_values("Percent_Change", ascending=False).head(10)
23/33:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
23/34:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
23/35:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
23/36:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
23/37:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
23/38:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
23/39:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
23/40:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
23/41: #gdp_df_merged['Continent'].nunique()
23/42: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
23/43:
#scratch work
gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
23/44:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita');

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
23/45:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
23/46:
#df of the life expectancy data, looking at the file in CSV it show that there are few top rows which are note, that is why I skiped first 4 rows at the time of reading the file.As it was showing error upon reading the csv file**

life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
23/47:
#droping the columns and making the table transpose. Column to rows.
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
23/48:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
23/49:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').head(10)
23/50:
#gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
23/51:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year']) 
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape #16821, 5
gdp_le.info()
23/52:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
23/53: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=False)
23/54:
#Scratch work
#Subsetting the df for top three countries
gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))]
23/55:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
23/56:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
23/57:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
23/58: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
23/59: gdp_le_2019['log_GDP'] = np.log(gdp_le_2019['GDP_Per_Capita'])
23/60: gdp_le_2019.head(5)
23/61: gdp_le_2019['log_GDP'].corr(gdp_le_2019['Life_Expectancy'])
23/62:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
plt.title("Regression plot of the log_GDP growth on Life expectancy")
23/63:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
#tc.head()
#tc.tal(i)
#tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
tc3.info()
tc3.describe()
23/64:
gdp_df_merged_tele = pd.merge(left = gdp_df_merged, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_df_merged_tele.info()
gdp_df_merged_tele.shape
gdp_df_merged_tele.head()
23/65:
gdp_df_merged_tele.sort_values("Percentage_Internet_User", ascending=False).head(10)

#Ireland and Bermida has the highest interntusers
23/66:
#INTERUSER IN 2014
sns.boxplot(data = gdp_df_merged_tele.loc[gdp_df_merged_tele['Year']== 2014], 
            x='Continent', y='Percentage_Internet_User');
23/67:
gdp_le_tele = pd.merge(left = gdp_le, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_le_tele.info()
gdp_le_tele.shape
gdp_le_tele.head()
23/68:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
23/69:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.show(figx)
23/70:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
23/71:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2014.shape #179, 6
gdp_le_tele_2014.head(10)
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
23/72:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth in 2014")
plt.show(figz)
23/73:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="Life_Expectancy", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on Life_Expectancy in 2014")
plt.show(figz)
23/74: a= gdp_le_tele_2014['GDP_Per_Capita'].corr(gdp_le_tele_2014['Life_Expectancy'])
23/75: b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
23/76: c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
23/77:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", c, "and life expectancy by", b, "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
19/20: pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls").info().dtypes()
19/21: pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls").info()#.dtypes()
19/22: hm-df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls").info()#.dtypes()
19/23: hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls").info()#.dtypes()
19/24:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls").info()
hm_df.head(10)
19/25:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls")
hm_df.head(10)
19/26:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls")
hm_df.head(10)
hm_df.tail(10)
19/27:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls")
hm_df.head(10)
hm_df.tail(10)
hm_df.shape
19/28:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls",
                     skiprows = 2)
hm_df.head(10)
#hm_df.tail(10)
#hm_df.shape #(347, 47)
19/29:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls",
                     skiprows = 1)
hm_df.head(10)
#hm_df.tail(10)
#hm_df.shape #(347, 47)
19/30:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls",
                     skiprows = 1)
hm_df.head(10)
#hm_df.tail(10)
hm_df.shape #(347, 47)
19/31:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls",
                     skiprows = 1)
hm_df.head(10)
#hm_df.tail(10)
#hm_df.shape #(347, 47)
hm_df.columns()
19/32:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls",
                     skiprows = 1)
hm_df.head(10)
#hm_df.tail(10)
#hm_df.shape #(347, 47)
hm_df.columns
24/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
24/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(20)
#tele1_df.info()
#tele1_df.columns
tele1_df.loc[tele1_df['Value'].isna()] # to find the first row is Nan in the column to reread the CSV droping those rows
24/3:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape #(4496, 6)
tele2.info()
tele2.tail(20)
24/4:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
24/5:
tc3['Year'].value_counts().sort_index(ascending=False)
tc3['Country'].value_counts().tail() #.sort_index(ascending=False).tail(5)
24/6:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls",
                     skiprows = 1)
hm_df.head(10)
#hm_df.tail(10)
#hm_df.shape #(347, 47)
hm_df.columns
24/7:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
24/8:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
tele1_df = pd.read_csv("../data/telecommunication_percentage usage.csv")
tele1_df.head(10)
tele1_df.tail(20)
#tele1_df.info()
#tele1_df.columns
tele1_df.loc[tele1_df['Value'].isna()] # to find the first row is Nan in the column to reread the CSV droping those rows
24/9:
tele_dropnan = tele1_df.dropna(subset=['Country or Area', 'Value', 'Year']) 
tele_dropnan.info()
tele_dropnan.shape #(4496, 6)
tele2 = tele_dropnan.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tele2.shape #(4496, 6)
tele2.info()
tele2.tail(20)
24/10:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
tc.head()
tc.tail()
tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
24/11:
tc3['Year'].value_counts().sort_index(ascending=False)
tc3['Country'].value_counts().tail() #.sort_index(ascending=False).tail(5)
24/12:
hm_df = pd.read_excel("../data/Intentional homicide victims by sex, counts and ra.xls",
                     skiprows = 1)
hm_df.head(10)
#hm_df.tail(10)
#hm_df.shape #(347, 47)
hm_df.columns
24/13: hm_df.head(10)
24/14: hm_df.dtypes()
24/15: hm_df.dtype()
24/16: hm_df.dtype
24/17: hm_df.info()
24/18: hm_df[2].value_counts()
24/19: hm_df.iloc[:, 2].value_counts()
24/20: hm_df.iloc[:, 2].nunique()
24/21: hm_df.iloc[:, 2].nunique().value_counts()
24/22: hm_df.iloc[:, 2].value_counts()
26/1:
cities = ['Nashville, TN 37115', 
          'Bowling Green, KY 42101', 
          'Spring Hill, TN 37174']
26/2:
def zip_slice(city):
    return #Fill this in
26/3:
for city in cities:
    print(zip_slice(city))
26/4:
def zip_split(city):
    return #Fill this in
26/5:
for city in cities:
    print(zip_split(city))
26/6:
import re

def zip_re(city):
    return #Fill this in
26/7:
for city in cities:
    print(zip_re(city))
26/8:
cities = ['Nashville, TN 37115', 
          'Bowling Green, KY 42101', 
          'Spring Hill, TN 37174']
26/9:
def zip_slice(city):
    return #Fill this in
26/10:
for city in cities:
    print(zip_slice(city))
26/11:
def zip_split(city):
    return #Fill this in
26/12:
for city in cities:
    print(zip_re(city))
26/13:
b = "Hello, World!"
print(b[2:5])
26/14:
def zip_slice(city):
    return [[:3]] #Fill this in
26/15:
def zip_slice(city):
    return [[,:3]] #Fill this in
26/16:
def zip_slice(city):
    return [[1:3]] #Fill this in
26/17:
cities = ['Nashville, TN 37115', 
          'Bowling Green, KY 42101', 
          'Spring Hill, TN 37174']
26/18:
def zip_slice(city):
    return [cities[1:3]] #Fill this in
26/19: city = city[-5]
26/20:
city = city[-5]
city
26/21:
city = cities[-5]
city
26/22:
cities = ['Nashville, TN 37115', 
          'Bowling Green, KY 42101', 
          'Spring Hill, TN 37174']
print(cities)
26/23:
city = cities[-5]
city
26/24:
city = cities[0]
city
26/25:
city = cities[-5]
city
26/26:
city = cities[0]
city
26/27:
city = cities[0]
city
26/28: city[-5]
26/29: city[-5:]
26/30:
def zip_slice(city):
     return [cities[1:3]] #Fill this in
26/31:
def zip_slice(city):
     return [cities[1:3]] #Fill this in
26/32: zip_slice('Nashville, TN 37115')
26/33:
def zip_slice(city):
     return [city[-5:]] #Fill this in
26/34: zip_slice('Nashville, TN 37115')
26/35:
def zip_slice(city):
     return city[-5:] #Fill this in
26/36: zip_slice('Nashville, TN 37115')
26/37: zip_slice(cities)
26/38:
for city in cities:
    print(zip_slice(city))
26/39:
cities = ['Nashville, TN 37115', 
          'Bowling Green, KY 42101', 
          'Spring Hill, TN 37174']
print(cities)
26/40:
def zip_slice(city):
     return city[-5:] #Fill this in
26/41: zip_slice(cities)
26/42:
for city in cities:
    print(zip_slice(city))
26/43:
for city in cities:
    print(zip_slice(city))
27/1:
cities = ['Nashville, TN 37115', 
          'Bowling Green, KY 42101', 
          'Spring Hill, TN 37174']
print(cities)
27/2:
city = cities[0]
city
27/3: city[-5:]
27/4:
def zip_slice(city):
     return city[-5:] #Fill this in
27/5:
for city in cities:
    print(zip_slice(city))
27/6:
def zip_split(city):
    return #Fill this in
27/7:
for city in cities:
    print(zip_split(city))
27/8:
import re

def zip_re(city):
    return #Fill this in
27/9:
for city in cities:
    print(zip_re(city))
27/10:
b = "Hello, World!"
print(b[2:5])
27/11: city
27/12:
def zip_split(city):
    return city.split(sep=" ")[-1] #Fill this in
27/13:
for city in cities:
    print(zip_split(city))
27/14: import re
27/15: city
27/16: re.search('\d{5}', city)[0]
27/17: re.search('\d{5}', city)[1]
27/18: re.search('\d{5}', city)[0]
27/19: re.search(r'\d+', city)[0]
27/20:
def zip_re(city):
    return re.search(r'\d{5}', city)[0]  #Fill this in
27/21:
for city in cities:
    print(zip_re(city))
28/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
28/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
28/3:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
28/4:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
28/5:
#Drop the 'Value Footnotes' column, 

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
28/6:
##After droping Value Footnote column rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
28/7:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
28/8:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
28/9:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
28/10: gdp_df_for_analysis.info()
28/11:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
28/12:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
28/13: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
28/14: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
28/15: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
28/16:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
28/17:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
#gdp_2014.shape #(238, 2)
28/18: gdp_2014['GDP_Per_Capita'].describe()
28/19:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
28/20: gdp_2014.sort_values('GDP_Per_Capita').head(5)
28/21: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
28/22:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.shape
gdp_1990_2017.head()
28/23:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()

#Create the gdp_pivoted df from the .loc df with 1990 and 2017 year and using the .dropna() method to delete the NaN row.
28/24: gdp_pivoted.head()
28/25: gdp_pivoted.describe()
28/26:
gdp_pivoted.shape
gdp_pivoted.columns
28/27: gdp_pivoted.info()
28/28:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
28/29: gdp_pivoted.sort_values("Percent_Change").head(10)
28/30: gdp_pivoted.sort_values("Percent_Change").tail(10)
28/31:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count

sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
28/32: gdp_pivoted.sort_values("Percent_Change", ascending=False).head(10)
28/33:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
28/34:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
28/35:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
28/36:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
28/37:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
28/38:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
28/39:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
28/40:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
28/41: #gdp_df_merged['Continent'].nunique()
28/42: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
28/43:
#scratch work
gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
28/44:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita');

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
28/45:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
28/46:
#df of the life expectancy data, looking at the file in CSV it show that there are few top rows which are note, that is why I skiped first 4 rows at the time of reading the file.As it was showing error upon reading the csv file**

life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
28/47:
#droping the columns and making the table transpose. Column to rows.
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
28/48:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
28/49:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').head(10)
28/50:
#gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
28/51:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year']) 
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape #16821, 5
gdp_le.info()
28/52:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
28/53: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=False)
28/54:
#Scratch work
#Subsetting the df for top three countries
gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))]
28/55:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
28/56:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
28/57:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
28/58: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
28/59: gdp_le_2019['log_GDP'] = np.log(gdp_le_2019['GDP_Per_Capita'])
28/60: gdp_le_2019.head(5)
28/61: gdp_le_2019['log_GDP'].corr(gdp_le_2019['Life_Expectancy'])
28/62:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
plt.title("Regression plot of the log_GDP growth on Life expectancy")
28/63:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
#tc.head()
#tc.tal(i)
#tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
tc3.info()
tc3.describe()
28/64:
gdp_df_merged_tele = pd.merge(left = gdp_df_merged, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_df_merged_tele.info()
gdp_df_merged_tele.shape
gdp_df_merged_tele.head()
28/65:
gdp_df_merged_tele.sort_values("Percentage_Internet_User", ascending=False).head(10)

#Ireland and Bermida has the highest interntusers
28/66:
#INTERUSER IN 2014
sns.boxplot(data = gdp_df_merged_tele.loc[gdp_df_merged_tele['Year']== 2014], 
            x='Continent', y='Percentage_Internet_User');
28/67:
gdp_le_tele = pd.merge(left = gdp_le, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_le_tele.info()
gdp_le_tele.shape
gdp_le_tele.head()
28/68:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
28/69:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.show(figx)
28/70:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
28/71:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2014.shape #179, 6
gdp_le_tele_2014.head(10)
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
28/72:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth in 2014")
plt.show(figz)
28/73:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="Life_Expectancy", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on Life_Expectancy in 2014")
plt.show(figz)
28/74: a= gdp_le_tele_2014['GDP_Per_Capita'].corr(gdp_le_tele_2014['Life_Expectancy'])
28/75: b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
28/76: c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
28/77:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", c, "and life expectancy by", b, "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
28/78:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="Life_Expectancy", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on Life_Expectancy in 2014")
plt.show(figz)
28/79:
#INTERUSER IN 2014
sns.boxplot(data = gdp_df_merged_tele.loc[gdp_df_merged_tele['Year']== 2014], 
            y='Continent', x='Percentage_Internet_User',
           );
28/80:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
28/81:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
30/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
30/2:
pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
22/1:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
#gdp_df.tail(5)
22/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
30/3:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
30/4:
gdp_df = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
30/5:
gdp_df = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df.info()
gdp_df.shape
gdp_df.columns
30/6:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
gdp_df.shape()
30/7:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
gdp_df.shape
31/1: import re
31/2:
accident_locations_a = [
    'I 24 & UNKNOWN RAMP',
    'MM 93 2 I 65',
    'MM 1 0 I 440'
]
31/3:
for location in accident_locations_a:
    print(re.search(r'# fill this in', location)[0])
31/4:
for location in accident_locations_a:
    print(re.search(r'I' \d, location)[0])
31/5:
for location in accident_locations_a:
    print(re.search(r'I \d', location)[0])
31/6:
for location in accident_locations_a:
    print(re.search(r'I \d [0-9]', location)[0])
31/7:
for location in accident_locations_a:
    print(re.search(r'I \d\d', location)[0])
31/8:
for location in accident_locations_a:
    print(re.search(r'I \d+', location)[0])
31/9:
for location in accident_locations_a:
    print(re.search(r'I \d (2, 3)', location)[0])
31/10:
for location in accident_locations_a:
    print(re.search(r'I \d(2, 3)', location)[0])
31/11:
for location in accident_locations_a:
    print(re.search(r'I \d\d\d', location)[0])
31/12:
for location in accident_locations_a:
    print(re.search(r'I \d\d\d', location)[0])
31/13:
for location in accident_locations_a:
    print(re.search(r'I \d\d', location)[0])
31/14:
for location in accident_locations_b:
    print(re.search(r'I\S\d+', location)[0])
31/15:
for location in accident_locations_b:
    print(re.search(r'I-\S\d+', location)[0])
31/16:
for location in accident_locations_b:
    print(re.search(r'I-\d+', location)[0])
31/17:
for location in accident_locations_c:
    print(re.search(r'I\d+', location)[0])
31/18:
for location in accident_locations_a:
    print(re.search(r'I \d+
                    ', location)[0]) # \d+ any number after the alphabet I
31/19:
for location in accident_locations_a:
    print(re.search(r'I \d+', location)[0]) # \d+ any number after the alphabet I
31/20:
for location in accident_locations_c:
    print(re.search(r'I \d+', location)[0])
31/21:
accident_locations_c = [
    'MM 1 0 I440',
    'I40 WEST EXIT RAMP & DONELSON PIKE',
    'MM 90 0 I65'
]
31/22:
for location in accident_locations_c:
    print(re.search(r'I \d+', location)[0])
32/1: import re
32/2:
accident_locations_a = [
    'I 24 & UNKNOWN RAMP',
    'MM 93 2 I 65',
    'MM 1 0 I 440'
]
32/3:
for location in accident_locations_a:
    print(re.search(r'I \d+', location)[0]) # \d+ any number after the alphabet I
32/4:
accident_locations_b = [
    'WEDGEWOOD & I-65 SOUTH',
    'MM 49 0 I-24 WEST',
    'MM 4 0 I-440 W'
]
32/5:
for location in accident_locations_b:
    print(re.search(r'I-\d+', location)[0])
32/6:
accident_locations_c = [
    'MM 1 0 I440',
    'I40 WEST EXIT RAMP & DONELSON PIKE',
    'MM 90 0 I65'
]
32/7:
for location in accident_locations_c:
    print(re.search(r'I \d+', location)[0])
32/8:
for location in full_accident_locations:
    print(re.search(r'I ?-?\d+', location)[0])
32/9:
full_accident_locations = [
    'I 24 & UNKNOWN RAMP',
    'MM 93 2 I 65',
    'MM 1 0 I 440',
    'WEDGEWOOD & I-65 SOUTH',
    'MM 49 0 I-24 WEST',
    'MM 4 0 I-440 W',
    'MM 1 0 I440',
    'I40 WEST EXIT RAMP & DONELSON PIKE',
    'MM 90 0 I65'
]
32/10:
for location in full_accident_locations:
    print(re.search(r'I ?-?\d+', location)[0])
32/11:
accident_locations_c = [
    'MM 1 0 I440',
    'I40 WEST EXIT RAMP & DONELSON PIKE',
    'MM 90 0 I65'
]
32/12:
for location in accident_locations_c:
    print(re.search(r'I \d+', location)[0])
32/13:
for location in accident_locations_c:
    print(re.search(r'I\d+', location)[0])
33/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
33/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
33/3:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
33/4:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
33/5:
#Drop the 'Value Footnotes' column, 

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
33/6:
##After droping Value Footnote column rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
33/7:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
33/8:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
33/9:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
33/10: gdp_df_for_analysis.info()
33/11:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
33/12:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
33/13: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
33/14: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
33/15: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
33/16:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
33/17:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
#gdp_2014.shape #(238, 2)
33/18: gdp_2014['GDP_Per_Capita'].describe()
33/19:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
33/20: gdp_2014.sort_values('GDP_Per_Capita').head(5)
33/21: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
33/22:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.shape
gdp_1990_2017.head()
33/23:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()

#Create the gdp_pivoted df from the .loc df with 1990 and 2017 year and using the .dropna() method to delete the NaN row.
33/24: gdp_pivoted.head()
33/25: gdp_pivoted.describe()
33/26:
gdp_pivoted.shape
gdp_pivoted.columns
33/27: gdp_pivoted.info()
33/28:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
33/29: gdp_pivoted.sort_values("Percent_Change").head(10)
33/30: gdp_pivoted.sort_values("Percent_Change").tail(10)
33/31:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count

sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
33/32: gdp_pivoted.sort_values("Percent_Change", ascending=False).head(10)
33/33:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
33/34:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
33/35:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
33/36:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
33/37:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
33/38:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
33/39:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
33/40:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
33/41: #gdp_df_merged['Continent'].nunique()
33/42: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
33/43:
#scratch work
gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
33/44:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita');

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
33/45:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
33/46:
#df of the life expectancy data, looking at the file in CSV it show that there are few top rows which are note, that is why I skiped first 4 rows at the time of reading the file.As it was showing error upon reading the csv file**

life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
33/47:
#droping the columns and making the table transpose. Column to rows.
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
33/48:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
33/49:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').head(10)
33/50:
#gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
33/51:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year']) 
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape #16821, 5
gdp_le.info()
33/52:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
33/53: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=False)
33/54:
#Scratch work
#Subsetting the df for top three countries
gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))]
33/55:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
33/56:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
33/57:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
33/58: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
33/59: gdp_le_2019['log_GDP'] = np.log(gdp_le_2019['GDP_Per_Capita'])
33/60: gdp_le_2019.head(5)
33/61: gdp_le_2019['log_GDP'].corr(gdp_le_2019['Life_Expectancy'])
33/62:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
plt.title("Regression plot of the log_GDP growth on Life expectancy")
33/63:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
#tc.head()
#tc.tal(i)
#tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
tc3.info()
tc3.describe()
33/64:
gdp_df_merged_tele = pd.merge(left = gdp_df_merged, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_df_merged_tele.info()
gdp_df_merged_tele.shape
gdp_df_merged_tele.head()
33/65:
gdp_df_merged_tele.sort_values("Percentage_Internet_User", ascending=False).head(10)

#Ireland and Bermida has the highest interntusers
33/66:
#INTERUSER IN 2014
sns.boxplot(data = gdp_df_merged_tele.loc[gdp_df_merged_tele['Year']== 2014], 
            y='Continent', x='Percentage_Internet_User',
           );
33/67:
gdp_le_tele = pd.merge(left = gdp_le, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_le_tele.info()
gdp_le_tele.shape
gdp_le_tele.head()
33/68:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
33/69:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.show(figx)
33/70:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
33/71:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2014.shape #179, 6
gdp_le_tele_2014.head(10)
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
33/72:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth in 2014")
plt.show(figz)
33/73:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="Life_Expectancy", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on Life_Expectancy in 2014")
plt.show(figz)
33/74: a= gdp_le_tele_2014['GDP_Per_Capita'].corr(gdp_le_tele_2014['Life_Expectancy'])
33/75: b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
33/76: c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
33/77:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", c, "and life expectancy by", b, "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
33/78: gdp_le_tele_2014.corr()
34/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
34/2:
#Reread the .csv with nrows
gdp_df = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df.info()
gdp_df.shape
gdp_df.columns
33/79: gdp_le_tele.plot(kind = 'scatter', x = 'GDP_Per_Capita', y = 'Percentage_Internet_User');
33/80:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland', 'India']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
33/81:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland'']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
33/82:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
33/83:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.legend(bbox_to_anchor =(1.0, 1.0)); 
plt.show(figx)
33/84:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
33/85:
  gdp_le_tele['log_gdp'] = np.log(gdp_le_tele['GDP_Per_Capita'])
    gdp_le_tele()
33/86:
  gdp_le_tele['log_gdp'] = np.log(gdp_le_tele['GDP_Per_Capita'])
    gdp_le_tele.head()
33/87: np.log(gdp_le_tele['GDP_Per_Capita'])
33/88:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_User", y=np.log(gdp_le_tele['GDP_Per_Capita']), hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
33/89: gdp_le_2019['log_gdp2'] = np.log(gdp_le_tele['GDP_Per_Capita'])
33/90:
gdp_le_tele['log_gdp2'] = np.log(gdp_le_tele['GDP_Per_Capita'])
gd
33/91:
gdp_le_tele['log_gdp2'] = np.log(gdp_le_tele['GDP_Per_Capita'])
gdp_le_tele.head()
33/92:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="log_gdp2", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.legend(bbox_to_anchor =(1.0, 1.0)); 
plt.show(figx)
33/93:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(    gdp_le_tele, col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
33/94:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(    gdp_le_tele, col ="Cnotinent")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
35/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
35/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
35/3:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
35/4:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
35/5:
#Drop the 'Value Footnotes' column, 

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
35/6:
##After droping Value Footnote column rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
35/7:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
35/8:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
35/9:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
35/10: gdp_df_for_analysis.info()
35/11:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
35/12:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
35/13: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
35/14: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
35/15: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
35/16:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
35/17:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
#gdp_2014.shape #(238, 2)
35/18: gdp_2014['GDP_Per_Capita'].describe()
35/19:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
35/20: gdp_2014.sort_values('GDP_Per_Capita').head(5)
35/21: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
35/22:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.shape
gdp_1990_2017.head()
35/23:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()

#Create the gdp_pivoted df from the .loc df with 1990 and 2017 year and using the .dropna() method to delete the NaN row.
35/24: gdp_pivoted.head()
35/25: gdp_pivoted.describe()
35/26:
gdp_pivoted.shape
gdp_pivoted.columns
35/27: gdp_pivoted.info()
35/28:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
35/29: gdp_pivoted.sort_values("Percent_Change").head(10)
35/30: gdp_pivoted.sort_values("Percent_Change").tail(10)
35/31:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count

sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
35/32: gdp_pivoted.sort_values("Percent_Change", ascending=False).head(10)
35/33:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
35/34:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
35/35:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
35/36:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
35/37:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
35/38:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
35/39:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
35/40:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
35/41: #gdp_df_merged['Continent'].nunique()
35/42: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
35/43:
#scratch work
gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
35/44:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita');

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
35/45:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
35/46:
#df of the life expectancy data, looking at the file in CSV it show that there are few top rows which are note, that is why I skiped first 4 rows at the time of reading the file.As it was showing error upon reading the csv file**

life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
35/47:
#droping the columns and making the table transpose. Column to rows.
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
35/48:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
35/49:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').head(10)
35/50:
#gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
35/51:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year']) 
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape #16821, 5
gdp_le.info()
35/52:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
35/53: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=False)
35/54:
#Scratch work
#Subsetting the df for top three countries
gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))]
35/55:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
35/56:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
35/57:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
35/58: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
35/59: gdp_le_2019['log_GDP'] = np.log(gdp_le_2019['GDP_Per_Capita'])
35/60: gdp_le_2019.head(5)
35/61: gdp_le_2019['log_GDP'].corr(gdp_le_2019['Life_Expectancy'])
35/62:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
plt.title("Regression plot of the log_GDP growth on Life expectancy")
35/63:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
#tc.head()
#tc.tal(i)
#tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
tc3.info()
tc3.describe()
35/64:
gdp_df_merged_tele = pd.merge(left = gdp_df_merged, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_df_merged_tele.info()
gdp_df_merged_tele.shape
gdp_df_merged_tele.head()
35/65:
gdp_df_merged_tele.sort_values("Percentage_Internet_User", ascending=False).head(10)

#Ireland and Bermida has the highest interntusers
35/66:
#INTERUSER IN 2014
sns.boxplot(data = gdp_df_merged_tele.loc[gdp_df_merged_tele['Year']== 2014], 
            y='Continent', x='Percentage_Internet_User',
           );
35/67:
gdp_le_tele = pd.merge(left = gdp_le, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_le_tele.info()
gdp_le_tele.shape
gdp_le_tele.head()
35/68:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
35/69:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.legend(bbox_to_anchor =(1.0, 1.0)); 
plt.show(figx)
35/70:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
35/71:
gdp_le_tele['log_gdp2'] = np.log(gdp_le_tele['GDP_Per_Capita'])
gdp_le_tele.head()
35/72:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="log_gdp2", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.legend(bbox_to_anchor =(1.0, 1.0)); 
plt.show(figx)
35/73:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2014.shape #179, 6
gdp_le_tele_2014.head(10)
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
35/74:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth in 2014")
plt.show(figz)
35/75:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="Life_Expectancy", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on Life_Expectancy in 2014")
plt.show(figz)
35/76: a= gdp_le_tele_2014['GDP_Per_Capita'].corr(gdp_le_tele_2014['Life_Expectancy'])
35/77: b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
35/78: c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
35/79:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", c, "and life expectancy by", b, "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
35/80: gdp_le_tele_2014.corr()
35/81: gdp_le_tele.head(5)
35/82:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
35/83:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
35/84:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
35/85:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
35/86:
#Drop the 'Value Footnotes' column, 

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
35/87:
##After droping Value Footnote column rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
35/88:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
35/89:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
35/90:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
35/91: gdp_df_for_analysis.info()
35/92:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
35/93:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
35/94: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
35/95: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
35/96: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
35/97:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
35/98:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
#gdp_2014.shape #(238, 2)
35/99: gdp_2014['GDP_Per_Capita'].describe()
35/100:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
35/101: gdp_2014.sort_values('GDP_Per_Capita').head(5)
35/102: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
35/103:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.shape
gdp_1990_2017.head()
35/104:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()

#Create the gdp_pivoted df from the .loc df with 1990 and 2017 year and using the .dropna() method to delete the NaN row.
35/105: gdp_pivoted.head()
35/106: gdp_pivoted.describe()
35/107:
gdp_pivoted.shape
gdp_pivoted.columns
35/108: gdp_pivoted.info()
35/109:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
35/110: gdp_pivoted.sort_values("Percent_Change").head(10)
35/111: gdp_pivoted.sort_values("Percent_Change").tail(10)
35/112:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count

sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
35/113: gdp_pivoted.sort_values("Percent_Change", ascending=False).head(10)
35/114:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
35/115:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
35/116:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
35/117:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
35/118:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
35/119:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
35/120:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
35/121:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
35/122: #gdp_df_merged['Continent'].nunique()
35/123: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
35/124:
#scratch work
gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
35/125:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita');

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
35/126:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
35/127:
#df of the life expectancy data, looking at the file in CSV it show that there are few top rows which are note, that is why I skiped first 4 rows at the time of reading the file.As it was showing error upon reading the csv file**

life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
35/128:
#droping the columns and making the table transpose. Column to rows.
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
35/129:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
35/130:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').head(10)
35/131:
#gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
35/132:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year']) 
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape #16821, 5
gdp_le.info()
35/133:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
35/134: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=False)
35/135:
#Scratch work
#Subsetting the df for top three countries
gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))]
35/136:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
35/137:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
35/138:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
35/139: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
35/140: gdp_le_2019['log_GDP'] = np.log(gdp_le_2019['GDP_Per_Capita'])
35/141: gdp_le_2019.head(5)
35/142: gdp_le_2019['log_GDP'].corr(gdp_le_2019['Life_Expectancy'])
35/143:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
plt.title("Regression plot of the log_GDP growth on Life expectancy")
35/144:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
#tc.head()
#tc.tal(i)
#tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
tc3.info()
tc3.describe()
35/145:
gdp_df_merged_tele = pd.merge(left = gdp_df_merged, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_df_merged_tele.info()
gdp_df_merged_tele.shape
gdp_df_merged_tele.head()
35/146:
gdp_df_merged_tele.sort_values("Percentage_Internet_User", ascending=False).head(10)

#Ireland and Bermida has the highest interntusers
35/147:
#INTERUSER IN 2014
sns.boxplot(data = gdp_df_merged_tele.loc[gdp_df_merged_tele['Year']== 2014], 
            y='Continent', x='Percentage_Internet_User',
           );
35/148:
gdp_le_tele = pd.merge(left = gdp_le, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_le_tele.info()
gdp_le_tele.shape
gdp_le_tele.head()
35/149:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
35/150:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.legend(bbox_to_anchor =(1.0, 1.0)); 
plt.show(figx)
35/151:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
35/152:
gdp_le_tele['log_gdp2'] = np.log(gdp_le_tele['GDP_Per_Capita'])
gdp_le_tele.head()
35/153:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="log_gdp2", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.legend(bbox_to_anchor =(1.0, 1.0)); 
plt.show(figx)
35/154:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2014.shape #179, 6
gdp_le_tele_2014.head(10)
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
35/155:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth in 2014")
plt.show(figz)
35/156:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="Life_Expectancy", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on Life_Expectancy in 2014")
plt.show(figz)
35/157: a= gdp_le_tele_2014['GDP_Per_Capita'].corr(gdp_le_tele_2014['Life_Expectancy'])
35/158: b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
35/159: c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
35/160:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", c, "and life expectancy by", b, "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
35/161: gdp_le_tele_2014.corr()
35/162: gdp_le_tele.head(5)
37/1:
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
import seaborn as sns
37/2: derby = pd.read_csv('../data/derbyplus.csv')
37/3: derby
37/4: derby['speed'].describe()
37/5:
derby['speed'].hist(edgecolor = 'black',
                    bins = 25);
37/6: derby['starters'].hist(edgecolor = 'black', bins = 20);
37/7: derby['condition'].value_counts(normalize = True)
37/8: derby.plot(x = 'year', y = 'speed', kind = 'scatter');
37/9: derby.groupby('condition')['speed'].describe()
37/10:
sns.boxplot(data = derby,
           x = 'condition',
           y = 'speed');
37/11: derby.plot(kind = 'scatter', x = 'starters', y = 'speed');
37/12: derby.plot(kind = 'scatter', x = 'year', y = 'starters');
37/13:
lm = smf.ols('speed ~ year', data = derby).fit()
lm.summary()
37/14:
derby.plot(x = 'year', y = 'speed', kind = 'scatter')
plt.plot(derby['year'], lm.fittedvalues, color = 'black');
37/15:
plt.scatter(lm.fittedvalues, lm.resid)
xmin, xmax = plt.xlim()
plt.hlines(y = 0, xmin = xmin, xmax = xmax, color = 'black')
plt.xlim(xmin, xmax);
37/16:
lm_quad = smf.ols('speed ~ year + I(year**2)', data = derby).fit()
lm_quad.summary()
37/17:
derby.plot(x = 'year', y = 'speed', kind = 'scatter')
plt.plot(derby['year'], lm_quad.fittedvalues, color = 'black');
37/18:
plt.scatter(lm_quad.fittedvalues, lm_quad.resid)
xmin, xmax = plt.xlim()
plt.hlines(y = 0, xmin = xmin, xmax = xmax, color = 'black')
plt.xlim(xmin, xmax);
37/19: sm.stats.anova_lm(lm, lm_quad)
37/20: derby['year_z'] = (derby['year'] - derby['year'].mean()) / derby['year'].std()
37/21:
lm_quad = smf.ols('speed ~ year_z + I(year_z**2)', data = derby).fit()
lm_quad.summary()
37/22:
lm_cubic = smf.ols('speed ~ year_z + I(year_z**2) + I(year_z**3)', data = derby).fit()
lm_cubic.summary()
37/23:
derby.plot(x = 'year', y = 'speed', kind = 'scatter')
plt.plot(derby['year'], lm_quad.fittedvalues, color = 'blue', label = 'quadratic')
plt.plot(derby['year'], lm_cubic.fittedvalues, color = 'red', label = 'cubic')
plt.legend();
37/24: sm.stats.anova_lm(lm_quad, lm_cubic)
37/25:
lm_condition = smf.ols('speed ~ year_z + I(year_z**2) + condition', data = derby).fit()
lm_condition.summary()
37/26: sm.stats.anova_lm(lm_quad, lm_condition)
37/27:
yearmin, yearmax = derby['year'].min(), derby['year'].max()
yearzmin, yearzmax = derby['year_z'].min(), derby['year_z'].max()

colors = ['blue', 'orange', 'black']

ax = sns.scatterplot(data = derby,
                     x = 'year',
                     y = 'speed',
                     hue = 'condition',
                     palette = colors)
for condition, color in zip(['good', 'slow', 'fast'], colors):
    pred = pd.DataFrame({
            'year': np.linspace(start = yearmin, stop = yearmax, num = 50),
            'year_z': np.linspace(start = yearzmin, stop = yearzmax, num = 50),
            'condition': condition
        })
    pred['prediction'] = lm_condition.predict(pred)
    
    pred.plot(x = 'year', y = 'prediction', color = color, ax = ax, label = '')

plt.legend();
37/28:
lm_interaction = smf.ols('speed ~ year_z + I(year_z**2) + condition + (year_z + I(year_z**2)):condition', data = derby).fit()
lm_interaction.summary()
37/29: sm.stats.anova_lm(lm_condition, lm_interaction)
37/30:
yearmin, yearmax = derby['year'].min(), derby['year'].max()
yearzmin, yearzmax = derby['year_z'].min(), derby['year_z'].max()

colors = ['blue', 'orange', 'black']

ax = sns.scatterplot(data = derby,
                     x = 'year',
                     y = 'speed',
                     hue = 'condition',
                     palette = colors)

for condition, color in zip(['good', 'slow', 'fast'], colors):
    pred = pd.DataFrame({
            'year': np.linspace(start = yearmin, stop = yearmax, num = 50),
            'year_z': np.linspace(start = yearzmin, stop = yearzmax, num = 50),
            'condition': condition
        })
    pred['prediction'] = lm_interaction.predict(pred)
    
    pred.plot(x = 'year', y = 'prediction', color = color, ax = ax, label = '')

plt.legend();
37/31:
lm_starters = smf.ols('speed ~ year_z + I(year_z**2) + condition + starters + (year_z + I(year_z**2)):condition', data = derby).fit()
lm_starters.summary()
37/32: sm.stats.anova_lm(lm_interaction, lm_starters)
42/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
42/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
42/3:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
42/4:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
42/5:
#Drop the 'Value Footnotes' column, 

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
42/6:
##After droping Value Footnote column rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
42/7:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
42/8:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
42/9:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
42/10: gdp_df_for_analysis.info()
42/11:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
42/12:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
42/13: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
42/14: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
42/15: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
42/16:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
42/17:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
#gdp_2014.shape #(238, 2)
42/18: gdp_2014['GDP_Per_Capita'].describe()
42/19:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
42/20: gdp_2014.sort_values('GDP_Per_Capita').head(5)
42/21: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
42/22:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.shape
gdp_1990_2017.head()
42/23:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()

#Create the gdp_pivoted df from the .loc df with 1990 and 2017 year and using the .dropna() method to delete the NaN row.
42/24: gdp_pivoted.head()
42/25: gdp_pivoted.describe()
42/26:
gdp_pivoted.shape
gdp_pivoted.columns
42/27: gdp_pivoted.info()
42/28:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
42/29: gdp_pivoted.sort_values("Percent_Change").head(10)
42/30: gdp_pivoted.sort_values("Percent_Change").tail(10)
42/31:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count

sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
42/32: gdp_pivoted.sort_values("Percent_Change", ascending=False).head(10)
42/33:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
42/34:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
42/35:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
42/36:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
42/37:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
42/38:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
42/39:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
42/40:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
42/41: #gdp_df_merged['Continent'].nunique()
42/42: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
42/43:
#scratch work
gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
42/44:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita');

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
42/45:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
42/46:
#df of the life expectancy data, looking at the file in CSV it show that there are few top rows which are note, that is why I skiped first 4 rows at the time of reading the file.As it was showing error upon reading the csv file**

life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
42/47:
#droping the columns and making the table transpose. Column to rows.
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
42/48:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
42/49:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').head(10)
42/50:
#gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
42/51:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year']) 
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape #16821, 5
gdp_le.info()
42/52:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
42/53: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=False)
42/54:
#Scratch work
#Subsetting the df for top three countries
gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))]
42/55:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
42/56:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
42/57:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
42/58: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
42/59: gdp_le_2019['log_GDP'] = np.log(gdp_le_2019['GDP_Per_Capita'])
42/60: gdp_le_2019.head(5)
42/61: gdp_le_2019['log_GDP'].corr(gdp_le_2019['Life_Expectancy'])
42/62:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
plt.title("Regression plot of the log_GDP growth on Life expectancy")
42/63:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
#tc.head()
#tc.tal(i)
#tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
tc3.info()
tc3.describe()
42/64:
gdp_df_merged_tele = pd.merge(left = gdp_df_merged, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_df_merged_tele.info()
gdp_df_merged_tele.shape
gdp_df_merged_tele.head()
42/65:
gdp_df_merged_tele.sort_values("Percentage_Internet_User", ascending=False).head(10)

#Ireland and Bermida has the highest interntusers
42/66:
#INTERUSER IN 2014
sns.boxplot(data = gdp_df_merged_tele.loc[gdp_df_merged_tele['Year']== 2014], 
            y='Continent', x='Percentage_Internet_User',
           );
42/67:
gdp_le_tele = pd.merge(left = gdp_le, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_le_tele.info()
gdp_le_tele.shape
gdp_le_tele.head()
42/68:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
42/69:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.legend(bbox_to_anchor =(1.0, 1.0)); 
plt.show(figx)
42/70:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
42/71:
gdp_le_tele['log_gdp2'] = np.log(gdp_le_tele['GDP_Per_Capita'])
gdp_le_tele.head()
42/72:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="log_gdp2", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.legend(bbox_to_anchor =(1.0, 1.0)); 
plt.show(figx)
42/73:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2014.shape #179, 6
gdp_le_tele_2014.head(10)
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
42/74:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth in 2014")
plt.show(figz)
42/75:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="Life_Expectancy", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on Life_Expectancy in 2014")
plt.show(figz)
42/76: a= gdp_le_tele_2014['GDP_Per_Capita'].corr(gdp_le_tele_2014['Life_Expectancy'])
42/77: b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
42/78: c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
42/79:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", c, "and life expectancy by", b, "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
42/80: gdp_le_tele_2014.corr()
42/81: gdp_le_tele.head(5)
42/82: import bar_chart_race as bcr
42/83: import bar_chart_race as bcr
44/1:
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import numpy as np
import matplotlib.pyplot as plt
44/2: gdp_le = pd.read_csv('../data/gdp_le.csv').dropna()
44/3: gdp_le
44/4: gdp_le_2019 = gdp_le[gdp_le['Year'] == 2019].copy()
44/5:
lm = # Fill this in
lm.summary()
44/6:
gdp_le_2019['GDP_thousdans'] = gdp_le_2019['GDP_thousdans']/1000
gdp_pivoted.head()
44/7:
gdp_le_2019['GDP_thousdans'] = gdp_le_2019['GDP_thousdans']/1000
gdp_le_2019.head()
44/8:
gdp_le_2019['GDP_thousdans'] = (gdp_le_2019['GDP_thousdans'])/1000
gdp_le_2019.head()
44/9:
gdp_le_2019['GDP_thousdans'] = (gdp_le_2019('GDP_thousdans')/1000
gdp_le_2019.head()
44/10:
gdp_le_2019['GDP_thousdans'] = (gdp_le_2019('GDP_thousdans'))/1000
gdp_le_2019.head()
44/11:
gdp_le_2019['GDP_thousdans'] = gdp_le_2019['GDP_thousdans']/1000
gdp_le_2019.head()
44/12: gdp_le_2019 = gdp_le[gdp_le['Year'] == 2019].copy()
44/13:
gdp_le_2019 = gdp_le[gdp_le['Year'] == 2019].copy()
gdp_le_2019.head()
44/14:
gdp_le_2019['GDP_thousdans'] = gdp_le_2019['GDP_Per_Capita']/1000
gdp_le_2019.head()
44/15:
lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
lm.summary()

3lm = # Fill this in
#lm.summary()
44/16:
lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
lm.summary()

#lm = # Fill this in
#lm.summary()
44/17:
ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
44/18:
#linear regression model
lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
lm.summary()
44/19:
#linear regression model
#lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
lm.summary()
44/20:
#linear regression model
#lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
#lm.summary()
44/21:
#linear regression model
#lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
#lm.summary()
44/22:
#linear regression model
lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
lm.summary()
44/23:
#creating and use thousands of dollars of GDP_Per_Capita as a new predictor variable for life expectancy
gdp_le_2019['GDP_thousdans'] = gdp_le_2019['GDP_Per_Capita']/1000
gdp_le_2019.head()
44/24:
#linear regression model
lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
lm.summary()

ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
44/25:
#linear regression model
lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
print(lm.summary())
ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
44/26:
#creating and use thousands of dollars of GDP_Per_Capita as a new predictor variable for life expectancy
gdp_le_2019['GDP_Thousdans'] = gdp_le_2019['GDP_Per_Capita']/1000
gdp_le_2019.head()
44/27:
lm = smf.ols('Life_Expectancy ~ GDP_Thousdans', data = gdp_le_2019).fit()
lm.summary()
44/28:
lm = smf.ols('Life_Expectancy ~ GDP_Thousdans', data = gdp_le_2019).fit()
print(lm.summary())

ax = gdp_le_2019.plot(x = 'GDP_Thousands', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Thousands': np.linspace(
        start = gdp_le_2019['GDP_Thousands'].min(),
        stop = gdp_le_2019['GDP_Thousands'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Thousands', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
44/29:
#creating and use thousands of dollars of GDP_Per_Capita as a new predictor variable for life expectancy
gdp_le_2019['GDP_Thousands'] = gdp_le_2019['GDP_Per_Capita']/1000
gdp_le_2019.head()
44/30:
#creating and use thousands of dollars of GDP_Per_Capita as a new predictor variable for life expectancy
gdp_le_2019['GDP_Thousands'] = gdp_le_2019['GDP_Per_Capita']/1000
gdp_le_2019.head()
44/31:
lm = smf.ols('Life_Expectancy ~ GDP_Thousands', data = gdp_le_2019).fit()
print(lm.summary())

ax = gdp_le_2019.plot(x = 'GDP_Thousands', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Thousands': np.linspace(
        start = gdp_le_2019['GDP_Thousands'].min(),
        stop = gdp_le_2019['GDP_Thousands'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Thousands', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
44/32:
ax = gdp_le_2019.plot(x = 'GDP_Thousands', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Thousands': np.linspace(
        start = gdp_le_2019['GDP_Thousands'].min(),
        stop = gdp_le_2019['GDP_Thousands'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Thousands', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
44/33:
lm = smf.ols('Life_Expectancy ~ GDP_Thousands', data = gdp_le_2019).fit()
print(lm.summary())
44/34:
ax = gdp_le_2019.plot(x = 'GDP_Thousands', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Thousands': np.linspace(
        start = gdp_le_2019['GDP_Thousands'].min(),
        stop = gdp_le_2019['GDP_Thousands'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Thousands', y = 'fitted', ax = ax, color = 'red', label = 'fitted');
44/35:
ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'red', label = 'fitted');
44/36:
ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
44/37:
#linear regression model
lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
print(lm.summary())

ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
45/1:
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import numpy as np
import matplotlib.pyplot as plt
45/2: gdp_le = pd.read_csv('../data/gdp_le.csv').dropna()
45/3: gdp_le
45/4:
gdp_le_2019 = gdp_le[gdp_le['Year'] == 2019].copy()
gdp_le_2019.head()
45/5:
#linear regression model
lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
print(lm.summary())

ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
45/6:
ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
45/7:
#creating and use thousands of dollars of GDP_Per_Capita as a new predictor variable for life expectancy
gdp_le_2019['GDP_Thousands'] = gdp_le_2019['GDP_Per_Capita']/1000
gdp_le_2019.head()
45/8:
lm = smf.ols('Life_Expectancy ~ GDP_Thousands', data = gdp_le_2019).fit()
print(lm.summary())
45/9:
ax = gdp_le_2019.plot(x = 'GDP_Thousands', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Thousands': np.linspace(
        start = gdp_le_2019['GDP_Thousands'].min(),
        stop = gdp_le_2019['GDP_Thousands'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Thousands', y = 'fitted', ax = ax, color = 'red', label = 'fitted');
45/10:
lm = # Fill this in
lm.summary()
45/11:
#linear regression model
lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
print(lm.summary())

ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'red', label = 'fitted');
45/12:
ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'red', label = 'fitted');
45/13:
lm = smf.ols('Life_Expectancy ~ np.log(GDP_Per_Capita', data = gdp_le_2019).fit()
print(lm.summary())
#lm.summary()
45/14:
lm = smf.ols('Life_Expectancy ~ np.log(GDP_Per_Capita)', data = gdp_le_2019).fit()
print(lm.summary())
#lm.summary()
45/15:
gdp_le_2019['log_gdp'] = np.log(gdp_le_2019['GDP_Per_Capita'])

ax = gdp_le_2019.plot(x = 'log_gdp', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['log_gdp'] = np.log(fit_df['GDP_Per_Capita'])
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'log_gdp', y = 'fitted', ax = ax, color = 'red', label = 'fitted');
45/16:
lm = smf.ols('Life_Expectancy ~ Year + I(Year**2)', data = derby).fit()
lm.summary()
45/17:
lm = smf.ols('Life_Expectancy ~ Year + I(Year**2)', data = gdp_le).fit()
lm.summary()
45/18:
lm = smf.ols('Life_Expectancy ~ I(GDP_Per_Capita/1000)', data = gdp_le).fit()
lm.summary()
47/1: import requests
47/2:
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
% matplotlib inline
import seaborn as sns
import datetime
47/3:
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl


% matplotlib inline

import seaborn as sns
import datetime
48/1:
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl


% matplotlib inline

import seaborn as sns
import datetime
48/2:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl


%matplotlib inline

import seaborn as sns
import datetime
48/3:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
48/4: type(response)
48/5: response.status_code
48/6: response.text
48/7: soup = BS(response.text)
48/8: print(soup.prettify())
48/9: soup.find('title')
48/10: soup.find('table', attrs={'class' : 'Members--table'})
48/11: table1 = soup.find('table', attrs={'class' : 'Members--table'})
48/12:
from IPython.core.display import HTML

HTML(table1)
48/13: table1 = soup.find('table', attrs={'class' : 'Members--table'})
48/14:
from IPython.core.display import HTML

HTML(table1)
48/15: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))[0]
48/16:
table1 = soup.find('table', attrs={'class' : 'Members--table'})
from IPython.core.display import HTML

HTML(table1)
48/17: table1 = soup.find('table', attrs={'class' : 'Members--table'})
48/18: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))[0]
48/19:
table1 = soup.find('table', attrs={'class' : 'Members--table'})
type(table1)
48/20:
table1 = soup.find('table', attrs={'class' : 'Members--table'})
table1
48/21: pd.read_html(str(soup.find('div', attrs={'class' : 'Members--table'})))[0]
48/22: pd.read_html(str(soup.find('div', attrs={'class' : 'Members--bio u-richtext'})))[0]
48/23: soup.find('div', attrs={'class' : 'Members--bio u-richtext'})))[0]
48/24: soup.find('div', attrs={'class' : 'Members--bio u-richtext'})[0]
48/25: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))
48/26: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))[0]
48/27: soup.find('div', attrs={'class' : 'Members--bio u-richtext'})
48/28: soup.find('span', attrs={'class' : 'Members--vote-pct'})
48/29: pd.read_html(str(soup.find('span', attrs={'class' : 'Members--vote-pct'})))
48/30:
import html5lib
pd.read_html(str(soup.find('span', attrs={'class' : 'Members--vote-pct'})))
48/31:
import html5lib
html5lib.parse(str(soup.find('span', attrs={'class' : 'Members--vote-pct'})))
48/32: soup.find('span', attrs={'class' : 'Members--vote-pct'})
48/33:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
pct_vote
48/34:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})[0]
pct_vote
48/35:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
pct_vote
48/36:
pct_vote = [soup.find('span', attrs={'class' : 'Members--vote-pct'})]
pct_vote
48/37:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'}).text()
pct_vote
48/38:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'}).text()
pct_vote

soup.findAll('div')[2].find('h1').text
48/39:
#pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'}).text()
#pct_vote

soup.findAll('div')[2].find('h1').text
48/40:
#pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'}).text()
#pct_vote

soup.findAll('div')[2].text
48/41:
#pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'}).text()
#pct_vote

soup.findAll('div')[1].text
48/42:
#pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'}).text()
#pct_vote

soup.findAll('div')
48/43:
#pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'}).text()
#pct_vote

soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
48/44:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'}).text()
pct_vote

#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
48/45:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
pct_vote

#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
48/46:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
type(pct_vote)

#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
48/47:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
pct_vote

#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
49/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl


%matplotlib inline

import seaborn as sns
import datetime
49/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
49/3: type(response)
49/4: response.status_code
49/5: response.text
49/6: soup = BS(response.text)
49/7: print(soup.prettify())
49/8: soup.find('title')
49/9:
table1 = soup.find('table', attrs={'class' : 'Members--table'})
table1
49/10: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))[0]
49/11:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
pct_vote

#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
49/12: /html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div[1]
49/13:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
pct_vote.str.extract('(\d+)')

#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
49/14:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
#pct_vote.str.extract('(\d+)')
[x.get('span') for x in pct_vote]
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
49/15:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
#pct_vote.str.extract('(\d+)')

type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
50/1:
squares = []
    for i in range {1, 2}
    squares.append(i**2)
    
squares
50/2:
squares = []
for i in range {1, 2}
    squares.append(i**2)
    
squares
50/3:
squares = []
for i in range {1, 2},
    squares.append(i**2)
    
squares
50/4:
squares = []
for i in range {1, 2}:
    squares.append(i**2)
    
squares
50/5:
squares = []
for i in range[1, 2]:
    squares.append(i**2)
    
squares
50/6:
squares = []
for i in range(1, 2):
    squares.append(i**2)
    
squares
50/7:
squares = []
for i in range(0,16):
    squares.append(i**2)
    
squares
50/8:
squares = []
for i in range(1,16):
    squares.append(i**2)
    
squares
50/9:
squares = []
for i in range(1,16):     #iterates from 1 to a number less than 16
    squares.append(i**2)
    
squares
50/10:
squares = [i**2 for i in range(1:16) ]
squares
    
    
  #  x for x in fruits if x != "apple"
50/11:
squares = [i**2 for i in range(1,16) ]
squares
    
    
  #  x for x in fruits if x != "apple"
50/12:
def square(x):
    return x**2
50/13:
squares = [square(i) for i in range(1,16) ]
squares
51/1:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
#pct_vote.str.extract('(\d+)')
pct_vote
#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl


%matplotlib inline

import seaborn as sns
import datetime
52/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
52/3: type(response)
52/4: response.status_code
52/5: response.text
52/6: soup = BS(response.text)
52/7: print(soup.prettify())
52/8: soup.find('title')
52/9:
table1 = soup.find('table', attrs={'class' : 'Members--table'})
table1
52/10: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))[0]
52/11:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
#pct_vote.str.extract('(\d+)')
pct_vote
#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/12: /html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div[1]
52/13:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
#pct_vote.str.extract('(\d+)')
pct_vote
type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/14: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))[1]
52/15: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))[0]
52/16:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
#pct_vote.str.extract('(\d+)')
pct_vote
#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/17:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})
#pct_vote.str.extract('(\d+)')
pct_vote.text
#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/18:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

re.findall('\d+', pct_vote)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/19:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

import re
re.findall('\d+', pct_vote)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/20:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

import re
re.findall('\d+', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/21: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))
52/22: soup.findAll('table', attrs={'class' : 'Members--table'})
52/23: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))
52/24: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))[0]
52/25: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))[1]
52/26: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'}))
52/27: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))
52/28:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
52/29:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

import re
#re.findall('\d+', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/30:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

import re
#re.findall('\d+', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/31:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

#import re
#re.findall('\d+', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/32:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

#import re
re.findall('\d*\.\d\%', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/33:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

#import re
re.findall('\d*\.\d\', '%', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/34:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

#import re
re.findall('\d*\.\d\%', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
52/35: soup.find('table', attrs={'class' : 'Members--list-item'})
52/36: soup.findALL('div', attrs={'class' : 'Members--list-item'})
52/37: soup.find('div', attrs={'class' : 'Members--list-item'})
52/38: soup.findAll('div', attrs={'class' : 'Members--list-item'})
52/39: pd.read_html(str(soup.findAll('div', attrs={'class' : 'Members--list-item'})))
52/40: pd.read_html(str(soup.findAll('div', attrs={'class' : 'Members'})))
52/41: pd.read_html(str(soup.findAll('div', attrs={'class' : 'u-richtext u-my4 u-mb4'})))
52/42: pd.read_html(str(soup.findAll('div', attrs={'class' : 'u-richtext u-mt4 u-mb4'})))
52/43: pd.read_html(str(soup.findAll('div', attrs={'class' : 'Members--bio u-richtext'})))
52/44: pd.read_html(str(soup.findAll('div', attrs={'class' : 'Members--bio u-richtext'})))
52/45: pd.read_html(str(soup.find('div', attrs={'class' : 'Members--bio u-richtext'})))
52/46: soup.find('div', attrs={'class' : 'Members--bio u-richtext'})
52/47: soup.find('div', attrs={'class' : 'Members--bio u-richtext'}).text
52/48: soup.findAll('div', attrs={'class' : 'Members--bio u-richtext'}).text
52/49: soup.find_all('div', attrs={'class' : 'Members--bio u-richtext'}).text
52/50: soup.find('div', attrs={'class' : 'Members--bio u-richtext'}).text
52/51: soup.find('table', attrs={'class' : 'Members--table'}).text #findAll find_all is not working
52/52: soup.find('table', attrs={'class' : 'Members--table'}) #findAll find_all is not working
52/53: soup.find('table', attrs={'class' : 'Members--table'}).text #findAll find_all is not working
52/54: soup.find('div', attrs={'class' : 'Members--list-item'}).text #findAll find_all is not working
52/55: pd.read_html(soup.find('div', attrs={'class' : 'Members--list-item'})) #.text #findAll find_all is not working
52/56: pd.read_html(str(soup.find('div', attrs={'class' : 'Members--list-item'})) #.text #findAll find_all is not working
52/57: soup.find('div', attrs={'class' : 'Members--list-item'})) #.text #findAll find_all is not working
52/58: soup.find('div', attrs={'class' : 'Members--list-item'}) #.text #findAll find_all is not working
52/59: list_item = soup.find('div', attrs={'class' : 'Members--list-item'}) #.text #findAll find_all is not working
52/60:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list#.text #findAll find_all is not working
52/61:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text

list_item
#.text #findAll find_all is not working
52/62:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text

type(list_item)
#.text #findAll find_all is not working
52/63: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))[0:3]
52/64: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))[0]
52/65: pd.read_html[(str(soup.findAll('table', attrs={'class' : 'Members--table'})))]
52/66: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))
52/67:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text

list_item
#.text #findAll find_all is not working
52/68: list_item.get_text(strip=True)
52/69:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).get_text()

list_item
#.text #findAll find_all is not working
52/70:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text

list_item
#.text #findAll find_all is not working
52/71:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item
#.text #findAll find_all is not working
52/72:
#list_item.get_text(strip=True)
x = []
for i in list_item:
    x.append(i.get_text(strip=True))
52/73:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item

list_item.split('(')

#.text #findAll find_all is not working
54/1:
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import numpy as np
import matplotlib.pyplot as plt
54/2: gdp_le = pd.read_csv('../data/gdp_le.csv').dropna()
54/3: gdp_le
54/4:
gdp_le_2019 = gdp_le[gdp_le['Year'] == 2019].copy()
gdp_le_2019.head()
54/5:
#linear regression model
lm = smf.ols('Life_Expectancy ~ GDP_Per_Capita', data = gdp_le_2019).fit()
print(lm.summary())

ax = gdp_le_2019.plot(x = 'GDP_Per_Capita', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Per_Capita', y = 'fitted', ax = ax, color = 'red', label = 'fitted');
54/6:
#creating and use thousands of dollars of GDP_Per_Capita as a new predictor variable for life expectancy
gdp_le_2019['GDP_Thousands'] = gdp_le_2019['GDP_Per_Capita']/1000
gdp_le_2019.head()
54/7:
lm = smf.ols('Life_Expectancy ~ GDP_Thousands', data = gdp_le_2019).fit()
print(lm.summary())
54/8:
ax = gdp_le_2019.plot(x = 'GDP_Thousands', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Thousands': np.linspace(
        start = gdp_le_2019['GDP_Thousands'].min(),
        stop = gdp_le_2019['GDP_Thousands'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'GDP_Thousands', y = 'fitted', ax = ax, color = 'red', label = 'fitted');
54/9:
lm = smf.ols('Life_Expectancy ~ np.log(GDP_Per_Capita)', data = gdp_le_2019).fit()
print(lm.summary())
#lm.summary()
54/10:
gdp_le_2019['log_gdp'] = np.log(gdp_le_2019['GDP_Per_Capita'])

ax = gdp_le_2019.plot(x = 'log_gdp', y = 'Life_Expectancy', kind = 'scatter')

fit_df = pd.DataFrame({
    'GDP_Per_Capita': np.linspace(
        start = gdp_le_2019['GDP_Per_Capita'].min(),
        stop = gdp_le_2019['GDP_Per_Capita'].max(), 
        num = 150)})
fit_df['log_gdp'] = np.log(fit_df['GDP_Per_Capita'])
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'log_gdp', y = 'fitted', ax = ax, color = 'red', label = 'fitted');
54/11:
lm = smf.ols('Life_Expectancy ~ I(GDP_Per_Capita/1000)', data = gdp_le).fit()
lm.summary()
54/12:
ax = gdp_le.plot(kind = 'scatter', x = 'Year', y = 'Life_Expectancy', alpha = 0.5)

fit_df = pd.DataFrame({
    'Year': np.linspace(
        start = gdp_le['Year'].min(),
        stop = gdp_le['Year'].max(), 
        num = 150)})
fit_df['fitted'] = lm.predict(fit_df)

fit_df.plot(x = 'Year', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
54/13:
continent = 'Asia'

sub_df = gdp_le[gdp_le['Continent'] == continent]

ax = sub_df.plot(x = 'Year', 
                 y = 'Life_Expectancy',
                 kind = 'scatter',
                 title = continent,
                 alpha = 0.6)

fit_df = pd.DataFrame({
    'Year': np.linspace(start = sub_df['Year'].min(),
                                  stop = sub_df['Year'].max()
                                 ),
    'Continent': continent
})
fit_df['fitted'] = lm_continent.predict(fit_df)
fit_df.plot(x = 'Year', y = 'fitted', ax = ax, color = 'black', label = 'fitted');
52/74:
list_item = soup.findAll('div', attrs={'class' : 'Members--list-item'}).text


list_item

list_item.split('(')

#.text #findAll find_all is not working
52/75:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item

list_item.split('(')

#.text #findAll find_all is not working
52/76:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item

list_item.split('(').string

#.text #findAll find_all is not working
52/77:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item

list_item.split('(')

#.text #findAll find_all is not working
52/78:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text{0}


list_item

list_item.split('(')

#.text #findAll find_all is not working
52/79:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text[0]


list_item

list_item.split('(')

#.text #findAll find_all is not working
52/80:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item

list_item.split('(')

#.text #findAll find_all is not working
52/81:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item

#list_item.split('(')

#.text #findAll find_all is not working
52/82: re.findall(r'\w+', list_item)
52/83: re.finadall(soup.find('div', attrs={'class' : 'Members--bio u-richtext'}).text) #findAll find_all is not working
52/84: re.findall(soup.find('div', attrs={'class' : 'Members--bio u-richtext'}).text) #findAll find_all is not working
52/85: soup.find('div', attrs={'class' : 'Members--bio u-richtext'}).text #findAll find_all is not working
57/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
57/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
57/3: type(response)
57/4: response.status_code
57/5: response.text
57/6: soup = BS(response.text)
57/7: print(soup.prettify())
57/8: soup.find('title')
57/9:
table1 = soup.find('table', attrs={'class' : 'Members--table'})
table1
57/10: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))
57/11: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))
57/12:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

#import re
re.findall('\d*\.\d\%', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
57/13: soup.find('div', attrs={'class' : 'Members--bio u-richtext'}).text #findAll find_all is not working
57/14: soup.find('table', attrs={'class' : 'Members--table'}).text #findAll find_all is not working
57/15:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item

#list_item.split('(')

#.text #findAll find_all is not working
57/16: re.findall(r'\w+', list_item)
57/17:
import html5lib
document = html5lib.parse
57/18:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item.replace("\n", "")

#list_item.split('(')

#.text #findAll find_all is not working
57/19:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item.replace("\n", " ")

#list_item.split('(')

#.text #findAll find_all is not working
57/20: re.search("\(\d+\.\d+\%", list_item)
57/21: re.search("\d+\.\d+\%", list_item)
57/22: re.search("\d+\.\d+\%", list_item)[0]
57/23: re.search("\d+\.\d+\%", list_item)
57/24: re.search("\d+\.\d+\%", list_item)[0]
57/25: pct_votes = re.search("\d+\.\d+\%", list_item)[0]
57/26: re.search("\d+\.\d+\%", list_item)[0]
57/27:
pct_votes = re.search("\d+\.\d+\%", list_item)[0]
re.search("(\bRaised\b)\:\W\$(\d*\,\d*\,\d*)", list_item)
57/28:
#pct_votes = re.search("\d+\.\d+\%", list_item)[0]
re.search("(\bRaised\b)\:\W\$(\d*\,\d*\,\d*)", list_item)
57/29:
#pct_votes = re.search("\d+\.\d+\%", list_item)[0]
re.search("(\bRaised\b)\:\W\$(\d*\,\d*\,\d*)", list_item)[0]
57/30:
#pct_votes = re.search("\d+\.\d+\%", list_item)[0]
re.search(r"(\bRaised\b)\:\W\$(\d*\,\d*\,\d*)", list_item)
57/31:
#pct_votes = re.search("\d+\.\d+\%", list_item)[0]
re.search(r"(\bRaised\b)\:\W\$(\d*\,\d*\,\d*)", list_item)[0]
57/32:
#pct_votes = re.search("\d+\.\d+\%", list_item)[0]
re.search(r"\bRaised\b\:\W\$(\d*\,\d*\,\d*)", list_item)[1]
57/33: soup.findAll('div', attrs={'class' : 'Members--list-item'})
57/34: soup.findAll('div', attrs={'class' : 'Members--list-item'}).text
57/35: soup.findAll('div', attrs={'class' : 'Members--list-item'})
57/36:
#soup.findAll('div', attrs={'class' : 'Members--list-item'})
re.search("\d+\.\d+\%", soup.findAll('div', attrs={'class' : 'Members--list-item'}))
57/37:
vote= soup.findAll('div', attrs={'class' : 'Members--list-item'})
re.search("\d+\.\d+\%", vote )
57/38:
vote= soup.findAll('div', attrs={'class' : 'Members--list-item'})
re.search("\d+\.\d+\%", vote)[0]
57/39:
vote = soup.findAll('div', attrs={'class' : 'Members--list-item'})
#re.search("\d+\.\d+\%", vote)[0]
57/40:
vote = soup.findAll('div', attrs={'class' : 'Members--list-item'})
type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/41:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item.replace("\n", " ").replace(",", "")
57/42:
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1.replace("\n", " ").replace(",", "")
57/43:
pct_votes = re.search("\d+\.\d+\%", list_item)[0]
raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item)[1]
57/44:
pct_votes = re.search("\d+\.\d+\%", list_item)[0]
raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item)[1]
raised
57/45:
pct_votes = re.search("\d+\.\d+\%", list_item)[0]
raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
raised
57/46:
pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
raised
57/47:
pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)
57/48:
pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
re.search(r"(\bRaised\b)\:\W\$(\d*)", list_item1)
57/49:
#tooj out the , from the $ amounts to get the vlue
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1.replace("\n", " ").replace(",", "")
57/50:
pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#re.search(r"(\bRaised\b)\:\W\$(\d*)", list_item1)
57/51:
pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
pct_votes
#re.search(r"(\bRaised\b)\:\W\$(\d*)", list_item1)
57/52:
pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
pct_votes
re.search(r"\bRaised\b\:\W\$\d*", list_item1)
57/53:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$\d*", list_item1)
57/54:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$\d*", list_item1)[0]
57/55:
#tooj out the , from the $ amounts to get the vlue
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1 = list_item1.replace("\n", " ").replace(",", "")
57/56:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$\d*", list_item1)[0]
57/57:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[0]
57/58:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
57/59:
soup.findAll('div', attrs={'class' : 'Members--list-item'})
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/60:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "")

    print(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/61:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "")

    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    print(Raised)
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/62:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "")

    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    print(Raised)
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/63:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "")
    
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    print(pct_votes, Raised)
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/64:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "")
    
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*), list_item1)[1]
    print(pct_votes, Raised, Spent)
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/65:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "")
    
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    print(pct_votes, Raised, Spent)
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/66:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\))
57/67:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)")
57/68:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(\([A-Z])\)")
57/69:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)")
57/70:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)
57/71:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
57/72:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[0]
57/73:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)
57/74:
#tooj out the , from the $ amounts to get the vlue
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ")
57/75:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)
57/76:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[0]
57/77:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
57/78:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
57/79:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Part = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    print(pct_votes, Raised, Spent)
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/80:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/81:
#tooj out the , from the $ amounts to get the vlue
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
57/82:
#tooj out the , from the $ amounts to get the vlue
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
57/83:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner), list_item1)
57/84:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)
57/85:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[0]
57/86:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[1]
57/87:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[2]
57/88:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[2]
re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)
57/89:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[2]
re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[1]
57/90:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[2]
re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[2]
57/91: re.search(r"\bWinner", list_item1)[2]
57/92: re.search(r"\bWinner", list_item1)
57/93: re.search(r"\bWinner", list_item1)[0]
57/94:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)[0]

if x == "Winner":
  print("Yes")
  
  else:
    print("No")
57/95:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)[0]

if x == "Winner":
  print("Yes"):
  
  else:
    print("No")
57/96:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)[0]

if x == "Winner":
  print("Yes"):  
else:
    print("No")
57/97:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)[0]

if x == "Winner":
  print("Yes")
else:
    print("No")
57/98:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
    x = re.search(r"\bWinner", list_item1)[0]

if x == "Winner":
  print("Yes")
else:
    print("No")
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/99:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
    x = re.search(r"\bWinner", list_item1)[0]
    if x == "Winner":
        print("Yes")
    else:
        print("No")
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/100:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
x = re.search(r"\bWinner", list_item1)[0]
    if x == "Winner":
        print("Yes")
    else:
        print("No")
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/101:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
x = re.search(r"\bWinner", list_item1)[0]
if x == "Winner":
    print("Yes")
else:
        print("No")
    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/102:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"\bWinner", list_item1)[0]
    print(Name, Party, pct_votes, Raised, Spent)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/103:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"(\bWinner)", list_item1)[0]
    print(Name, Party, pct_votes, Raised, Spent)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/104:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[2]
    print(Name, Party, pct_votes, Raised, Spent)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/105:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)
    print(Name, Party, pct_votes, Raised, Spent)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/106:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)
    print(Name, Party, pct_votes, Raised, Spent, Winner)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/107:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent, Winner)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/108:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[0]
    print(Name, Party, pct_votes, Raised, Spent, Winner)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/109:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)
    print(Name, Party, pct_votes, Raised, Spent, Winner)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/110:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"(\bIncumbent)", list_item1)
    print(Name, Party, pct_votes, Raised, Spent, Winner)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/111:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent, Winner)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/112:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent, Winner)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/113:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/114:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)[0]

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)[0]

if y == "Incumbent":
  print("Yes")
else:
    print("No")
57/115:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)[1]

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)[0]

if y == "Incumbent":
  print("Yes")
else:
    print("No")
57/116:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)[0]

if y == "Incumbent":
  print("Yes")
else:
    print("No")
57/117:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
  print("Yes")
else:
    print("No")
57/118:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
  print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/119:
#tooj out the , from the $ amounts to get the vlue
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
57/120:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[2]
re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[2]
57/121:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

    if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
  print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/123:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

    if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
      print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/125:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

    if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
    print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
57/127:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
    print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
60/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
60/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
60/3: type(response)
60/4: response.status_code
60/5: response.text
60/6: soup = BS(response.text)
60/7: print(soup.prettify())
60/8: soup.find('title')
60/9:
table1 = soup.find('table', attrs={'class' : 'Members--table'})
table1
60/10: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))
60/11: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))
60/12:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

#import re
re.findall('\d*\.\d\%', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
60/13: soup.find('div', attrs={'class' : 'Members--bio u-richtext'}).text #findAll find_all is not working
60/14: soup.find('table', attrs={'class' : 'Members--table'}).text #findAll find_all is not working
60/15:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item.replace("\n", " ")

#list_item.split('(')

#.text #findAll find_all is not working
60/16:
pct_votes = re.search("\d+\.\d+\%", list_item)[0]
raised = re.search(r"\bRaised\b\:\W\$(\d*\,\d*\,\d*)", list_item)[1]
60/17:
#tooj out the , from the $ amounts to get the vlue
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
60/18:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[2]
re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[2]
60/19:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
  print("Yes")
else:
    print("No")
60/20:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
    print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
60/21:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
60/22:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
60/23: type(response)
60/24: response.status_code
60/25: response.text
60/26: soup = BS(response.text)
60/27: print(soup.prettify())
60/28: soup.find('title')
60/29:
table1 = soup.find('table', attrs={'class' : 'Members--table'})
table1
60/30: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))
60/31: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))
60/32:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

#import re
re.findall('\d*\.\d\%', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
60/33: soup.find('div', attrs={'class' : 'Members--bio u-richtext'}).text #findAll find_all is not working
60/34: soup.find('table', attrs={'class' : 'Members--table'}).text #findAll find_all is not working
60/35:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item.replace("\n", " ")

#list_item.split('(')

#.text #findAll find_all is not working
60/36:
pct_votes = re.search("\d+\.\d+\%", list_item)[0]
raised = re.search(r"\bRaised\b\:\W\$(\d*\,\d*\,\d*)", list_item)[1]
60/37:
#tooj out the , from the $ amounts to get the vlue
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
60/38:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[2]
re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[2]
60/39:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
  print("Yes")
else:
    print("No")
60/40:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
    print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
60/41:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    #Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    #pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    #Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    #Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
60/42:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Part = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    #pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    #Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    #Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
60/43:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    #pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    #Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    #Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
60/44:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_vote.append(re.search("\d+\.\d+\%", list_item1)[0])
    #Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    #Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
60/45:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_vote.append(re.search("\d+\.\d+\%", list_item1)[1])
    #Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    #Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
60/46:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_vote.append(re.search("\d+\.\d+\%", list_item1)[0])
    #Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    #Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
60/47:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
    print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
60/48:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    #Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)
60/49:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    Raised = []
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    Spent = []
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    Winner = []
    Winer.append(re.search(r"(\bIncumbent)", list_item1)[1])
    print(Name, Party, pct_votes, Raised, Spent)
60/50:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    Raised = []
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    Spent = []
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    Winner = []
    Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
    print(Name, Party, pct_votes, Raised, Spent)
60/51:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
    print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
60/52:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
    print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
60/53:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    Raised = []
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    Spent = []
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    Winner = []
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
    print(Name, Party, pct_votes, Raised, Spent)
60/54:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    Raised = []
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    Spent = []
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    Winner = []
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
    print(Name)
    #Party, pct_votes, Raised, Spent)
60/55:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    Raised = []
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    Spent = []
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    Winner = []
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
    type(Name)
    #Party, pct_votes, Raised, Spent)
60/56:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    Raised = []
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    Spent = []
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    Winner = []
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
    print(Name)
    #Party, pct_votes, Raised, Spent)
60/57:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    Raised = []
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    Spent = []
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    Winner = []
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
    print(Name, Party, pct_votes, Raised, Spent)
60/58:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame (Name, columns = ['Candidate_Name'])
60/59:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame (Name, columns = ['Candidate_Name'])
df.head(4)
60/60:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame (Name, columns = ['Candidate_Name'])
df.head()
60/61:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame (Name, columns = ['Candidate_Name'])
df.head()
df.shape
60/62:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame ([Name], columns = ['Candidate_Name'])
df.head()
df.shape
60/63:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame ([Name], columns = ['Candidate_Name'])
df.head()
#df.shape
60/64:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame (Name, Party columns = ['Candidate_Name', 'Party'])
df.head()
#df.shape
60/65:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame (Name, Party, columns = ['Candidate_Name', 'Party'])
df.head()
#df.shape
60/66:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame ([Name], [Party], columns = ['Candidate_Name', 'Party'])
df.head()
#df.shape
60/67:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame ([Name], columns = ['Candidate_Name', 'Party'])
df.head()
#df.shape
60/68:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame ([Name], columns = ['Candidate_Name'])
df.head()
#df.shape
60/69:
di = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent}
pd.DataFrame(di)
60/70:
df_ds7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent}
pd.DataFrame(df_ds7)
60/71:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
  print("Yes")
else:
    print("No")
60/72:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    Raised = []
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    Spent = []
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    Winner = []
    Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
    print(Name, Party, pct_votes, Raised, Spent)
60/73:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[2]
re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[2]
60/74:
#tooj out the , from the $ amounts to get the vlue
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
60/75:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[2]
re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[2]
60/76:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []

for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
    print(Name, Party, pct_votes, Raised, Spent)
60/77:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []

for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
print(Name, Party, pct_votes, Raised, Spent)
60/78: re.search(r"(\bIncumbent)", 'Incumbent')
60/79: re.search(r"(\bIncumbent)", 'IncumbentX')
60/80: re.search(r"(\bIncumbent)", 'IncXXtX')
60/81:
x = re.search(r"\bWinner", list_item1)

if x:
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y:
  print("Yes")
else:
    print("No")
60/82: list_item1
60/83:
list_item1 = soup.findAll('div', attrs={'class' : 'Members--list-item'})[0]


x = re.search(r"\bWinner", list_item1)

if x:
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y:
  print("Yes")
else:
    print("No")
60/84:
list_item1 = soup.findAll('div', attrs={'class' : 'Members--list-item'})[0].text


x = re.search(r"\bWinner", list_item1)

if x:
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y:
  print("Yes")
else:
    print("No")
60/85:
list_item1 = soup.findAll('div', attrs={'class' : 'Members--list-item'})[1].text


x = re.search(r"\bWinner", list_item1)

if x:
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y:
  print("Yes")
else:
    print("No")
60/86:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []

for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
print(Name, Party, pct_votes, Raised, Spent)
60/87:
#list_item1 = soup.findAll('div', attrs={'class' : 'Members--list-item'})[1].text


x = re.search(r"\bWinner", list_item1)

if x:
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y:
  print("Yes")
else:
    print("No")
60/88: list_item1
60/89:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []

for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    Winner = re.search(r"\bWinner", list_item1)

    if Winner:
      print("Yes")
    else:
        print("No")

    Incumbent = re.search(r"\bIncumbent", list_item1)

    if Incumbent:
      print("Yes")
    else:
        print("No")
    
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
print(Name, Party, pct_votes, Raised, Spent)
60/90:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
#Winner = []

x = []

for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    Winner = re.search(r"\bWinner", list_item1)
    
    

    if Winner:
        x.append("Yes")
        

    Incumbent = re.search(r"\bIncumbent", list_item1)

    if Incumbent:
      print("Yes")
    else:
        print("No")
    
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
print(Name, Party, pct_votes, Raised, Spent)
60/91:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
#Winner = []

x = []

for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    Winner = re.search(r"\bWinner", list_item1)
    
    

    if Winner:
        x.append("Yes")
        

    Incumbent = re.search(r"\bIncumbent", list_item1)

    if Incumbent:
      print("Yes")
    else:
        print("No")
    
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
print(Name, Party, pct_votes, Raised, Spent, x)
60/92:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
#Winner = []

x = []

for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    Winner = re.search(r"\bWinner", list_item1)
    
    

    if Winner:
        x.append("Yes")
    else:
        x.append("No")
        

    Incumbent = re.search(r"\bIncumbent", list_item1)

    if Incumbent:
      print("Yes")
    else:
        print("No")
    
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
print(Name, Party, pct_votes, Raised, Spent, x)
60/93:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1) 
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
    
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
60/94:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
60/95: *Web scrapping the https://www.opensecrets.org/races/election starting from TN district 7*
60/96:
#Created the dict for the column names and putting the list as values and created the df_TN7
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
60/97:
#This is to creat the df for all the membrs for TN_district07 and use it for all other districts

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
    
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
60/98:
#Created the dict for the column names and putting the list as values and created the df_TN7
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
60/99:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup = BS(r.text,'html.parser')
print(baseurl)
60/100:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup = BS(r.text,'html.parser')
print(baseurl)
d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
a_tags = d.find_all('a')
60/101:
#soup = BS(response.text)
soup_ALL.find('title')
#list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
60/102:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text,'html.parser')


print(baseurl)
#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
60/103:
#soup = BS(response.text)
soup_ALL.find('title')
#list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
60/104:
#soup = BS(response.text)
soup_ALL.find('title')
list_item_ALL = soup.find('div', attrs={'class' : 'Members--list-item'}).text
60/105:
#soup = BS(response.text)
soup_ALL.find('title')
list_item_ALL = soup_ALL.find('div', attrs={'class' : 'Members--list-item'}).text
60/106:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text)


print(baseurl)
#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
60/107:
#soup = BS(response.text)
soup_ALL.find('title')
list_item_ALL = soup_ALL.find('div', attrs={'class' : 'Members--list-item'}).text
60/108:
#soup = BS(response.text)
soup_ALL.find('title')
#list_item_ALL = soup_ALL.find('div', attrs={'class' : 'Members--list-item'}).text
60/109:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text)


print(baseurl)
soup_ALL.find('title')
#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
60/110:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text)


print(baseurl)
soup_ALL.find('title')

soup_ALL.find('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
60/111:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text)


print(baseurl)
soup_ALL.find('title')

ls_item = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
60/112:
#soup = BS(response.text)
soup_ALL.find('title')
list_item_ALL = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})
60/113:
#soup = BS(response.text)
soup_ALL.find('title')
list_item_ALL = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})
ls_item
60/114:
#soup = BS(response.text)
soup_ALL.find('title')
list_item_ALL = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})
ls_item_ALL
60/115:
#soup = BS(response.text)
soup_ALL.find('title')
ls_item_ALL = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})
ls_item_ALL
60/116:
#soup = BS(response.text)
soup_ALL.find('title')
ls_item_ALL = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})
ls_item_ALL
60/117:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item.replace("\n", " ")

#list_item.split('(')

#.text #findAll find_all is not working
62/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
62/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
62/3: type(response)
62/4: response.status_code
62/5: response.text
62/6: soup = BS(response.text)
62/7: print(soup.prettify())
62/8: soup.find('title')
62/9:
table1 = soup.find('table', attrs={'class' : 'Members--table'})
table1
62/10: pd.read_html(str(soup.find('table', attrs={'class' : 'Members--table'})))
62/11: pd.read_html(str(soup.findAll('table', attrs={'class' : 'Members--table'})))
62/12:
pct_vote = soup.find('span', attrs={'class' : 'Members--vote-pct'})

#pct_vote.str.extract('(\d+)')
pct_vote.text

#import re
re.findall('\d*\.\d\%', pct_vote.text)

#type(pct_vote)
#soup.findALL(/html/body/div[6]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/div)[1]
62/13: soup.find('div', attrs={'class' : 'Members--bio u-richtext'}).text #findAll find_all is not working
62/14: soup.find('table', attrs={'class' : 'Members--table'}).text #findAll find_all is not working
62/15:
list_item = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item.replace("\n", " ")

#list_item.split('(')

#.text #findAll find_all is not working
62/16:
pct_votes = re.search("\d+\.\d+\%", list_item)[0]
raised = re.search(r"\bRaised\b\:\W\$(\d*\,\d*\,\d*)", list_item)[1]
62/17:
#tooj out the , from the $ amounts to get the vlue
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
62/18:
#pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
#pct_votes
re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
re.search(r"\([A-Z]\)\W*(\bIncumbent)\W*(\bWinner)", list_item1)[2]
re.search(r"(\bIncumbent)\W*(\bWinner)", list_item1)[2]
62/19:
#re.search(r"\bWinner", list_item1)[0]


#x == "Winner"
x = re.search(r"\bWinner", list_item1)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
  print("Yes")
else:
    print("No")
62/20:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = re.search(r"(.+)\(([A-Z])\)", list_item1)[1]
    Party = re.search(r"(.+)\(([A-Z])\)", list_item1)[2]
    pct_votes = re.search("\d+\.\d+\%", list_item1)[0]
    Raised = re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1]
    Spent = re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1]
    #Winner = re.search(r"(\bIncumbent)", list_item1)[1]
    print(Name, Party, pct_votes, Raised, Spent)

if x == "Winner":
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y == "Incumbent":
    print("Yes")
else:
    print("No")

    
    
#type(vote)
#re.search("\d+\.\d+\%", vote)[0]
62/21:
#list_item1 = soup.findAll('div', attrs={'class' : 'Members--list-item'})[1].text


x = re.search(r"\bWinner", list_item1)

if x:
  print("Yes")
else:
    print("No")

y = re.search(r"\bIncumbent", list_item1)

if y:
  print("Yes")
else:
    print("No")
62/22:
for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    Name = []
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
    Party = []
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    pct_votes = []
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    Raised = []
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    Spent = []
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    Winner = []
    Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
    print(Name, Party, pct_votes, Raised, Spent)
    
##The cod breaks because the list breaks when there is no value meaning There is no word Winner.
62/23:
#list_name = ['item_1', 'item_2', 'item_3',...]
df = pd.DataFrame ([Name], columns = ['Candidate_Name'])
df.head()
#df.shape
62/24:
df_ds7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent}
pd.DataFrame(df_ds7)
62/25:
#This is to creat the df for all the membrs for TN_district07 and use it for all other districts

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
    
    #Winner.append(re.search(r"(\bIncumbent)", list_item1)[1])
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
62/26:
#Created the dict for the column names and putting the list as values and created the df_TN7
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
62/27:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text)


print(baseurl)
soup_ALL.find('title')

ls_item = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
62/28:
#soup = BS(response.text)
soup_ALL.find('title')
ls_item_ALL = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})
ls_item_ALL
62/29:
#soup = BS(response.text)
soup_ALL.find('title')
#ls_item_ALL = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})
#ls_item_ALL
62/30:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text)


print(baseurl)
soup_ALL.find('title')

ls_item = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
62/31:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text)


print(baseurl)
soup_ALL.find('title')

#ls_item = soup_ALL.find('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
62/32:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text)


print(baseurl)
soup_ALL.find('title')

soup_ALL.find('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
62/33:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text)


print(baseurl)
soup_ALL.find('title')

soup_ALL.find_all('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
62/34:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_ALL = BS(r.text)


print(baseurl)
soup_X.find('title')

soup_ALL.find_all('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
62/35:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_x = BS(r.text)


print(baseurl)
soup_X.find('title')

soup_ALL.find_all('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
62/36:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_x = BS(r.text)


print(baseurl)
soup_x.find('title')

soup_ALL.find_all('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
62/37:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_x = BS(r.text)


print(baseurl)
soup_x.find('title')

soup_x.find('div', attrs={'class' : 'Members--list-item'})

#d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
63/1: Web scrapping the https://www.opensecrets.org/races/election starting from TN district 7
63/2:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
63/3:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
63/4:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
type(response)
63/5:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
type(response)
response.status_code
63/6:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)
type(response)
# response.status_code  #200 not broke read properly
63/7:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)  
type(response)       # it is a requests.status.models.Response
# response.status_code  #200 not broke read properly
63/8:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)  
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
63/9:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)  
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text
63/10:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)  
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
63/11:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)  
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
print(soup.prettify())
63/12:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)  
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
63/13:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'

response = requests.get(URL)  
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
63/14:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text


#list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
63/15:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
#list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
63/16:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
63/17:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
63/18:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
63/19: #### Now converting these list into a Dictionary object and using that to creat the Data Frame
63/20:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
63/21: #### There is pattern in website were 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N' where the id=TN is the stae and 07 is the district so we are going to follow the logic and creat a variabe called state feed the state code to it and a d_num variable which will change adding 1 from the previos district number and starts with 01
63/22:
#testing url
state = 'AK'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup = BS(r.text,'html.parser')
print(baseurl)
63/23:
state = 'TN'
d_num = 1

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

#response.status_code  #200 not broke read properly
#response.text         # helps open the object to see it looks
#soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
#soup.find('title')
63/24:
state = 'TN'
d_num = 1

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
#response.text         # helps open the object to see it looks
#soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
#soup.find('title')
63/25:
state = 'TN'
d_num = 1

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
#soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
#soup.find('title')
63/26:
state = 'TN'
d_num = 1

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
#soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
#soup.find('title')
63/27:
state = 'TN'
d_num = 1

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soupX.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
#soupX.find('title')
63/28:
state = 'TN'
d_num = 1

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
print(soupX.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
#soupX.find('title')
63/29:
state = 'TN'
d_num = 1

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
print(soupX.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soupX.find('title')
63/30:
state = 'TN'
d_num = 1

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
#soupX = BS(responseX.text) #converts the response object in Beautiful object
soupX.find('title')
63/31:
state = 'TN'
d_num = 1

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
#soupX = BS(responseX.text) #converts the response object in Beautiful object
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
list_itemX
63/32: soupX.find('table', attrs={'class' : 'Members--table'})
63/33:
state = 'TN'
d_num = 1

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
#soupX = BS(responseX.text) #converts the response object in Beautiful object
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'})
list_itemX
63/34:
state = 'TN'
d_num = 2

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
#soupX = BS(responseX.text) #converts the response object in Beautiful object
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'})
list_itemX
63/35:
state = 'TN'
d_num = 2

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'})
list_itemX
63/36:
state = 'TN'
d_num = 2

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
print(soup.prettify())
soupX.find('title')

#list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'})
#list_itemX
63/37:
state = 'TN'
d_num = 2

#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'
print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
print(soup.prettify())
soupX.find('title')

#list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'})
#list_itemX
63/38:
state = 'TN'
d_num = 2

#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'
print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'})
#list_itemX
63/39:
state = 'TN'
d_num = 2

#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'
print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'})
list_itemX
63/40:
state = 'TN'
d_num = 2

#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'
print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'})
list_itemX
63/41:
state = 'TN'
d_num = 2

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'
print(URLX)

responseX = requests.get(URLX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'})
list_itemX
63/42:
state = 'TN'
d_num = 2

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'
URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id=TN01&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'})
list_itemX
63/43:
state = 'TN'
d_num = 2

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'
URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id=TN01&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
list_itemX
63/44:
state = 'TN'
d_num = 2

URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'
URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id=TN01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
list_itemX
63/45:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage

#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'
URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id=TN01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
list_itemX
63/46: soupX.find('table', attrs={'class' : 'Members--table'})
63/47: soupX.find('table', attrs={'class' : 'Members--table'}).text
63/48:
NameX = []
PartyX = []
pct_votesX = []
RaisedX = []
SpentX = []
WinnerX = []
IncumbentX = []


for list_itemX in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(NameX, PartyX, pct_votesX, RaisedX, SpentX, WinnerX, IncumbentX)
63/49:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id=TN01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
list_itemX
63/50:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id=TN01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
63/51:
NameX = []
PartyX = []
pct_votesX = []
RaisedX = []
SpentX = []
WinnerX = []
IncumbentX = []


for list_itemX in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    NameX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    PartyX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votesX.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    RaisedX.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    SpentX.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        WinnerX.append("Yes")
    else:
        WinnerX.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      IncumbentX.append("Yes")
    else:
       IncumbentX.append("No") 
print(NameX, PartyX, pct_votesX, RaisedX, SpentX, WinnerX, IncumbentX)
63/52:
NameX = []
PartyX = []
pct_votesX = []
RaisedX = []
SpentX = []
WinnerX = []
IncumbentX = []


for list_itemX in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    NameX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    PartyX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votesX.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    RaisedX.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    SpentX.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        WinnerX.append("Yes")
    else:
        WinnerX.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      IncumbentX.append("Yes")
    else:
       IncumbentX.append("No") 
print(NameX, PartyX, pct_votesX, RaisedX, SpentX, WinnerX, IncumbentX)
63/53:
NameX = []
PartyX = []
pct_votesX = []
RaisedX = []
SpentX = []
WinnerX = []
IncumbentX = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    NameX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    PartyX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votesX.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    RaisedX.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    SpentX.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        WinnerX.append("Yes")
    else:
        WinnerX.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      IncumbentX.append("Yes")
    else:
       IncumbentX.append("No") 
print(NameX, PartyX, pct_votesX, RaisedX, SpentX, WinnerX, IncumbentX)
63/54:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
63/55:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
63/56:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
63/57:
NameX = []
PartyX = []
pct_votesX = []
RaisedX = []
SpentX = []
WinnerX = []
IncumbentX = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    NameX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    PartyX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votesX.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    RaisedX.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    SpentX.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        WinnerX.append("Yes")
    else:
        WinnerX.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      IncumbentX.append("Yes")
    else:
       IncumbentX.append("No") 
print(NameX, PartyX, pct_votesX, RaisedX, SpentX, WinnerX, IncumbentX)
63/58:
d = soupX.find('div', attrs={'class' : 'Members--list-item'}).text  #Navigation showing list of districts
a_tags = d.find_all('a')


#soupX.find('div', attrs={'class' : 'Members--list-item'}).text

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
63/59:
d = soupX.find('div', attrs={'class' : 'Members--list-item'})  #Navigation showing list of districts
a_tags = d.find_all('a')


#soupX.find('div', attrs={'class' : 'Members--list-item'}).text

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
63/60:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
63/61:
NameX = []
PartyX = []
pct_votesX = []
RaisedX = []
SpentX = []
WinnerX = []
IncumbentX = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    NameX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    PartyX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votesX.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    RaisedX.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    SpentX.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        WinnerX.append("Yes")
    else:
        WinnerX.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      IncumbentX.append("Yes")
    else:
       IncumbentX.append("No") 
print(NameX, PartyX, pct_votesX, RaisedX, SpentX, WinnerX, IncumbentX)
63/62:
a_tags = soup.find('ol',attrs = {'class':'SubNav-items'}).find_all('a')  #Navigation showing list of districts
a_tags
63/63:
a_tags = soup.find('div', attrs={'class' : 'Members--list-item'}).find_all('a')  #Navigation showing list of districts
a_tags
63/64:
a_tags = soup.find('div', attrs={'class' : 'Members--list-item'}).text.find_all('a')  #Navigation showing list of districts
a_tags
63/65:
a_tags = soup.find('div', attrs={'class' : 'Members--list-item'})
#.find_all('a')  #Navigation showing list of districts
a_tags
63/66:
a_tags = soup.find('div', attrs={'class' : 'Members--list-item'}).text
#.find_all('a')  #Navigation showing list of districts
a_tags
63/67:
a_tags = soup.find('div', attrs={'class' : 'Members--list-item'})
#.find_all('a')  #Navigation showing list of districts
a_tags
63/68:
a_tags = soupX.find('div', attrs={'class' : 'Members--list-item'})
#.find_all('a')  #Navigation showing list of districts
a_tags
63/69:
a_tags = soupX.find('div', attrs={'class' : 'Members--list-item'}).find_all('a')  #Navigation showing list of districts
a_tags
63/70:
#a_tags = soupX.find('div', attrs={'class' : 'Members--list-item'}).find_all('a')  #Navigation showing list of districts
#a_tags

a_tags = soup.find('ol',attrs = {'class':'SubNav-items'}).find_all('a')  #Navigation showing list of districts
a_tags
63/71:
a_tags = soupX.find('div', attrs={'class' : 'Members--list-item'}).find_all('a')  #Navigation showing list of districts
a_tags
63/72:
NameX = []
PartyX = []
pct_votesX = []
RaisedX = []
SpentX = []
WinnerX = []
IncumbentX = []

while count <= d_count:
    URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
    print(URLOX)

    responseX = requests.get(URLOX)  #creating a response object pulling the URL
    type(responseX)       # it is a requests.status.models.Response

    responseX.status_code  #200 not broke read properly
    responseX.text         # helps open the object to see it looks
    soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
    #print(soup.prettify())
    soupX.find('title')

    list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

    list_itemX

    for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemX = list_itemX.text

        list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")


        NameX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])

        PartyX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

        pct_votesX.append(re.search("\d+\.\d+\%", list_itemX)[0])

        RaisedX.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

        SpentX.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

        x = re.search(r"\bWinner", list_itemX)  
        #the search makes the word match so it stops if no match so that is why weused the append YEs or No
        if x:
            WinnerX.append("Yes")
        else:
            WinnerX.append("No")

        y = re.search(r"\bIncumbent", list_itemX)

        if y:
          IncumbentX.append("Yes")
        else:
           IncumbentX.append("No") 
count+=1 # increment count

print(NameX, PartyX, pct_votesX, RaisedX, SpentX, WinnerX, IncumbentX)
63/73:
NameX = []
PartyX = []
pct_votesX = []
RaisedX = []
SpentX = []
WinnerX = []
IncumbentX = []

state = 'TN'
d_num = 1

while count <= d_num:
    URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
    print(URLOX)

    responseX = requests.get(URLOX)  #creating a response object pulling the URL
    type(responseX)       # it is a requests.status.models.Response

    responseX.status_code  #200 not broke read properly
    responseX.text         # helps open the object to see it looks
    soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
    #print(soup.prettify())
    soupX.find('title')

    list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

    list_itemX

    for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemX = list_itemX.text

        list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")


        NameX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])

        PartyX.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

        pct_votesX.append(re.search("\d+\.\d+\%", list_itemX)[0])

        RaisedX.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

        SpentX.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

        x = re.search(r"\bWinner", list_itemX)  
        #the search makes the word match so it stops if no match so that is why weused the append YEs or No
        if x:
            WinnerX.append("Yes")
        else:
            WinnerX.append("No")

        y = re.search(r"\bIncumbent", list_itemX)

        if y:
          IncumbentX.append("Yes")
        else:
           IncumbentX.append("No") 
d_num+=1 # increment count

print(NameX, PartyX, pct_votesX, RaisedX, SpentX, WinnerX, IncumbentX)
63/74:



soupX.find('div', attrs={'class' : 'Members--list-item'}).text

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
63/75:



soupX.find('div', attrs={'class' : 'Members--list-item'}).text

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
63/76:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
63/77:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
63/78:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
63/79:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
64/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
64/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
64/3:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
64/4:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
64/5:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
64/6:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
64/7:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
64/8:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
64/9:



soupX.find('div', attrs={'class' : 'Members--list-item'}).text

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
64/10:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN1)
62/38:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_x = BS(r.text)

soup_y = BS(r.text,'html.parser')

print(baseurl)
soup_x.find('title')

soup_x.find('div', attrs={'class' : 'Members--list-item'})

d = soup.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
62/39:
state = 'TN'
d_num = 1
baseurl = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'
r = requests.get(baseurl)
soup_x = BS(r.text)

soup_y = BS(r.text,'html.parser')

print(baseurl)
soup_x.find('title')

#soup_x.find('div', attrs={'class' : 'Members--list-item'})

d = soup_y.find('ol',attrs = {'class':'SubNav-items'})  #Navigation showing list of districts
#a_tags = d.find_all('a')
62/40:
 #Navigation showing list of districts
    

d = soup_y.find('ol',attrs = {'class':'SubNav-items'}) 
#a_tags = d.find_all('a')
62/41:
 #Navigation showing list of districts
    

d = soup_y.find('ol',attrs = {'class':'SubNav-items'}) 
d
#a_tags = d.find_all('a')
62/42:
 #Navigation showing list of districts
    

d = soup_x.find('ol',attrs = {'class':'SubNav-items'}) 
d
#a_tags = d.find_all('a')
62/43:
 #Navigation showing list of districts
    

d = soup_x.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
#a_tags = d.find_all('a')
62/44:
 #Navigation showing list of districts
    

d = soup_x.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags
#a_tags = d.find_all('a')
64/11:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags
62/45:
 #Navigation showing list of districts
    

d = soup_x.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count  
#a_tags = d.find_all('a')
64/12:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
64/13:
count = 1

count = 1



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN0'  + str(count) + '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    
    print(url)
65/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
65/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
65/3:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
65/4:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
65/5:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
65/6:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
65/7:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
65/8:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN1)
65/9:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
65/10:
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN0'  + str(count) + '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    
    print(url)
    
    list_itemY = soup.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

#list_itemX
65/11:
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN0'  + str(count) + '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    
    print(url)
    
    list_itemY = soup.find('div', attrs={'class' : 'Members--list-item'})
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

#list_itemX
66/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
66/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
66/3:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
66/4:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
66/5:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
66/6:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
66/7:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
66/8:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN1)
66/9:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
66/10:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    
    print(url)
    
    #list_itemY = soup.find('div', attrs={'class' : 'Members--list-item'})
    
    count+=1 # increment count
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

#list_itemX
66/11:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    
    print(url)
    
    #list_itemY = soup.find('div', attrs={'class' : 'Members--list-item'})
    
    count+=1 # increment count
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

#list_itemX
66/12:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    
    print(url)
    
    list_itemY = soup.find('div', attrs={'class' : 'Members--list-item'})
    
    count+=1 # increment count
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

#list_itemX
66/13:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    
    print(url)
    
    list_itemY = soup.find('div', attrs={'class' : 'Members--list-item'})
    
    count+=1 # increment count
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

    list_itemY
66/14:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soupX = BS(r.text,'html.parser')
    
    print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemY = list_itemY.text

    list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    
    count+=1 # increment count
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

    list_itemY
66/15:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soupX = BS(r.text,'html.parser')
    
    print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    
    count+=1 # increment count
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

    list_itemY
66/16:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soupX = BS(r.text,'html.parser')
    
    print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    
    count+=1 # increment count

        list_itemY
66/17:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soupX = BS(r.text,'html.parser')
    
    print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        
        list_itemY
        
    count+=1 # increment count
66/18:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    soupX = BS(er.text)
    
    print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        
        list_itemY
        
    count+=1 # increment count
66/19:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        
        list_itemY
        
    count+=1 # increment count
66/20:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        
        list_itemY
        
    count+=1 # increment count
66/22:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        
        list_itemY
        
count+=1 # increment count
68/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
68/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
68/3:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
69/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
69/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
69/3:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
69/4:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
69/5:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
69/6:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
69/7:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
69/8:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN1)
69/9:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
69/10:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
   # soupX = BS(r.text)
    
    print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        
        list_itemY
        
    count+=1 # increment count
69/11:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    soup = BS(r.text,'html.parser')
   # soupX = BS(r.text)
    
    #print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        
        list_itemY
        
    count+=1 # increment count
69/12:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
69/13:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    
    #soup = BS(r.text,'html.parser')
    soup = BS(r.text)
    
    #print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        list_itemY
        
    count+=1 # increment count
69/14:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    #print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        list_itemY
        
    count+=1 # increment count
69/15:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    for list_itemY in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        list_itemY
        
    count+=1 # increment count
69/16:  soupX.findAll('div', attrs={'class' : 'Members--list-item'})
69/17:  soupX.findAll('div', attrs={'class' : 'Members--list-item'}).text
69/18:  soupX.find_all('div', attrs={'class' : 'Members--list-item'}).text
69/19:  soupX.find_all('div', attrs={'class' : 'Members--list-item'})
69/20: xyz = soupX.find_all('div', attrs={'class' : 'Members--list-item'})
69/21:
xyz = soupX.find_all('div', attrs={'class' : 'Members--list-item'})
xyz = xyz.text
xyz
69/22:
xyz = soupX.find_all('div', attrs={'class' : 'Members--list-item'})
type(xyz)
xyz
69/23:
xyz = soupX.find_all('div', attrs={'class' : 'Members--list-item'})
type(xyz)
#xyz
69/24:
xyz = soupX.find_all('div', attrs={'class' : 'Members--list-item'}).text
type(xyz)
#xyz
69/25:
xyz = soupX.findAll('div', attrs={'class' : 'Members--list-item'}).text
type(xyz)
#xyz
69/26:
xyz = soupX.findAll('div', attrs={'class' : 'Members--list-item'})
type(xyz)
#xyz
69/27:
xyz = soupX.findAll('div', attrs={'class' : 'Members--list-item'})
xyz = xyz.text
type(xyz)
#xyz
69/28:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = xyz.text
type(xyz)
#xyz
69/29:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = xyz.text
type(xyz)
xyz
69/30:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        list_itemY
        
    count+=1 # increment count
69/31:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        #list_itemY = list_itemY.text

        #list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        list_itemY
        
    count+=1 # increment count
69/32:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        #list_itemY = list_itemY.text

        #list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        type(list_itemY)
        
    count+=1 # increment count
69/33:
state = 'TN'
d_num = 1
count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)
    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        #list_itemY = list_itemY.text

        #list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        type(list_itemY)
        
count+=1 # increment count
69/34:
state = 'TN'

count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    Name = []
    Party = []
    pct_votes = []
    Raised = []
    Spent = []
    Winner = []
    Incumbent = []
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        #list_itemY = list_itemY.text

        #list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        type(list_itemY)
        
    count+=1 # increment count
69/35:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = (xyz.text).replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
type(xyz)
xyz
69/36:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = (xyz.text).replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
type(xyz)
xyz
69/37:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = (xyz.text).replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
Name = re.search(r"(.+)\(([A-Z])\)", xyz)[1]
type(xyz)
xyz
Name
71/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
71/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
71/3:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
71/4:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
71/5:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
71/6:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
71/7:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
71/8:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN1)
71/9:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
71/10:
state = 'TN'

count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    Name = []
    Party = []
    pct_votes = []
    Raised = []
    Spent = []
    Winner = []
    Incumbent = []
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        #list_itemY = list_itemY.text

        #list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        type(list_itemY)
        
    count+=1 # increment count
71/11:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = (xyz.text).replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
Name = re.search(r"(.+)\(([A-Z])\)", xyz)[1]
type(xyz)
xyz
Name
71/12:
state = 'TN'

count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    Name = []
    Party = []
    pct_votes = []
    Raised = []
    Spent = []
    Winner = []
    Incumbent = []
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        type(list_itemY)
        
    count+=1 # increment count
71/13:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN1)
df_TN1.head()
71/14:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
71/15:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN1)
71/16:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_01 = pd.DataFrame(df_TN1)
TN1_01.head()
71/17:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_01 = pd.DataFrame(df_TN1)
TN_01.head()
71/18:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
71/19:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN2 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_02 = pd.DataFrame(df_TN1)
TN_02.head()
71/20:
state : {'TN': 9, 'CAL':53}

for key, value in state.item():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
71/21:
state = {'TN': 9, 'CAL':53}

for key, value in state.item():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
71/22: state = {'TN': 9, 'CAL':53}
71/23:


for key, value in state_dict.items():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
71/24:


for key, value in state.items():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
71/25:
state = {'TN': 9, 'CAL':53}

for key, value in state.items():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
71/26:
state = {'TN': 9, 'CAL':53}

for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
71/27:
state = {'TN': 9, 'CAL':53}

for key, value in state.items():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
71/28:
state = {'TN': 9, 'CAL':53}

for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
71/29:


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
71/30:


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        responseX = requests.get(URLOX)  #creating a response object pulling the URL
        type(responseX)       # it is a requests.status.models.Response

        responseX.status_code  #200 not broke read properly
        responseX.text         # helps open the object to see it looks
        soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        list_itemX
71/31:
state = {'TN': 9, 'CAL':53}

for key, value in state.items():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
71/32:


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        print(list_itemX)
71/33:


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
71/34:


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
71/35:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
71/36:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic_list = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_02 = pd.DataFrame(dic_list)
TN_CA.head()
71/37:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic_list = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
71/38: TA_CA.shape
71/39: TN_CA.shape
71/40:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []

for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
71/41: TN_CA.shape
71/42:
state = {'TN': 9, 'CAL':53}

for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
71/43: pd.ExcelFile("..\data\USCongressionalDistricts.xlsx")
71/44: pd.ExcelFile("../data/USCongressionalDistricts.xlsx")
71/45:

pd.read_excel('../data/USCongressionalDistricts.xlsx', sheet_name=None)
71/46:

pd.read_excel('../data/USCongressionalDistricts.xlsx')
71/47:

STREP = pd.read_csv('../data/STREP.csv')
71/48:

STREP = pd.read_csv('..\data\STREP.csv')
71/49:
import pandas as pd
STREP = pd.read_csv('..\data\STREP.csv')
71/50:
import pandas as pd
STREP = pd.read_csv('C:\Users\upadh\Documents\NSS_Projects\webscraping_open_secrets-jolly_ranchers\data')
71/51:
import pandas as pd
STREP = pd.read_csv('..\Documents\NSS_Projects\webscraping_open_secrets-jolly_ranchers\data')
71/52:
import pandas as pd
STREP = pd.read_csv('..\Documents\NSS_Projects\webscraping_open_secrets-jolly_ranchers\data\STREP.csv')
71/53:
import pandas as pd
STREP = pd.read_csv('..\data\STREP.csv')
71/54: dic_df = pd.read_csv('..\data\STREP.csv')
71/55: dic_df = pd.read_csv('../data/STREP.csv')
72/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
72/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
72/3:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
72/4:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
72/5:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
72/6:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
72/7:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
72/8:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_01 = pd.DataFrame(df_TN1)
TN_01.head()
72/9:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
72/10:
state = 'TN'

count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    Name = []
    Party = []
    pct_votes = []
    Raised = []
    Spent = []
    Winner = []
    Incumbent = []
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        type(list_itemY)
        
    count+=1 # increment count
72/11:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = (xyz.text).replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
Name = re.search(r"(.+)\(([A-Z])\)", xyz)[1]
type(xyz)
xyz
Name
72/12:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+str(d_num)+'&spec=N'
print(URLOX)
72/13:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
72/14:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN2 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_02 = pd.DataFrame(df_TN1)
TN_02.head()
72/15:
state = {'TN': 9, 'CAL':53}

for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
72/16:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
72/17:
state = {'TN': 9, 'CAL':53}

for key, value in state.items():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
72/18:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
72/19: TN_CA.shape
72/20:
url_state = 'https://www.britannica.com/topic/United-States-House-of-Representatives-Seats-by-State-1787120'
r_state = requests.get(url)
soup_state = BS(r_state.text,'html.parser')
co_table = pd.read_html(str(soup.find('table')))
co_table_df = co_table[0]
co_table_df = co_table_df.drop(co_table_df.tail(1).index)
co_table_df.head()
72/21:
url_state = 'https://www.britannica.com/topic/United-States-House-of-Representatives-Seats-by-State-1787120'
r_state = requests.get(url)
soup_state = BS(r_state.text,'html.parser')
soup_state
#co_table = pd.read_html(str(soup.find('table')))
#co_table_df = co_table[0]
#co_table_df = co_table_df.drop(co_table_df.tail(1).index)
#co_table_df.head()
72/22:
url_state = 'https://www.britannica.com/topic/United-States-House-of-Representatives-Seats-by-State-1787120'
r_state = requests.get(url)
soup_state = BS(r_state.text,'html.parser')
soup_state
co_table = pd.read_html(str(soup_state.find('table')))
co_table_df = co_table[0]
co_table_df = co_table_df.drop(co_table_df.tail(1).index)
co_table_df.head()
72/23:
url_state = 'https://www.britannica.com/topic/United-States-House-of-Representatives-Seats-by-State-1787120'
r_state = requests.get(url)
soup_state = BS(r_state.text,'html.parser')

co_table = pd.read_html(str(soup_state.find('table')))
co_table_df = co_table[0]
co_table_df = co_table_df.drop(co_table_df.tail(1).index)
co_table_df.head()
72/24:
url_state = 'https://www.britannica.com/topic/United-States-House-of-Representatives-Seats-by-State-1787120'
r_state = requests.get(url_state)
soup_state = BS(r_state.text,'html.parser')

co_table = pd.read_html(str(soup_state.find('table')))
co_table_df = co_table[0]
co_table_df = co_table_df.drop(co_table_df.tail(1).index)
co_table_df.head()
72/25:
state_abb = {
    "Alabama": "AL", "Alaska": "AK", "Arizona": "AZ",
    "Arkansas": "AR", "California": "CA","Colorado": "CO",
    "Connecticut": "CT", "Delaware": "DE", "Florida": "FL",
    "Georgia": "GA", "Hawaii": "HI", "Idaho": "ID",
    "Illinois": "IL", "Indiana": "IN", "Iowa": "IA",
    "Kansas": "KS", "Kentucky": "KY", "Louisiana": "LA",
    "Maine": "ME", "Maryland": "MD", "Massachusetts": "MA",
    "Michigan": "MI", "Minnesota": "MN", "Mississippi": "MS",
    "Missouri": "MO","Montana": "MT", "Nebraska": "NE",
    "Nevada": "NV", "New Hampshire": "NH", "New Jersey": "NJ",
    "New Mexico": "NM", "New York": "NY", "North Carolina": "NC",
    "North Dakota": "ND", "Ohio": "OH","Oklahoma": "OK",
    "Oregon": "OR", "Pennsylvania": "PA", "Rhode Island": "RI",
    "South Carolina": "SC", "South Dakota": "SD",
    "Tennessee": "TN", "Texas": "TX", "Utah": "UT",
    "Vermont": "VT", "Virginia": "VA", "Washington": "WA",
    "West Virginia": "WV", "Wisconsin": "WI", "Wyoming": "WY"
    }
72/26: co_table_df['state_code'] = co_table_df.state.map(state_abb)
72/27:
co_table_df['state_code'] = co_table_df.state.map(state_abb)
co_table_df.head
72/28:
co_table_df['state_code'] = co_table_df.state.map(state_abb)
co_table_df.(head)
72/29:
co_table_df['state_code'] = co_table_df.state.map(state_abb)
co_table_df.head()
72/30:
co_tab_dict = dict(zip(co_table_df.state_code, co_table_df.representatives))
co_tab_dic
72/31:
co_tab_dict = dict(zip(co_table_df.state_code, co_table_df.representatives))
co_tab_dict
72/32:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
72/33:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/34:
state_rep = dict(zip(co_table_df.state_code, co_table_df.representatives))
state_rep
72/35:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_rep.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/36:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_rep.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/37:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
72/38: TN_CA.shape
72/39:
url_state = 'https://www.britannica.com/topic/United-States-House-of-Representatives-Seats-by-State-1787120'
r_state = requests.get(url_state)
soup_state = BS(r_state.text,'html.parser')

co_table = pd.read_html(str(soup_state.find('table')))
co_table_df = co_table[0]
co_table_df = co_table_df.drop(co_table_df.tail(1).index)
co_table_df.head()
72/40:
state_abb = {
    "Alabama": "AL", "Alaska": "AK", "Arizona": "AZ",
    "Arkansas": "AR", "California": "CA","Colorado": "CO",
    "Connecticut": "CT", "Delaware": "DE", "Florida": "FL",
    "Georgia": "GA", "Hawaii": "HI", "Idaho": "ID",
    "Illinois": "IL", "Indiana": "IN", "Iowa": "IA",
    "Kansas": "KS", "Kentucky": "KY", "Louisiana": "LA",
    "Maine": "ME", "Maryland": "MD", "Massachusetts": "MA",
    "Michigan": "MI", "Minnesota": "MN", "Mississippi": "MS",
    "Missouri": "MO","Montana": "MT", "Nebraska": "NE",
    "Nevada": "NV", "New Hampshire": "NH", "New Jersey": "NJ",
    "New Mexico": "NM", "New York": "NY", "North Carolina": "NC",
    "North Dakota": "ND", "Ohio": "OH","Oklahoma": "OK",
    "Oregon": "OR", "Pennsylvania": "PA", "Rhode Island": "RI",
    "South Carolina": "SC", "South Dakota": "SD",
    "Tennessee": "TN", "Texas": "TX", "Utah": "UT",
    "Vermont": "VT", "Virginia": "VA", "Washington": "WA",
    "West Virginia": "WV", "Wisconsin": "WI", "Wyoming": "WY"
    }
72/41:
co_table_df['state_code'] = co_table_df.state.map(state_abb)
co_table_df.head()
72/42:
state_rep = dict(zip(co_table_df.state_code, co_table_df.representatives))
state_rep
72/43:
You can achieve this using formatted string
num = 1
number = f"{num:02}"
--------------
output:
"01"
72/44:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_rep.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/45:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_rep.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/46: print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)
72/47: failedurls = []
72/48: print(failedurl)
72/49:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state.items().items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/50:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/51:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/52: print(state)
72/53:
state_big = {'AL': 7,
 'AK': 1,
 'AZ': 9,
 'AR': 4,
 'CA': 53,
 'CO': 7,
 'CT': 5,
 'DE': 1,
 'FL': 27,
 'GA': 14,
 'HI': 2,
 'ID': 2,
 'IL': 18,
 'IN': 9,
 'IA': 4,
 'KS': 4,
 'KY': 6,
 'LA': 6,
 'ME': 2,
 'MD': 8,
 'MA': 9,
 'MI': 14,
 'MN': 8,
 'MS': 4,
 'MO': 8,
 'MT': 1,
 'NE': 3,
 'NV': 4,
 'NH': 2,
 'NJ': 12,
 'NM': 3,
 'NY': 27,
 'NC': 13,
 'ND': 1,
 'OH': 16,
 'OK': 5,
 'OR': 5,
 'PA': 18,
 'RI': 2,
 'SC': 7,
 'SD': 1,
 'TN': 9,
 'TX': 36,
 'UT': 4,
 'VT': 1,
 'VA': 11,
 'WA': 10,
 'WV': 3,
 'WI': 8,
 'WY': 1}
72/54:
state_big = {'AL': 7,
 'AK': 1,
 'AZ': 9,
 'AR': 4,
 'CA': 53,
 'CO': 7,
 'CT': 5,
 'DE': 1,
 'FL': 27,
 'GA': 14,
 'HI': 2,
 'ID': 2,
 'IL': 18,
 'IN': 9,
 'IA': 4,
 'KS': 4,
 'KY': 6,
 'LA': 6,
 'ME': 2,
 'MD': 8,
 'MA': 9,
 'MI': 14,
 'MN': 8,
 'MS': 4,
 'MO': 8,
 'MT': 1,
 'NE': 3,
 'NV': 4,
 'NH': 2,
 'NJ': 12,
 'NM': 3,
 'NY': 27,
 'NC': 13,
 'ND': 1,
 'OH': 16,
 'OK': 5,
 'OR': 5,
 'PA': 18,
 'RI': 2,
 'SC': 7,
 'SD': 1,
 'TN': 9,
 'TX': 36,
 'UT': 4,
 'VT': 1,
 'VA': 11,
 'WA': 10,
 'WV': 3,
 'WI': 8,
 'WY': 1}
72/55:
state_big = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4, 'CA': 53, 'CO': 7, 'CT': 5, 'DE': 1, 'FL': 27, 'GA': 14, 'HI': 2, 'ID': 2, 
             'IL': 18, 'IN': 9, 'IA': 4, 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9, 'MI': 14, 'MN': 8, 'MS': 4,
             'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NY': 27, 'NC': 13, 'ND': 1, 'OH': 16, 'OK': 5,
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'UT': 4, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
72/56:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/57: #print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)
72/58:
state_big = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4, 'CO': 7, 'CT': 5, 'DE': 1, 'FL': 27, 'GA': 14, 'HI': 2, 'ID': 2, 
             'IL': 18, 'IN': 9, 'IA': 4, 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9, 'MI': 14, 'MN': 8, 'MS': 4,
             'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NY': 27, 'NC': 13, 'ND': 1, 'OH': 16, 'OK': 5,
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'UT': 4, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
72/59:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/60:
state_C = {'CA': 53, 'CO': 7, 'CT': 5,}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/61:
state_C = {'CA': 53,  'CT': 5,}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/62:
state_C = {'CA': 53,  'CT': 5,}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/63:
state_C = {'CA': 53, 'CO': 7,'CT': 5,}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_C.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/64:
state_C = { 'CO': 7,'CT': 5,}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_C.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/65:
state_C = {'CO': 7}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_C.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/66:

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.head()
72/67:

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_strep = pd.DataFrame(dic_list)
USA_strep.shape
USA_strep.head()
72/68: USA_strep.shape
72/69: print(TN_CA)
72/70: TN_CA
72/71:
state = {'TN': 9, 'CA':53}

for key, value in state.items():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
72/72:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
72/73:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
72/74: TN_CA.to_csv('TN_CA.csv')
72/75:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
72/76:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_A.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_A = pd.DataFrame(dic_list)

USA_A.head()
72/77: USA_A.shape
72/78: USA_A
72/79: USA_A.to_csv('AL_AK_AZ_AR.csv')
72/80:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_C.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_C = pd.DataFrame(dic_list)

USA_C.head()
72/81:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DFGH = {'DE': 1, 'FL': 27, 'GA': 14, 'HI': 2}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_DFGH.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_DFGH = pd.DataFrame(dic_list)

USA_DFGH.head()
72/82:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DFGH = {'DE': 1, 'FL': 27, 'GA': 14, 'HI': 2}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_DFGH.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_DFGH = pd.DataFrame(dic_list)

USA_DFGH.head()
72/83:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DFGH = {'DE': 1, 'FL': 27, 'HI': 2}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_DFGH.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_DFGH = pd.DataFrame(dic_list)

USA_DFGH.head()
72/84:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_DFGH.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_DF = pd.DataFrame(dic_list)

USA_DF.head()
72/85:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_DF.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_DF = pd.DataFrame(dic_list)

USA_DF.head()
72/86: USA_DF.shape
72/87: USA_DF
72/88: USA_DF.to_csv('DE_FL.csv')
72/89:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2,'IL': 18, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_I.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_I = pd.DataFrame(dic_list)

USA_I.head()
72/90:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_I.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_I = pd.DataFrame(dic_list)

USA_I.head()
72/91: USA_I.shape
72/92: USA_I
72/93: USA_I.to_csv('ID_IA_IN.csv')
72/94:
state_big = { 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9, 'MI': 14, 'MN': 8, 'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NY': 27, 'NC': 13, 'ND': 1, 'OH': 16, 'OK': 5,
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'UT': 4, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'GA': 14, 'HI': 2, 'IL': 18,}
72/95:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_big = pd.DataFrame(dic_list)

USA_big.head()
72/96:
state_big = { 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9,  'MN': 8, 'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NY': 27, 'NC': 13, 'ND': 1, 'OH': 16, 'OK': 5,
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'UT': 4, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14,}
72/97:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_big = pd.DataFrame(dic_list)

USA_big.head()
72/98:
state_big = { 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9,   'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NY': 27, 'NC': 13, 'ND': 1, 'OH': 16, 'OK': 5,
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'UT': 4, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8,}
72/99:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_big = pd.DataFrame(dic_list)

USA_big.head()
72/100:
state_big = { 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9,   'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NC': 13, 'ND': 1, 'OH': 16, 'OK': 5,
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'UT': 4, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, }
72/101:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_big = pd.DataFrame(dic_list)

USA_big.head()
72/102:
state_big = { 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9,   'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NC': 13, 'ND': 1, 'OH': 16, 
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'UT': 4, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, }
72/103:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_big = pd.DataFrame(dic_list)

USA_big.head()
72/104:
state_big = { 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9,   'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NC': 13, 'ND': 1, 'OH': 16, 
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
72/105:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_big = pd.DataFrame(dic_list)

USA_big.head()
72/106: USA_done.shape
72/107: USA_big.shape
72/108: USA_big.to_csv('USA_big.csv')
72/109:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
72/110: USA_done.shape
72/111: USA_big.shape #USA_done.shape
72/112: USA_big
72/113:
USA_big.shape 

USA_done.shape
72/114:
#USA_big.shape 

USA_done.shape
72/115:
state_big = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4, 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9,   'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NC': 13, 'ND': 1, 'OH': 16, 
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
72/116:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
72/117:
state_big = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,   'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4, 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9,   'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NC': 13, 'ND': 1, 'OH': 16, 
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
72/118:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
72/119:
#USA_big.shape 

USA_done.shape
72/120: USA_done
72/121: USA_done.to_csv('USA_done.csv')
72/122:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'CT': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
72/123:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'CT': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
72/124:
state = 'CT'
d_num = 03

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
72/125:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
72/126:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
72/127:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN2 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_02 = pd.DataFrame(df_TN1)
TN_02.head()
72/128:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
72/129:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
72/130:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name[]
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
72/131:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
72/132:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)
72/133:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[0]
72/134:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1 [0])
72/135:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[0])
72/136:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
72/137:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])

print(Name)
72/138:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])

print(list_item1, Name)
72/139:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN2 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_02 = pd.DataFrame(df_TN1)
TN_02.head()
72/140:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN2 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_02 = pd.DataFrame(df_TN1)
TN_02.head()
72/141:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1, Name)
72/142:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


#print(list_item1, Name)
72/143:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
72/144:
state = 'CT'
d_num = 3

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
72/145:
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/146:
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/147: USA_done.to_csv('USA_done.csv')
72/148:
state = 'CT'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
72/149:
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/150:
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT01&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/151:
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT04&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/152:
state = 'CT'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
72/153:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT05&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/154:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO075&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/155:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO7&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/156:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO06&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/157:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO07&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/158:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO06&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/159:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO05&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/160:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO04&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/161:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO03&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/162:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/163:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/164:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'GA'
d_num = 1

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/165:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'GA'
d_num = 1

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/166:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'GA'
d_num = 2

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
72/167:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'GA'
d_num = 3

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
73/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
73/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
73/3:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
73/4:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
73/5:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
73/6:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
73/7:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
73/8:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_01 = pd.DataFrame(df_TN1)
TN_01.head()
73/9:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
73/10:
state = 'TN'

count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    Name = []
    Party = []
    pct_votes = []
    Raised = []
    Spent = []
    Winner = []
    Incumbent = []
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        type(list_itemY)
        
    count+=1 # increment count
73/11:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = (xyz.text).replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
Name = re.search(r"(.+)\(([A-Z])\)", xyz)[1]
type(xyz)
xyz
Name
73/12:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+str(d_num)+'&spec=N'
print(URLOX)
73/13:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
73/14:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN2 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_02 = pd.DataFrame(df_TN1)
TN_02.head()
73/15:
state = {'TN': 9, 'CA':53}

for key, value in state.items():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
73/16:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
73/17: TN_CA.shape
73/18:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
73/19:
url_state = 'https://www.britannica.com/topic/United-States-House-of-Representatives-Seats-by-State-1787120'
r_state = requests.get(url_state)
soup_state = BS(r_state.text,'html.parser')

co_table = pd.read_html(str(soup_state.find('table')))
co_table_df = co_table[0]
co_table_df = co_table_df.drop(co_table_df.tail(1).index)
co_table_df.head()
73/20:
state_abb = {
    "Alabama": "AL", "Alaska": "AK", "Arizona": "AZ",
    "Arkansas": "AR", "California": "CA","Colorado": "CO",
    "Connecticut": "CT", "Delaware": "DE", "Florida": "FL",
    "Georgia": "GA", "Hawaii": "HI", "Idaho": "ID",
    "Illinois": "IL", "Indiana": "IN", "Iowa": "IA",
    "Kansas": "KS", "Kentucky": "KY", "Louisiana": "LA",
    "Maine": "ME", "Maryland": "MD", "Massachusetts": "MA",
    "Michigan": "MI", "Minnesota": "MN", "Mississippi": "MS",
    "Missouri": "MO","Montana": "MT", "Nebraska": "NE",
    "Nevada": "NV", "New Hampshire": "NH", "New Jersey": "NJ",
    "New Mexico": "NM", "New York": "NY", "North Carolina": "NC",
    "North Dakota": "ND", "Ohio": "OH","Oklahoma": "OK",
    "Oregon": "OR", "Pennsylvania": "PA", "Rhode Island": "RI",
    "South Carolina": "SC", "South Dakota": "SD",
    "Tennessee": "TN", "Texas": "TX", "Utah": "UT",
    "Vermont": "VT", "Virginia": "VA", "Washington": "WA",
    "West Virginia": "WV", "Wisconsin": "WI", "Wyoming": "WY"
    }
73/21:
co_table_df['state_code'] = co_table_df.state.map(state_abb)
co_table_df.head()
73/22:
state_rep = dict(zip(co_table_df.state_code, co_table_df.representatives))
state_rep
73/23:
state_big = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,   'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4, 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9,   'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NC': 13, 'ND': 1, 'OH': 16, 
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
73/24:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'CT': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
73/25:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'CT': 2}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
73/26:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'CO': 7}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
73/27:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'CO': 3}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
73/28:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'CO': 2}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
73/29:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'UT': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
73/30:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'CT': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
73/31:
state = 'CT'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
73/32:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'GA'
d_num = 3

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
73/33:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
73/34:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'OK': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
73/35:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'OK': 3}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broke read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
73/36:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'GA'
d_num = 3

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    ST.append(key)
    ST_DIT.append(f"{i:02}")
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
73/37:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
74/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
74/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
74/3:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
74/4:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
74/5:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/6:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
74/7:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
74/8:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_01 = pd.DataFrame(df_TN1)
TN_01.head()
74/9:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
74/10:
state = 'TN'

count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    Name = []
    Party = []
    pct_votes = []
    Raised = []
    Spent = []
    Winner = []
    Incumbent = []
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        type(list_itemY)
        
    count+=1 # increment count
74/11:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = (xyz.text).replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
Name = re.search(r"(.+)\(([A-Z])\)", xyz)[1]
type(xyz)
xyz
Name
74/12:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+str(d_num)+'&spec=N'
print(URLOX)
74/13:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
74/14:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN2 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_02 = pd.DataFrame(df_TN1)
TN_02.head()
74/15:
state = {'TN': 9, 'CAL':53}

for key, value in state.items():
    for i in range(1, value+1):
        print(str(key) + " " + str(i))
74/16:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.head()
74/17: TN_CA.shape
74/18:
url_state = 'https://www.britannica.com/topic/United-States-House-of-Representatives-Seats-by-State-1787120'
r_state = requests.get(url_state)
soup_state = BS(r_state.text,'html.parser')

co_table = pd.read_html(str(soup_state.find('table')))
co_table_df = co_table[0]
co_table_df = co_table_df.drop(co_table_df.tail(1).index)
co_table_df.head()
74/19:
state_abb = {
    "Alabama": "AL", "Alaska": "AK", "Arizona": "AZ",
    "Arkansas": "AR", "California": "CA","Colorado": "CO",
    "Connecticut": "CT", "Delaware": "DE", "Florida": "FL",
    "Georgia": "GA", "Hawaii": "HI", "Idaho": "ID",
    "Illinois": "IL", "Indiana": "IN", "Iowa": "IA",
    "Kansas": "KS", "Kentucky": "KY", "Louisiana": "LA",
    "Maine": "ME", "Maryland": "MD", "Massachusetts": "MA",
    "Michigan": "MI", "Minnesota": "MN", "Mississippi": "MS",
    "Missouri": "MO","Montana": "MT", "Nebraska": "NE",
    "Nevada": "NV", "New Hampshire": "NH", "New Jersey": "NJ",
    "New Mexico": "NM", "New York": "NY", "North Carolina": "NC",
    "North Dakota": "ND", "Ohio": "OH","Oklahoma": "OK",
    "Oregon": "OR", "Pennsylvania": "PA", "Rhode Island": "RI",
    "South Carolina": "SC", "South Dakota": "SD",
    "Tennessee": "TN", "Texas": "TX", "Utah": "UT",
    "Vermont": "VT", "Virginia": "VA", "Washington": "WA",
    "West Virginia": "WV", "Wisconsin": "WI", "Wyoming": "WY"
    }
74/20:
co_table_df['state_code'] = co_table_df.state.map(state_abb)
co_table_df.head()
74/21:
state_rep = dict(zip(co_table_df.state_code, co_table_df.representatives))
state_rep
74/22:
state_big = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,   'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4, 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9,   'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NC': 13, 'ND': 1, 'OH': 16, 
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
74/23:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

CT = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in CT.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
74/24:
TN_CA.shape
TN_CA.tail(20)
74/25: TN_CA.to_csv('TN_CAL.csv')
74/26:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'NY'
d_num = 1

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    ST.append(key)
    ST_DIT.append(f"{i:02}")
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
74/27:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
74/28:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'CT'
d_num = 3

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    ST.append(key)
    ST_DIT.append(f"{i:02}")
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
74/29:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'CT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    ST.append(key)
    ST_DIT.append(f"{i:02}")
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
74/30:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'CT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    ST.append(sate)
    ST_DIT.append(d_num)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
74/31:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'CT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    ST.append(state)
    ST_DIT.append(d_num)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
74/32:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

notdone = {'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
74/33:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

notdone = {'CO': 7, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
74/34:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

notdone = {'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
74/35:
state = 'GA'
d_num = 5

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
74/36:
state = 'GA'
d_num = 5

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT02&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
74/37:
state = 'GA'
d_num = 5

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT02&spec=N'

URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
74/38:
state = 'GA'
d_num = 5

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CT02&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
74/39:
state = 'GA'
d_num = 5

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
URLOX1 = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'


#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
74/40:
state = 'GA'
d_num = 5

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
URLOX1 = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'


#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
#print(URLOX)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
74/41:
state = 'GA'
d_num = 5

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLOX1 = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'


#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1

Name=[]
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
if x:
    Winner.append("Yes")
else:
    Winner.append("No")
        
y = re.search(r"\bIncumbent", list_itemX1)

if y:
    Incumbent.append("Yes")
else:
    Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)


print(list_item1)
74/42:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

notdone = {'GA': 4, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
74/43:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

notdone = {'GA': 4, 'HI': 1, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
74/44:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
74/45:
state = 'HI'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
URLOX1 = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'


#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX1)

responseX1 = requests.get(URLOX1)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')

list_itemX1 = soupX1.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX1
list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
74/46:
#USA_big.shape 

USA_done.shape
74/47: print(state)
74/48: USA_done
74/49: USA_done.to_csv('USA_done.csv')
74/50:
#not done #error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

state = 'UT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')



Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text

    list_itemX1 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX1)[1])
    
    x = re.search(r"\bWinner", list_itemX1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

dic = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
df = pd.DataFrame(dic)
df.head()
74/51:
state = 'UT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
#responseX1.text         # helps open the object to see it looks
#soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
#soupX1.find('title')
#list_itemX1 = list_itemX1.text
74/52:
state = 'UT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
#soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
#soupX1.find('title')
#list_itemX1 = list_itemX1.text
74/53:
state = 'UT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
#soupX1.find('title')
#list_itemX1 = list_itemX1.text
74/54:
state = 'UT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
#list_itemX1 = list_itemX1.text
74/55:
state = 'UT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
74/56:
state = 'UT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
74/57:
state = 'UT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
74/58:
state = 'UT'
d_num = 4

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
74/59:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/60:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/61:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/62:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1]).sort()
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/63:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "").sort()
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1]).sort()
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/64:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2.sort())[1]).sort()
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/65:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2.sort())[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/66:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    list_itemX2.sort()
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/67:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    list_itemX2.sort()
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1])
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/68:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    list_itemX2.sort()
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1])
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/69:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    list_itemX2.sort()
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1])
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/70:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    list_itemX2.sort()
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1])
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
    print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/71:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    list_itemX2.sort()
    print(list_itemX2)
    print(Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1]))
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
    #print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/72:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    list_itemX2.sort()
    print(list_itemX2)
    print(Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1]))
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/73:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    list_itemX2.sort()
    print(list_itemX2)
   # print(Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1]))
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/74:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    print(list_itemX2)
   # print(Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1]))
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/75:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1]))
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/76:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1]))
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/77:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1]))
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/78:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2.[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/79:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/80:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1]).sort()
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/81:
state = 'UT'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/82:
notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/83:
#notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/84:
#notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/85:
#notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/86:
#notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/87:
#notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/88:
#notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'NY': 27, 'OK': 5, 'UT': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/89:
#notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'OK': 5, 'UT': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/90:
#notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'UT': 4}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/91:
#notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'CA': 53}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/92:
#notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'CAL': 53}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/93: not_done.shape
74/94: not_done
74/95: not_done.head(20)
74/96:
#notdone = {'CA':53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'CA': 28}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
not_done = pd.DataFrame(dic_list)
not_done.head()
74/97: not_done.shape
74/98: not_done.head(20)
74/99: not_done.to_csv('CA_29.csv')
74/100: not_done.to_csv('CA_28.csv')
76/1:
import requests
import matplotlib.pyplot as plt
76/2:
endpoint = 'http://numbersapi.com/8'

response = requests.get(endpoint)
76/3: response
76/4: response.text
76/5:
number = 95

endpoint = f'http://numbersapi.com/{number}'

response = requests.get(endpoint)

response.text
76/6:
number_range = "1..10"

endpoint = f'http://numbersapi.com/{number_range}'

response = requests.get(endpoint)
76/7: response.text
76/8:
res = response.json()
res
76/9: res['5']
76/10:
endpoint = 'http://numbersapi.com/random'

response = requests.get(endpoint)

response.text
76/11:
endpoint = 'http://numbersapi.com/random'

params = {
    'min': 500,
    'max': 600
}

response = requests.get(endpoint, params = params)

response.text
76/12: import json
76/13:
with open('keys.json') as fi:
    credentials = json.load(fi)
76/14: api_key = credentials['api_key']
76/15: endpoint = ''
76/16:
params = {

}
76/17: response = requests.get(endpoint, params = params)
76/18:
endpoint = 'http://numbersapi.com/30'

response = requests.get(endpoint)
76/19: response
76/20: response.text
76/21:
endpoint = 'http://numbersapi.com/8'

response = requests.get(endpoint)
76/22: response
76/23: response.text
74/101:
#notdone = {'CA':53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
notdone = {'CO': 7}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
CO_not_done = pd.DataFrame(dic_list)
CO_not_done.head()
76/24:
params = {
    'min':200
    'max':300

}
76/25:
params = {
    'min':200,
    'max':300

}
76/26: response = requests.get(endpoint, params = params)
76/27: import json
76/28:
with open('keys.json') as fi:
    credentials = json.load(fi)
76/29: api_key = credentials['https://api.nasa.gov/planetary/apod?api_key=gbQBMLXjkEgh6rY3kpe53Oa64Lb6rGgD4v5eYVLW]
76/30: api_key = credentials['https://api.nasa.gov/planetary/apod?api_key=gbQBMLXjkEgh6rY3kpe53Oa64Lb6rGgD4v5eYVLW']
76/31: import json
76/32:
with open('keys.json') as fi:
    credentials = json.load(fi)
76/33: api_key = credentials['https://api.nasa.gov/planetary/apod?api_key=gbQBMLXjkEgh6rY3kpe53Oa64Lb6rGgD4v5eYVLW']
76/34: api_key = credentials['api_key']
76/35: api_key
76/36:
with open('keys.json') as fi:
    credentials = json.load(fi)
76/37: api_key = credentials['api_key']
76/38: api_key
74/102: CO_not_done.shape
74/103: CO_not_done.to_csv('CO.csv')
76/39: endpoint = 'https://api.nasa.gov/neo/rest/v1/feed'
76/40:
#?start_date=START_DATE&end_date=END_DATE&api_key=API_KEY'
params = {
    'start_date': '2022-01-01',
    'end_date': '2022-01-07',
    'api_key': api_key
}
76/41: response = requests.get(endpoint, params = params)
76/42: response
76/43:
res = response.json()
res
76/44: res.keys()
76/45:
# Your Code Here
len(res['near_earth_objects']['2022-01-03'])
76/46:
# Your Code Here
res['near_earth_objects']['2022-01-03'][0]['is_potentially_hazardous_asteroid']
74/104: USA_done.head(50)
74/105: USA_done.shape
74/106:
CO_not_done.shape
CO_not_done["Party"].valuecounts()
74/107:
CO_not_done.shape
CO_not_done["Party"].value_counts()
74/108:
Leftover = {'CA':53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
#notdone = {'CO': 7}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in leftover.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
leftover = pd.DataFrame(dic_list)
leftover.head(20)
74/109:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
#notdone = {'CO': 7}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in st_notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
leftover = pd.DataFrame(dic_list)
leftover.head(20)
74/110:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
#notdone = {'GA': 14}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in st_notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
leftover = pd.DataFrame(dic_list)
leftover.head(20)
74/111:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
#notdone = {'GA': 14, 'OK': 5,}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in st_notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
leftover = pd.DataFrame(dic_list)
leftover.head(20)
74/112: leftover.shape
74/113: leftover.to_csv(leftover.csv)
74/114: leftover.to_csv('leftover.csv')
74/115:
leftover.shape
CO_not_done["Party"].value_counts()
74/116:
leftover.shape
leftover["Party"].value_counts()
74/117:
leftover.shape
leftover["Party"].value_counts()
leftover['State'].value_cunts()
74/118:
leftover.shape
leftover["Party"].value_counts()
leftover['State'].value_counts()
74/119: leftover.to_csv('leftover.csv')
74/120:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    #Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/121:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/122:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    #Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/123:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    #pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/124:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/125:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/126:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/127:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[0])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/128:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/129:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5,}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA_OK.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA_OK = pd.DataFrame(dic_list)
GA_OK.head(20)
74/130:
state = 'OK'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/131:
state = 'OK'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/132:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5,}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA_OK.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA_OK = pd.DataFrame(dic_list)
GA_OK.head(20)
74/133:
state = 'OK'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/134:
state = 'OK'
d_num = 4

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/135:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/136:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[2])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/137:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[0])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/138:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[0])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/139:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[0])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/140:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[0])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/141:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/142:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/143:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            if s:
                Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            else:
                Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/144:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            if s == (re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
                Spent.append(s)
            else:
                Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/145:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            if s == (re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]):
                Spent.append(s)
            else:
                Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/146:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            if re.match((r"\bSpent\b\:\W\$\d*", list_itemX)) is not None:
                Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            else:
                Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/147:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            if re.match(("\bSpent\b\:\W\$\d*", list_itemX)) is not None:
                Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            else:
                Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/148:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in the list_itemx:
                if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/149:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemx:
                if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/150:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/151:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/152:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)\W", list_itemX)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/153:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)\W", element)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/154:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/155:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/156:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/157:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/158:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            #for element in list_itemX:
                #if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                   # Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
               # else:
                   # Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/159:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            #for element in list_itemX:
                #if re.match("(\bSpent\b\:\W\$\d*)", element) is not None:
                   # Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
               # else:
                   # Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/160:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/161:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/162:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX.text:
                if re.match("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX.text)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX.text)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/163:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX.text)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX.text)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/164:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX.text)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/165:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "").text
            
            print(list_itemX)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            for element in list_itemX:
                if re.match("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX.text)[1]))
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/166:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/167:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
GA_OK = {'GA': 14, 'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/168: GA.shape
74/169: GA.to_csv('GA.csv')
74/170:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA = pd.DataFrame(dic_list)
GA.head(20)
74/171:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in OK.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
OK = pd.DataFrame(dic_list)
OK.head(20)
74/172:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in OK.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
OK = pd.DataFrame(dic_list)
OK.head(20)
74/173: OK.shape
74/174: OK.to_csv('OK.csv')
74/175:
for element in list_itemX2:
                if re.match("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX2)[1]))
74/176:
for element in list_itemX2:
                if re.match("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX2)[1]))
                print(Spent)
74/177:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
OK = pd.DataFrame(dic_list)
OK.head(20)
74/178:
for element in list_itemX2:
                if re.search("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX2)[1]))
                print(Spent)
74/179:
for element in list_itemX2:
                if re.search("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\(-\$\d*)", list_itemX2)[1]))
                print(Spent)
74/180:
for element in list_itemX2:
                if re.search("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX2)[1]))
                print(Spent)
74/181:
GA_OK = pandas.concat([GA, OK])
GA_Ok.shape
74/182:
GA_OK = pd.concat([GA, OK])
GA_Ok.shape
74/183:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GAdf = pd.DataFrame(dic_list)
GAdf.head(20)
74/184: GAdf.to_csv('GA.csv')
74/185:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in OK.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
OKdf = pd.DataFrame(dic_list)
OKdf.head(20)
74/186: OKdf.to_csv('OK.csv')
74/187:
GA_OK = pd.concat([GAdf, OKdf])
GA_Ok.shape
74/188: GA_OK = pd.concat([GAdf, OKdf])
74/189: GA_OK.head()
74/190: GA_OK.tail()
74/191: GA_OK.shape()
74/192: GA_OK.shape
74/193:
for element in list_itemX2:
                if re.search("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX2)[1]))
                print(Spent)
74/194: OK.shape
74/195:
for element in list_itemX2:
                if re.search("(\bSpent\b\:\W\-\$(\d*))", element) is None:
                    Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1]))
                else:
                    Spent.append((re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX2)[1]))
                print(Spent)
74/196: not_done.head(20)
74/197: not_done.shape
74/198: not_done['State'].value_counts()
74/199:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in st_notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            #print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_estND = pd.DataFrame(dic_list)
USA_restND.head(20)
74/200: USA_estND.had()
74/201: USA_estND.head()
74/202: USA_estND.shape
74/203:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in st_notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            #print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_restND = pd.DataFrame(dic_list)
USA_restND.head(20)
74/204: USA_restND.shape
74/205: USA_estND.shape
74/206:
USA_estND.shape #(282, 9)
USA_restND.shape
74/207: USA_done.shape
74/208: USA_DND = pd.concat([USA_done, USA_restND])
74/209: USA_DND.shape
74/210: USA_DNA.to_csv('../data/USA_exceptGA_OK.csv')
74/211: USA_DND.to_csv('../data/USA_exceptGA_OK.csv')
74/212: USA_DND.to_csv('USA_exceptGA_OK.csv')
74/213:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
            
            if re.search("(\bSpent\b\:\W\-\$(\d*))", list_itemX2) is None:
                Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1]))
            else:
                Spent.append((re.search(r"\bSpent\b\:\W\(-\$\d*)", list_itemX2)[1]))
                
                  

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]        
                  
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA5 = pd.DataFrame(dic_list)
GA5.head(20)
77/1:
import requests
import matplotlib.pyplot as plt
74/214: GA_Ok.to_csv('GA_OK_notaccurate.csv')
77/2:
endpoint = ' https://data.nashville.gov/Police/Metro-Nashville-Police-Department-Incidents/2u6v-ujjs.'

response = requests.get(endpoint)
77/3: response
77/4:
endpoint = ' https://data.nashville.gov/Police/Metro-Nashville-Police-Department-Incidents.'

response = requests.get(endpoint)
77/5: response
77/6:
endpoint = 'https://data.nashville.gov/Police/Metro-Nashville-Police-Department-Incidents/2u6v-ujjs'

response = requests.get(endpoint)
77/7: response
77/8: response.text
77/9:
API_endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'
API_response = request.get(API_endpoint)
77/10:
API_endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'
API_response = requests.get(API_endpoint)
77/11:
response
API_response
77/12: API_response.text
74/215:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
            
            if re.search(r"(\bSpent\b\:\W\-\$(\d*))", list_itemX2) is None:
                Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1]))
            else:
                Spent.append((re.search(r"\bSpent\b\:\W\(-\$\d*)", list_itemX2)[1]))
                
                  

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]        
                  
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA5 = pd.DataFrame(dic_list)
GA5.head(20)
74/216:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
            
            if re.search(r"(\bSpent\b\:\W\-\$(\d*))", list_itemX2) is None:
                Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
            else:
                Spent.append((re.search(r"\bSpent\b\:\W\(-\$\d*)", list_itemX2)[1]))
                
                  

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]        
                  
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA5 = pd.DataFrame(dic_list)
GA5.head(20)
74/217:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
            
            if re.search(r"(\bSpent\b\:\W\-\$(\d*))", list_itemX2) is None:
                Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
            else:
                Spent.append(re.search(r"\bSpent\b\:\W\(-\$\d*)", list_itemX2)[1])
                
                  

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]        
                  
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA5 = pd.DataFrame(dic_list)
GA5.head(20)
74/218:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
            
            if re.search(r"(\bSpent\b\:\W\-\$(\d*))", list_itemX2) is None:
                Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
            else:
                Spent.append(re.search(r"\bSpent\b\:\W\(-\$\d*)", list_itemX2)[1])
                
                  

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]        
                  
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA5 = pd.DataFrame(dic_list)
GA5.head(20)
74/219:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
74/220:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
74/221:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
74/222:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
74/223:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/224:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
74/225:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
74/226:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_01 = pd.DataFrame(df_TN1)
TN_01.head()
74/227:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
74/228:
state = 'TN'

count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    Name = []
    Party = []
    pct_votes = []
    Raised = []
    Spent = []
    Winner = []
    Incumbent = []
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        type(list_itemY)
        
    count+=1 # increment count
74/229:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = (xyz.text).replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
Name = re.search(r"(.+)\(([A-Z])\)", xyz)[1]
type(xyz)
xyz
Name
74/230:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+str(d_num)+'&spec=N'
print(URLOX)
74/231:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
74/232:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.tai()
74/233:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.tail()
74/234:
url_state = 'https://www.britannica.com/topic/United-States-House-of-Representatives-Seats-by-State-1787120'
r_state = requests.get(url_state)
soup_state = BS(r_state.text,'html.parser')

co_table = pd.read_html(str(soup_state.find('table')))
co_table_df = co_table[0]
co_table_df = co_table_df.drop(co_table_df.tail(1).index)
co_table_df.head()
74/235:
state_abb = {
    "Alabama": "AL", "Alaska": "AK", "Arizona": "AZ",
    "Arkansas": "AR", "California": "CA","Colorado": "CO",
    "Connecticut": "CT", "Delaware": "DE", "Florida": "FL",
    "Georgia": "GA", "Hawaii": "HI", "Idaho": "ID",
    "Illinois": "IL", "Indiana": "IN", "Iowa": "IA",
    "Kansas": "KS", "Kentucky": "KY", "Louisiana": "LA",
    "Maine": "ME", "Maryland": "MD", "Massachusetts": "MA",
    "Michigan": "MI", "Minnesota": "MN", "Mississippi": "MS",
    "Missouri": "MO","Montana": "MT", "Nebraska": "NE",
    "Nevada": "NV", "New Hampshire": "NH", "New Jersey": "NJ",
    "New Mexico": "NM", "New York": "NY", "North Carolina": "NC",
    "North Dakota": "ND", "Ohio": "OH","Oklahoma": "OK",
    "Oregon": "OR", "Pennsylvania": "PA", "Rhode Island": "RI",
    "South Carolina": "SC", "South Dakota": "SD",
    "Tennessee": "TN", "Texas": "TX", "Utah": "UT",
    "Vermont": "VT", "Virginia": "VA", "Washington": "WA",
    "West Virginia": "WV", "Wisconsin": "WI", "Wyoming": "WY"
    }
74/236:
co_table_df['state_code'] = co_table_df.state.map(state_abb)
co_table_df.head()
74/237:
state_rep = dict(zip(co_table_df.state_code, co_table_df.representatives))
state_rep
74/238:
state_big = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,   'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4, 'KS': 4, 'KY': 6, 'LA': 6, 'ME': 2, 'MD': 8, 'MA': 9,   'MS': 4, 'MO': 8, 'MT': 1, 'NE': 3, 'NV': 4, 'NH': 2, 'NJ': 12, 'NM': 3, 'NC': 13, 'ND': 1, 'OH': 16, 
             'OR': 5, 'PA': 18, 'RI': 2, 'SC': 7, 'SD': 1, 'TN': 9, 'TX': 36, 'VT': 1, 'VA': 11, 'WA': 10, 'WV': 3,
             'WI': 8, 'WY': 1}
#done{'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4,  'CT': 5, 'DE': 1, 'FL': 27,  'ID': 2, 'IN': 9, 'IA': 4,}
#error{'CA': 53, 'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4, }

#CA is part of the TN_CA df as
74/239:

state_A = {'AL': 7, 'AK': 1, 'AZ': 9, 'AR': 4}
state_C = {'CA': 53, 'CO': 7, 'CT': 5}
state_DF = {'DE': 1, 'FL': 27}
state_GH = {'GA': 14, 'HI': 2}
state_I = {'ID': 2, 'IN': 9, 'IA': 4}

notdone = {'CO': 7, 'CT': 5, 'GA': 14, 'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27, 'OK': 5, 'UT': 4}
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in state_big.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_done = pd.DataFrame(dic_list)

USA_done.head()
77/13: API_response.text
74/240:
state = 'GA'
d_num = 5

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []

#URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=CO01&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX1 = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX1)       # it is a requests.status.models.Response

responseX1.status_code  #200 not broke read properly
responseX1.text         # helps open the object to see it looks
soupX1 = BS(responseX1.text) #converts the response object in Beautiful object
#print(soup.prettify())
soupX1.find('title')
for list_itemX1 in soupX1.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX1 = list_itemX1.text
    #print(list_itemX1)
    list_itemX2 = list_itemX1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    print(list_itemX2)
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX2)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
    
    
    #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
    Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])
    #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
    Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
    
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
74/241:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in st_notdone.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            #print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]
            
            
            
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
USA_restND = pd.DataFrame(dic_list)
USA_restND.head(20)
74/242:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
            
            if re.search(r"(\bSpent\b\:\W\-\$(\d*))", list_itemX2) is None:
                Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
            else:
                Spent.append(re.search(r"\bSpent\b\:\W\(-\$\d*)", list_itemX2)[1])
                
                  

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]        
                  
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA5 = pd.DataFrame(dic_list)
GA5.head(20)
77/14: Import json
77/15: import json
77/16: import json
78/1:
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import folium
from folium.plugins import MarkerCluster
from folium.plugins import FastMarkerCluster
78/2:
zipcodes = gpd.read_file('../data/Zip Codes.geojson')
zipcodes.head( )
78/3: type(zipcodes)
78/4: zipcodes.crs
78/5: zipcodes.loc[0, 'geometry']
78/6: print(zipcodes.loc[0, 'geometry'])
78/7: zipcodes.plot();
78/8:
bus_stops = pd.read_csv('../data/WeGo_Transit_Bus_Stops.csv')
bus_stops.head(10)
78/9: bus_stops.info()
78/10:
bus_stops[['lat', 'lng']] = (
    bus_stops
    ['Mapped Location']
    .str.strip('()')
    .str.split(',', expand = True)
    .astype(float)
)
78/11:
from shapely.geometry import Point
import shapely
78/12:
bus_stops['geometry'] = gpd.points_from_xy(bus_stops['lng'], bus_stops['lat'])

bus_stops.head()
78/13:
bus_geo = gpd.GeoDataFrame(bus_stops, 
                           crs = zipcodes.crs, 
                           geometry = bus_stops['geometry'])
78/14: type(bus_geo)
78/15: zipcodes = zipcodes[['zipcode', 'poname', 'geometry']]
78/16: stops_by_zip = gpd.sjoin(bus_geo, zipcodes, predicate = 'within')
78/17: stops_by_zip.head()
78/18: stops_by_zip['zipcode'].value_counts()
78/19:
stops_in_37207 = stops_by_zip.loc[stops_by_zip['zipcode'] == '37207']
stops_in_37207.shape
78/20:
polygon37207 = zipcodes.loc[zipcodes['zipcode'] == '37207']
polygon37207.shape
78/21: stops_by_zip
78/22:
ax = polygon37207.plot(figsize = (8, 10), color = 'lightgreen')
stops_in_37207.plot( ax = ax, column = 'Route Numbers', legend = True);
78/23: polygon37207.to_crs('+proj=cea').centroid.to_crs(polygon37207.crs)
78/24: center = polygon37207.to_crs('+proj=cea').centroid.to_crs(polygon37207.crs).iloc[0]
74/243:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
            
            if re.search(r"(\bSpent\b\:\W\-\$(\d*))", list_itemX2) is None:
                Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
            else:
                Spent.append(re.search(r"\bSpent\b\:\W\(-\$\d*)", list_itemX2)[1])
                
                  

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]        
                  
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA5 = pd.DataFrame(dic_list)
GA5.head(20)
74/244:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
            
            if re.search(r"(\bSpent\b\:\W\-\$(\d*))", list_itemX2) is None:
                Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
            else:
                Spent.append(re.search(r"\bSpent\b\:\W(-\$\d*)", list_itemX2)[1])
                
                  

            #Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]        
                  
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA5 = pd.DataFrame(dic_list)
GA5.head(20)
74/245:
st_notdone = {'CA':53, 'CO': 7, 'CT': 5,  'HI': 2, 'IL': 18, 'MI': 14, 'MN': 8, 'NY': 27,  'UT': 4}
GA_OK = {'GA': 14, 'OK': 5}
GA = {'GA': 14}
OK = {'OK': 5}

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []
failedurls = []


for key, value in GA.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        #type(res1)       # it is a requests.status.models.Response
              
        retry_count = 0
        next_url = False
        while res1.status_code != 200:
            time.sleep(5)
            if retry_count > 10:
                failedurls.append(url)
                next_url = True
                break
            res1 = requests.get(url)
            retry_count +=1
        
        if next_url:
            continue
        #soup = BS(r.text,'html.parser')
    
        #print(url)      
        

        #res1.status_code  #200 not broken read properly
        #res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX2 = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            print(list_itemX2)
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\((.)\)", list_itemX2)[1])
   
            Party.append(re.search(r"(.+)\((.)\)", list_itemX2)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX2)[0])
        
            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX2)[1])
            
            if re.search(r"(\bSpent\b\:\W\-\$(\d*))", list_itemX2) is None:
                Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX2)[1])
            else:
                Spent.append(re.search(r"\bSpent\b\:\W(-\$\d*)", list_itemX2)[1])
                
                  

           
            #Raised.append(re.search(r"\$(\d*)\s*\bSpent\b", list_itemX2)[1])

            #Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
            
            #Spent.append(re.search(r"\$(\d*)\s*\bCash on Hand\b", list_itemX2)[1])
            #s = re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]
            #m = re.search(r"\bSpent\b\:\W\-\$(\d*), list_itemX)[1]        
                  
            
            
            
            #if s == re.match
                #Spent.append((re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1]))
            #else:
                #Spent.append(re.search(r"\bSpent\b\:\W\-\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
GA5 = pd.DataFrame(dic_list)
GA5.head(20)
74/246: dfcsv = pd.read_csv("data/STREP.csv")
74/247: dfcsv = pd.read_excel("data/USCongressionalDistricts.xlsx")
77/17:
endpoint = 'https://data.nashville.gov/Police/Metro-Nashville-Police-Department-Incidents/2u6v-ujjs'

response = requests.get(endpoint)
77/18: response
77/19: response.text
77/20:
MNPDres = response.json()
MNPDres
77/21:
MNPD_res = response.json()
MNPD_res
77/22:
res = response.json()
MNPD_res
77/23:
res = response.json()
res
77/24:
endpoint = 'https://data.nashville.gov/Police/Metro-Nashville-Police-Department-Incidents/2u6v-ujjs.json'

response = requests.get(endpoint)
77/25: response
77/26: response.text
77/27:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
77/28: response
77/29: response.text
77/30:
res = response.json()
res
77/31:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"
}

response = requests.get(endpoint, params = params)

response.text
77/32:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    'zip_code' : 37215
}

response = requests.get(endpoint, params = params)

response.text
77/33:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    'zip_code' : '37215'
}

response = requests.get(endpoint, params = params)

response.text
77/34:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    'zip_code' : '37207'
}

response = requests.get(endpoint, params = params)

response.text
77/35:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response = requests.get(endpoint, params = params)

response.text
77/36:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response = requests.get(endpoint, params = params)

response.text
79/1:
import requests
import matplotlib.pyplot as plt
79/2:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
79/3: response
79/4: response.text
79/5:
res = response.json()
res
79/6:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response = requests.get(endpoint, params = params)

response.text
79/7:
endpoint = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'&zip_code=37215"

response = requests.get(endpoint)

response.text
79/8: response.keys
79/9: (response.json).keys()
79/10: (response.json).keys
79/11:
res = response.json
res
79/12: res.keys()
79/13: res.key()
79/14:
res = response.json()
res

res.keys()
79/15:
import requests
import matplotlib.pyplot as plt
import json
79/16:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
79/17: response
79/18: response.text
79/19:
res = response.json()
res

res.keys()
79/20:
res = response.json()
res
79/21:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response = requests.get(endpoint, params = params)

response.text
79/22:
res = response.json
res
79/23: res.keys()
80/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
80/2:
from scipy.stats import poisson
from ipywidgets import interact, FloatSlider, IntSlider
80/3:
@interact(mu = FloatSlider(value = 2.5, min = 0, max = 5))
def poisson_plot(mu):
    x = np.arange(start = 0, stop = 10, step = 1)
    y = poisson.pmf(x, mu = mu)

    plt.figure(figsize = (10,5))
    plt.bar(x, y, edgecolor = 'black')
    plt.xticks(x)
    plt.xlabel('Number of Occurrences')
    plt.ylabel('Probability')
    plt.title(f'The Poisson Distribution, $\mu = {mu}$');
80/4:
doctor_visits = pd.read_csv('../data/doctor_visits.csv')
doctor_visits.head(2)
80/5: doctor_visits['age'] = doctor_visits['age'] * 100
80/6:
dv_summary = (
    doctor_visits
    .assign(group = pd.qcut(doctor_visits['age'],
                            q = 10,
                           duplicates = 'drop'))
    .groupby('group')
    [['age', 'visits']]
    .mean()
    .reset_index()
)

dv_summary
80/7:
pr_hosp = smf.glm('visits ~ age',
                 data = doctor_visits,
                 family = sm.families.Poisson()
                 ).fit()

pr_hosp.summary()
80/8:
fit_df = pd.DataFrame({
    'age': np.linspace(start = doctor_visits['age'].min(),
                       stop = doctor_visits['age'].max(),
                       num = 150)
})
fit_df['fitted'] = pr_hosp.predict(fit_df)

fit_df.plot(x = 'age', y = 'fitted', figsize = (10,6))

fontsize = 14
plt.xlabel('Age', fontsize = fontsize)
plt.ylabel('Estimated Mean Number of Visits', fontsize = fontsize);
80/9:
@interact(age = IntSlider(value = 40, min = doctor_visits['age'].min(), max = doctor_visits['age'].max()))
def fit_plot(age):
    
    mu = np.exp(pr_hosp.params['Intercept'] + age * pr_hosp.params['age'])
    x = np.arange(start = 0, stop = 10, step = 1)
    y = poisson.pmf(x, mu = mu)

    plt.figure(figsize = (10,5))
    plt.bar(x, y, edgecolor = 'black')
    plt.xticks(x)
    plt.xlabel('Number of Occurrences')
    plt.ylabel('Probability')
    plt.title(f'Estimated Distribution of Number of Visits\nage = {age}\n$\mu$={round(mu, 3)}');
79/24:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response = requests.get(endpoint, params = params)

response.text
79/25:
res = response.json
res
79/26: res.keys()
79/27:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response2 = requests.get(endpoint, params = params)

response2.text
79/28:
res2 = response.json
res2
79/29: res2.keys()
79/30:
res2 = response.json
res2
76/47: import json
76/48:
with open('keys.json') as fi:
    credentials = json.load(fi)
76/49: api_key = credentials['api_key']
76/50: api_key
76/51: endpoint = 'https://api.nasa.gov/neo/rest/v1/feed'
76/52:
#?start_date=START_DATE&end_date=END_DATE&api_key=API_KEY'
params = {
    'start_date': '2022-01-01',
    'end_date': '2022-01-07',
    'api_key': api_key
}
76/53: response = requests.get(endpoint, params = params)
81/1:
import requests
import matplotlib.pyplot as plt
81/2:
endpoint = 'http://numbersapi.com/8'

response = requests.get(endpoint)
81/3: response
81/4: response.text
81/5:
number = 95

endpoint = f'http://numbersapi.com/{number}'

response = requests.get(endpoint)

response.text
81/6:
number_range = "1..10"

endpoint = f'http://numbersapi.com/{number_range}'

response = requests.get(endpoint)
81/7: response.text
81/8:
res = response.json()
res
81/9: res['5']
81/10:
endpoint = 'http://numbersapi.com/random'

response = requests.get(endpoint)

response.text
81/11:
endpoint = 'http://numbersapi.com/random'

params = {
    'min': 500,
    'max': 600
}

response = requests.get(endpoint, params = params)

response.text
81/12: import json
81/13:
with open('keys.json') as fi:
    credentials = json.load(fi)
81/14: api_key = credentials['api_key']
81/15: api_key
81/16: endpoint = 'https://api.nasa.gov/neo/rest/v1/feed'
81/17:
#?start_date=START_DATE&end_date=END_DATE&api_key=API_KEY'
params = {
    'start_date': '2022-01-01',
    'end_date': '2022-01-07',
    'api_key': api_key
}
81/18: response = requests.get(endpoint, params = params)
81/19: response
81/20:
res = response.json()
res  #there are dic with dic within
81/21: res.keys() #print the keys of the dictionary
81/22: res['near_earth_objects']
81/23: res['near_earth_objects'].keys()
81/24:
# Your Code Here
len(res['near_earth_objects']['2022-01-03'])
81/25:
# Your Code Here
res['near_earth_objects']['2022-01-03'][0]['is_potentially_hazardous_asteroid']
81/26: # Your Code Here
81/27:
max_diam = []
hazardous = []
miss_dist = []
for day, objs in res['near_earth_objects'].items():
    for obj in objs:
        max_diam.append(float(obj['estimated_diameter']['miles']['estimated_diameter_max']))
        hazardous.append(obj['is_potentially_hazardous_asteroid'])
        miss_dist.append(float(obj['close_approach_data'][0]['miss_distance']['miles']))

plt.figure(figsize = (17, 10))
plt.scatter(max_diam, miss_dist, c = hazardous)
plt.xlabel('max diameter (miles)')
plt.ylabel('miss distance (miles)');
81/28: import pandas as pd
81/29: pd.DataFrame(response.json()['near_earth_objects']['2022-01-07']).head(2)
81/30: pd.json_normalize(response.json()['near_earth_objects']['2022-01-07']).head(2)
81/31:
response_df = pd.json_normalize(response.json()['near_earth_objects']['2022-01-07'])
response_df.explode('close_approach_data').head(2)
81/32: pd.json_normalize(response_df.explode('close_approach_data')['close_approach_data']).head(2)
81/33:
pd.concat([
    response_df.explode('close_approach_data').drop(columns = ['close_approach_data']),
    pd.json_normalize(response_df.explode('close_approach_data')['close_approach_data'])
], axis = 1).head(2)
81/34:
endpoint = ''

params = {

}
81/35: response = requests.get(endpoint, params = params)
79/31: res.keys()
79/32:
agg_burg = pd.DataFrame(res2)
agg_burg.head
79/33:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
79/34:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
79/35: response
79/36: response.text
79/37:
res = response.json()
res
79/38: res.keys()
79/39: #res.keys()
79/40:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response2 = requests.get(endpoint, params = params)

response2.text
79/41:
res2 = response.json
res2
79/42: res2.keys()
79/43:
agg_burg = pd.DataFrame(res2)
agg_burg.head
79/44: #res2.keys()
79/45:
agg_burg = pd.DataFrame(res2)
agg_burg.head
79/46:
res2 = response.json()
res2
79/47: res2.keys()
79/48:
agg_burg = pd.DataFrame(res2)
agg_burg.head
79/49:
agg_burg = pd.DataFrame(res2)
agg_burg.head()
79/50:
agg_burg = pd.DataFrame(res2)
agg_burg.head()
agg_burg.shape
79/51:
agg_burg = pd.DataFrame(res2)
agg_burg.head()
agg_burg.shape
agg_brug.columns
79/52:
agg_burg = pd.DataFrame(res2)
agg_burg.head()
agg_burg.shape
agg_burg.columns
79/53:
agg_burg = pd.DataFrame(res2)
agg_burg.head()
agg_burg.shape
agg_burg.columns
agg_burg["offense_description"].value_counts()
79/54: agg_burg.offense_description.unique
79/55:
endpoint1 = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"

response1 = requests.get(endpoint)

response1.text
79/56:
endpoint1 = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"

response1 = requests.get(endpoint)

response1.text

res1 = response1.json()
res1
79/57:
endpoint1 = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"

response1 = requests.get(endpoint)

response1.text

res1 = response1.json()
res1
res1.keys()
79/58:
endpoint1 = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"

response1 = requests.get(endpoint)

response1.text

res1 = response1.json()
res1
#res1.keys()

agg_burg1 = pd.DataFrame(res2)
agg_burg1.head()
agg_burg1.shape
agg_burg1.columns
agg_burg1["offense_description"].value_counts()
79/59: agg_burg.offense_description.unique
79/60:
endpoint1 = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"

response1 = requests.get(endpoint)

response1.text

res1 = response1.json()
res1
#res1.keys()

agg_burg1 = pd.DataFrame(res2)
agg_burg1.head()
#agg_burg1.shape
#agg_burg1.columns
#agg_burg1["offense_description"].value_counts()
79/61:
endpoint1 = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"

response1 = requests.get(endpoint)

response1.text

res1 = response1.json()
res1
#res1.keys()

agg_burg1 = pd.DataFrame(res2)
agg_burg1.head()
agg_burg1.shape
#agg_burg1.columns
#agg_burg1["offense_description"].value_counts()
79/62:
endpoint1 = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"

response1 = requests.get(endpoint)

response1.text

res1 = response1.json()
res1
#res1.keys()

agg_burg1 = pd.DataFrame(res2)
agg_burg1.head()
agg_burg1.shape
agg_burg1.columns
#agg_burg1["offense_description"].value_counts()
79/63:
endpoint1 = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"

response1 = requests.get(endpoint)

response1.text

res1 = response1.json()
res1
#res1.keys()

agg_burg1 = pd.DataFrame(res1)
agg_burg1.head()
agg_burg1.shape
agg_burg1.columns
#agg_burg1["offense_description"].value_counts()
79/64: agg_burg1.offense_description.unique
79/65:
endpoint1 = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"

response1 = requests.get(endpoint1)

response1.text

res1 = response1.json()
res1
#res1.keys()

agg_burg1 = pd.DataFrame(res1)
agg_burg1.head()
agg_burg1.shape
agg_burg1.columns
#agg_burg1["offense_description"].value_counts()
79/66: agg_burg1.offense_description.unique
79/67:
endpoint1 = f"https://data.nashville.gov/resource/2u6v-ujjs.json?offense_description=BURGLARY- AGGRAVATED&$where=incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'"

response1 = requests.get(endpoint1)

response1.text

res1 = response1.json()
res1
#res1.keys()

agg_burg1 = pd.DataFrame(res1)
agg_burg1.head()
agg_burg1.shape
agg_burg1.columns
agg_burg1["offense_description"].value_counts()
79/68: res1.keys()
79/69: res1.keys
79/70:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
79/71:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response2 = requests.get(endpoint, params = params)

#response2.text only needed depending on the request. for json from API it is needed
79/72:
res2 = response2.json()
#res2
79/73:
endpoint1 = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response2 = requests.get(endpoint1, params = params)

#response2.text only needed depending on the request. for json from API it is needed
79/74:
res2 = response2.json()
#res2
79/75:
agg_burg = pd.DataFrame(res2)
agg_burg.head()
agg_burg.shape
agg_burg.columns
agg_burg["offense_description"].value_counts()
79/76:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
79/77: response
79/78:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
response
res = response.json() 
res
#response.text #this is not needed as the website sends a json object rather than the url object
res #it is a list of dictionary

#However, it will be easier to work with as a json. We can use the json method to convert the results to a dictionary.
#res.keys() #so this will not work

#as this is a list of dictionaries it can be directly converted to data frame using the pd.DataFrame code.
79/79:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
response
res = response.json() 
res
#response.text #this is not needed as the website sends a json object rather than the url object
res #it is a list of dictionary

#However, it will be easier to work with as a json. We can use the json method to convert the results to a dictionary.
#res.keys() #so this will not work

#as this is a list of dictionaries it can be directly converted to data frame using the pd.DataFrame code.
df = pd.DataFrame(res)
79/80:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
response
res = response.json() 
res
#response.text #this is not needed as the website sends a json object rather than the url object
res #it is a list of dictionary

#However, it will be easier to work with as a json. We can use the json method to convert the results to a dictionary.
#res.keys() #so this will not work

#as this is a list of dictionaries it can be directly converted to data frame using the pd.DataFrame code.
df = pd.DataFrame(res)
df.head()
79/81:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
response
res = response.json() 
res
#response.text #this is not needed as the website sends a json object rather than the url object
res #it is a list of dictionary

#However, it will be easier to work with as a json. We can use the json method to convert the results to a dictionary.
#res.keys() #so this will not work

#as this is a list of dictionaries it can be directly converted to data frame using the pd.DataFrame code.
df = pd.DataFrame(res)
df.head(2)
79/82:
**Part1: Finding the aggravated burglary in Davidson county for the time period 01/01/2022 - 09/30/2022***
    **we will create the response object using the url and the parameters.**
    ** this website does not need an API key**
79/83:
endpoint1 = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response1 = requests.get(endpoint1, params = params)

response1
79/84: ## Converting the resonse object into a json object
79/85:
res1 = response1.json()
#res2
79/86:
agg_burg = pd.DataFrame(res1)
agg_burg.head()
#agg_burg.shape
#agg_burg.columns
#agg_burg["offense_description"].value_counts()
79/87:
agg_burg = pd.DataFrame(res1)
agg_burg.head(3)
#agg_burg.shape
#agg_burg.columns
#agg_burg["offense_description"].value_counts()
79/88:
agg_burg = pd.DataFrame(res1)
agg_burg.head(3)
agg_burg.shape
#agg_burg.columns
#agg_burg["offense_description"].value_counts()
79/89:
agg_burg.columns
#agg_burg["offense_description"].value_counts()
79/90:
agg_burg["offense_description"].value_counts() looking for the description it should only be BURGLARY- AGGRAVATED
#agg_burg.offense_description.unique
79/91:
agg_burg["offense_description"].value_counts() #looking for the description it should only be BURGLARY- AGGRAVATED
#agg_burg.offense_description.unique
79/92:
agg_burg = pd.DataFrame(res1)
agg_burg.head(3)
agg_burg.shape

#We only have 1000 rows we need to increase this becuase this is restricted by the API and there is a parameter that can be set to go upto 2000
79/93:
agg_burg = pd.DataFrame(res1)
agg_burg.head(3)
agg_burg.shape

#We only have 1000 rows we need to increase this becuase this is restricted by the API and 
##there is a parameter that can be set to go upto 2000
79/94: agg_burg.offense_description.unique
79/95:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5

params1 = {
    'NAME' : 'B01001_001E',
    '&for' : 'tract:*&in=county:037&in=state:47'
}
response_pop = requests.get(endpoint1, params = params)

response_pop
79/96:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5'

params1 = {
    'NAME' : 'B01001_001E',
    '&for' : 'tract:*&in=county:037&in=state:47'
}
response_pop = requests.get(endpoint1, params = params)

response_pop
79/97: response_pop.text
79/98:
response_pop.text
response_pop.json()
79/99:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.
79/100:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
79/101:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
79/102:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
TNDA_pop.shape
79/103:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
TNDA_pop.shape
79/104:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
#TNDA_pop.shape
79/105:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
TNDA_pop.shape
79/106:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5'

params1 = {
    'NAME' : 'B01001_001E',
    '&for' : 'tract:*&in=county:037&in=state:47'
}
response_pop = requests.get(endpoint_pop, params = params)

response_pop
79/107:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5'

params1 = {
    'NAME' : 'B01001_001E',
    '&for' : 'tract:*&in=county:037&in=state:47'
}
response_pop = requests.get(endpoint_pop, params = params1)

response_pop
79/108:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'NAME' : 'B01001_001E',
    '&for' : 'tract:*&in=county:037&in=state:47'
}
response_pop = requests.get(endpoint_pop)

response_pop
79/109:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
TNDA_pop.shape
79/110:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
#TNDA_pop.shape
79/111:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TNDA_pop.iloc[0] #pull the row 1 in a list called header
#med_income= med_income[1:] #delete the row 0
TNDA_pop.columns = header
TNDA_pop.head()
79/112:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TNDA_pop.iloc[0] #pull the row 1 in a list called header
TNDA_pop = TNDA_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_pop.columns = header
TNDA_pop.head()
79/113:
#Renaming the column B01001_00IE as population

TNDA_pop = TNDA_pop.rename(columns={'B01001_001E':'Population'})
TNDA_pop.head()
79/114:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

#params1 = {
    'NAME' : 'B01001_001E',
    '&for' : 'tract:*&in=county:037&in=state:47'
}
response_inc = requests.get(endpoint_inc)

response_inc
79/115:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'NAME' : 'B01001_001E',
    '&for' : 'tract:*&in=county:037&in=state:47'
}
response_inc = requests.get(endpoint_inc)

response_inc
79/116:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'NAME' : 'B01001_001E',
    '&for' : 'tract:*&in=county:037&in=state:47'
}
response_inc = requests.get(endpoint_inc)

response_inc
79/117:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'NAME' : 'S1901_C01_012E',
    'for' : 'tract:*&in=county:037&in=state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc
79/118:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'NAME' : 'S1901_C01_012E',
    'for' : 'tract:*&in=county:037&in=state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text
79/119:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract:*&in=county:037&in=state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text
79/120:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text
79/121:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text
79/122:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.jason()
79/123:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
79/124:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_in
79/125:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
79/126:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc.keys()
79/127:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TNDA_inc = pd.DataFrame(res_inc)
TNDA_inc.head()
79/128: TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
79/129:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)
, 
response_pop
79/130:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}


response_pop = requests.get(endpoint_pop)
, 
response_pop
79/131:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}


response_pop = requests.get(endpoint_pop)
, 
response_pop
79/132:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)
, 
response_pop
79/133:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
#TNDA_pop.shape
79/134:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TNDA_pop.iloc[0] #pull the row 1 in a list called header
TNDA_pop = TNDA_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_pop.columns = header
TNDA_pop.head()
79/135:
#Renaming the column B01001_00IE as population

TNDA_pop = TNDA_pop.rename(columns={'B01001_001E':'Population'})
TNDA_pop.head()
79/136:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)
, 
response_pop
79/137:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TNDA_inc = pd.DataFrame(res_inc)
TNDA_inc.head()
TNDA_inc.shape
79/138:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TNDA_inc = pd.DataFrame(res_inc)
TNDA_inc.head()
#TNDA_inc.shape (175, 5)
79/139:
## Naming the columns same as done for the pop because the header row is missing.
header = TNDA_inc.iloc[0] #pull the row 1 in a list called header
TNDA_inc = TNDA_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_inc.columns = header
TNDA_inc.head()
79/140:
#Renaming the column S1901_C01_012E as population

TNDA_inc = TNDA_inc.rename(columns={'S1901_C01_012E':'MedianIncome'})
TNDA_inc.head()
79/141: TNDA_pop_inc = pd.merge(TNDA_pop, TNDA_inc, on=['NAME', 'state', 'county', 'tract'])
79/142:
TNDA_pop_inc = pd.merge(TNDA_pop, TNDA_inc, on=['NAME', 'state', 'county', 'tract'])
TNDA_pop_inc.shape
79/143:
TNDA_pop_inc = pd.merge(TNDA_pop, TNDA_inc, on=['NAME', 'state', 'county', 'tract'])
TNDA_pop_inc.shape(174, 6)
TNDA_pop_inc.head(3)
79/144:
TNDA_pop_inc = pd.merge(TNDA_pop, TNDA_inc, on=['NAME', 'state', 'county', 'tract'])
TNDA_pop_inc.shape #(174, 6)
TNDA_pop_inc.head(3)
83/1:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
83/2:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
response
res = response.json() 
res
#response.text #this is not needed as the website sends a json object rather than the url object
res #it is a list of dictionary

#However, it will be easier to work with as a json. We can use the json method to convert the results to a dictionary.
#res.keys() #so this will not work

#as this is a list of dictionaries it can be directly converted to data frame using the pd.DataFrame code.
df = pd.DataFrame(res)
df.head(2)
83/3:
endpoint1 = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response1 = requests.get(endpoint1, params = params)

response1
83/4:
res1 = response1.json()
#res1
83/5:
agg_burg = pd.DataFrame(res1)
agg_burg.head(3)
agg_burg.shape

#We only have 1000 rows we need to increase this becuase this is restricted by the API and 
##there is a parameter that can be set to go upto 2000
83/6:
agg_burg.columns
#agg_burg["offense_description"].value_counts()
83/7: agg_burg["offense_description"].value_counts() #looking for the description it should only be BURGLARY- AGGRAVATED
83/8: agg_burg.offense_description.unique
83/9:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'


response_pop = requests.get(endpoint_pop)
, 
response_pop
83/10:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)
, 
response_pop
83/11:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
#TNDA_pop.shape
83/12:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TNDA_pop.iloc[0] #pull the row 1 in a list called header
TNDA_pop = TNDA_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_pop.columns = header
TNDA_pop.head()
83/13:
#Renaming the column B01001_00IE as population

TNDA_pop = TNDA_pop.rename(columns={'B01001_001E':'Population'})
TNDA_pop.head()
83/14:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TNDA_inc = pd.DataFrame(res_inc)
TNDA_inc.head()
#TNDA_inc.shape (175, 5)
83/15:
## Naming the columns same as done for the pop because the header row is missing.
header = TNDA_inc.iloc[0] #pull the row 1 in a list called header
TNDA_inc = TNDA_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_inc.columns = header
TNDA_inc.head()
83/16:
#Renaming the column S1901_C01_012E as population

TNDA_inc = TNDA_inc.rename(columns={'S1901_C01_012E':'MedianIncome'})
TNDA_inc.head()
83/17:
TNDA_pop_inc = pd.merge(TNDA_pop, TNDA_inc, on=['NAME', 'state', 'county', 'tract'])
TNDA_pop_inc.shape #(174, 6)
TNDA_pop_inc.head(3)
83/18: TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
83/19: import geopandas as gpd
83/20: TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
83/21:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
83/22:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
TN-shape.shape
83/23:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
TN_shape.shape
83/24:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
#TN_shape.shape #(1701, 13)
83/25: agg_burg1.head()
83/26: # Merging the TN_burgg to TN shape file
83/27: agg_burg1.head()
83/28: agg_burg.head()
83/29:
agg_burg.head
agg_burg.column
83/30:
agg_burg.head
agg_burg.columns
83/31: TN_BURG['geometry'] = gpd.points_from_xy(agg_burg['longitude'], agg_burg['latitude'])
83/32: agg_burg['geometry'] = gpd.points_from_xy(agg_burg['longitude'], agg_burg['latitude'])
83/33:
agg_burg['geometry'] = gpd.points_from_xy(agg_burg['longitude'], agg_burg['latitude'])
agg_burg.head()
83/34:
TN_BURG_GEO = gpd.GeoDataFrame(agg_burg, 
                           crs = TN_shape.crs, 
                           geometry = agg_burg['geometry'])
83/35: TN_BURG_GEO.head()
83/36: type(TN_BURG_GEO)
83/37: TN_BURG_GEO.plot()
83/38: BURG_TNDA = gpd.sjoin(agg_burg, TNDA_pop_inc, predicate = 'within')
83/39: BURG_TNDA = gpd.sjoin(TN_BURG_GEO, TNDA_pop_inc, predicate = 'within')
83/40: BURG_TNDA = gpd.sjoin(TN_BURG_GEO, TN_shape, predicate = 'within')
83/41:
BURG_TNDA = gpd.sjoin(TN_BURG_GEO, TN_shape, predicate = 'within')
BURG_TNDA.shape
83/42: BURG_TNDA.head()
83/43:
BURG_TNDA.head()
BURG_TNDA.columns
83/44: BURG_TNDA.head()
89/1:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
89/2:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
response
res = response.json() 
res
#response.text #this is not needed as the website sends a json object rather than the url object
res #it is a list of dictionary

#However, it will be easier to work with as a json. We can use the json method to convert the results to a dictionary.
#res.keys() #so this will not work

#as this is a list of dictionaries it can be directly converted to data frame using the pd.DataFrame code.
df = pd.DataFrame(res)
df.head(2)
89/3:
endpoint1 = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response1 = requests.get(endpoint1, params = params)

response1
89/4:
res1 = response1.json()
#res1
89/5:
agg_burg = pd.DataFrame(res1)
agg_burg.head(3)
agg_burg.shape

#We only have 1000 rows we need to increase this becuase this is restricted by the API and 
##there is a parameter that can be set to go upto 2000
89/6:
agg_burg.columns
#agg_burg["offense_description"].value_counts()
89/7: agg_burg["offense_description"].value_counts() #looking for the description it should only be BURGLARY- AGGRAVATED
89/8: agg_burg.offense_description.unique
89/9:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'


response_pop = requests.get(endpoint_pop)
, 
response_pop
89/10:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)
, 
response_pop
89/11:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
#TNDA_pop.shape
89/12:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TNDA_pop.iloc[0] #pull the row 1 in a list called header
TNDA_pop = TNDA_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_pop.columns = header
TNDA_pop.head()
89/13:
#Renaming the column B01001_00IE as population

TNDA_pop = TNDA_pop.rename(columns={'B01001_001E':'Population'})
TNDA_pop.head()
89/14:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TNDA_inc = pd.DataFrame(res_inc)
TNDA_inc.head()
#TNDA_inc.shape (175, 5)
89/15:
## Naming the columns same as done for the pop because the header row is missing.
header = TNDA_inc.iloc[0] #pull the row 1 in a list called header
TNDA_inc = TNDA_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_inc.columns = header
TNDA_inc.head()
89/16:
#Renaming the column S1901_C01_012E as population

TNDA_inc = TNDA_inc.rename(columns={'S1901_C01_012E':'MedianIncome'})
TNDA_inc.head()
89/17:
TNDA_pop_inc = pd.merge(TNDA_pop, TNDA_inc, on=['NAME', 'state', 'county', 'tract'])
TNDA_pop_inc.shape #(174, 6)
TNDA_pop_inc.head(3)
89/18: import geopandas as gpd
89/19:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
#TN_shape.shape #(1701, 13)
89/20:
agg_burg.head
agg_burg.columns
89/21:
agg_burg['geometry'] = gpd.points_from_xy(agg_burg['longitude'], agg_burg['latitude'])
agg_burg.head()
89/22:
TN_BURG_GEO = gpd.GeoDataFrame(agg_burg, 
                           crs = TN_shape.crs, 
                           geometry = agg_burg['geometry'])
89/23: type(TN_BURG_GEO)
89/24: TN_BURG_GEO.plot()
89/25:
BURG_TNDA = gpd.sjoin(TN_BURG_GEO, TN_shape, predicate = 'within')
BURG_TNDA.shape
89/26:
BURG_TNDA.head()
BURG_TNDA.columns
89/27: BURG_TNDA.head()
89/28: .groupby(['Year'])['Year'].count()
89/29: BURG_TNDA.columns()
89/30: BURG_TNDA.columns
89/31: TNDA.groupby(['TRACTCE'])['TRACTCE'].count()
89/32: BURG_TNDA.groupby(['TRACTCE'])['TRACTCE'].count()
89/33: BURG_TNDA['TRACTCE'].nunique()
90/1: import pandas as pd
90/2: df1 = pd.read_csv('../data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False')
90/3: df1 = pd.read_csv('../data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
90/4: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
90/5: df1.head()
90/6: df1.shape
91/1: import pandas as pd
91/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
91/3: df1.head()
91/4: df1.shape
91/5: df1.info()
91/6: df1['ACCOUNT'].value_counts()
91/7: df1['ACCOUNT'].value_counts().sort_index()
91/8: df1['USEDTIME'].value_counts()
91/9: df1['REQTIME'].value_counts()
91/10: df1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum()
91/11: df1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort.index()
91/12: df1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index()
91/13: df1['ACCOUNT'].nunique()
91/14: df1['ACCOUNT'].value_counts()
91/15: df1['USEDMEM'].value_counts()
91/16: df1['STATE'].nunique()
91/17: df1['STATE'].value_counts()
91/18: df1['STATE'].unique()
91/19:
df2 = df1.groupby(['ACCOUNT','PARTITION', 'STATE'])['USEDMEM].agg("sum")
print(df2)
91/20: df2 = df1.groupby(['ACCOUNT','PARTITION', 'STATE'])['USEDMEM].sum()
91/21: df1.groupby(['ACCOUNT','PARTITION', 'STATE'])['USEDMEM].sum()
91/22: df1.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['USEDMEM].agg('sum')
91/23: df1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum()
91/24: df1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index()
91/25: df2 = df1.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['USEDMEM].agg('sum')
91/26: df2 = df1.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['USEDMEM].sum()
91/27: df1.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['USEDMEM].sum()
91/28: df1.groupby(['ACCOUNT', 'PARTITION'])['USEDMEM].sum()
91/29:
df1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum()

#df1.groupby(['ACCOUNT', 'PARTITION'])['USEDMEM].sum()
91/30:
df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM'].sum()

#df1.groupby(['ACCOUNT', 'PARTITION'])['USEDMEM].sum()
91/31:
df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM'].agg('sum')

#df1.groupby(['ACCOUNT', 'PARTITION'])['USEDMEM].sum()
91/32:
df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM'].agg('sum')

#df1.groupby(['ACCOUNT', 'PARTITION'])['USEDMEM].sum()
91/33:
df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM'].agg('sum')
df2.head()

#df1.groupby(['ACCOUNT', 'PARTITION'])['USEDMEM].sum()
91/34:
df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM'].agg('sum')
df2.head()
df2.shape
91/35:
df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
df2.head()
df2.shape
91/36:
df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
df2.head()
91/37:
df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
df2.head(50)
91/38:
df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
df2.head(10)
91/39: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
91/40: df_State_comleted.shape
91/41: df_State_comleted['ACCOUNT'].value_counts()
91/42:
timeInterval ='00:35:01'
list = timeInterval.split(':')
hours = list[0]
minutes = list[1]
seconds = list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print("total = ", total)
91/43: time = df1['USEDTIME']
91/44:
time = df1['USEDTIME']
type(time)
91/45:
time = df1['USEDTIME']
print(time)
91/46:
time = df1['USEDTIME'].split(':')
print(time)
91/47:
time = df1['USEDTIME']
timelist = time.split(':')
91/48:
time_list = df1.USEDTIME.tolist()
#timelist = time.split(':')
91/49:
time_list = df1.USEDTIME.tolist()

type(time_list)
#timelist = time.split(':')
91/50:
time_list = df1.USEDTIME.tolist()

type(time_list)
time_list2 = time_list.split(':')
91/51:
time_list = df1.USEDTIME.tolist()

type(time_list)
time_list
91/52:
time_list = df1.USEDTIME.tolist()

type(time_list)
time_list
hours = list[0]
minutes = list[1]
seconds = list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print("total = ", total)
91/53:
time_list = df1.USEDTIME.tolist()

type(time_list)
time_list
hours = time_list[0]
minutes = time_list[1]
seconds = tme_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print("total = ", total)
91/54:
time_list = df1.USEDTIME.tolist()

type(time_list)
time_list
hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print("total = ", total)
91/55: df1['Time'] = pd.to_timedelta(df1['USEDTIME'])
91/56: df1.head()
91/57:
time_list = df1.Time.tolist()

type(time_list)
time_list
#hours = time_list[0]
#minutes = time_list[1]
#seconds = time_list[2]
#total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
91/58:
time_list = df1.Time.tolist()

type(time_list)
time_list.split(':')
#hours = time_list[0]
#minutes = time_list[1]
#seconds = time_list[2]
#total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
91/59: df1['Time]'.max()
91/60: df1['Time]'.max
91/61: df1['Time'].max()
91/62:
time_list = df1.Time.tolist()

type(time_list)
time_list
#hours = time_list[0]
#minutes = time_list[1]
#seconds = time_list[2]
#total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
91/63: time = df1['USEDTIME'].tolist()[0]
91/64:
time = df1['USEDTIME'].tolist()[0]
time
91/65:
time = df1['USEDTIME'].tolist()[0]
time_list = time.split(':')
time_list
91/66:
hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
91/67:
hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
91/68: df1.head(10)
91/69:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split(':')
time_list
91/70: import regex as re
91/71: import re
91/72: re.split('[-:]', time)
91/73:
time = df1['USEDTIME'].tolist()[0]
time_list = time.split('-',':')
time_list

x = []
def time_function(time_list):
    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.appendtotal)
print(total)
91/74:
time = df1['USEDTIME'].tolist()[0]
time_list = time.split('-',':')
time_list

x = []
def time_function(time_list):
    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(total)
91/75:
time = df1['USEDTIME'].tolist()[0]
time_list = time.re.split(['-:'], time)
time_list

x = []
def time_function(time_list):
    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(total)
91/76:
time = df1['USEDTIME'].tolist()[0]
time_list = re.split(['-:'], time)
time_list

x = []
def time_function(time_list):
    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(total)
91/77:
time = df1['USEDTIME'].tolist()[0]
time_list = re.split('[-:]', time)
time_list

x = []
def time_function(time_list):
    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(total)
91/78:
time = df1['USEDTIME'].tolist()[0]
time_list = re.split('[-:]', time)
time_list

x = []
def time_function(time_list):
    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(x)
91/79:
time = df1['USEDTIME'].tolist()[0]
time_list = re.split('[-:]', time)
time_list


def time_function(time_list):
    x = []
    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(x)
91/80:
time = df1['USEDTIME'].tolist()[0, 1]
time_list = re.split('[-:]', time)
time_list

x = []
def time_function(time_list):

    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(x)
91/81:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list

x = []
def time_function(time_list):

    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(x)
91/82:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list

x = []
def time_function(time_list):

    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(total)
91/83:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list

x = []
def time_function(time_list):

    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(total)
91/84:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list

x = []
def time_function(time_list):

    hours = time_list[0]
    minutes = time_list[1]
    seconds = time_list[2]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(time_list, total)
91/85:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list

x = []
def time_function(time_list):

    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    x.append(total)
print(time_list, total)
91/86:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list

x = []
def time_function(time_list):

    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(time_list, total)
91/87:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list

x = []
def time_function(time_list):

    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total)
91/88:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list
print(time_list)
x = []
def time_function(time_list):

    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total)
91/89:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list
print(time_list)
x = []

def time_function(time_list):

    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    sec = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(sec)
91/90:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list
print(time_list)
x = []

def time_function(time_list):

    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    sec = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
print(sec)
91/91:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list
print(time_list)
x = []

def time_function(time_list):
    days = time_list[0]
    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
print(sec)
91/92:
time = df1['USEDTIME'].tolist()[0]
time_list = re.split('[-:]', time)
time_list
print(time_list)
x = []

def time_function(time_list):
    days = time_list[0]
    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
print(sec)
91/93:
time = df1['USEDTIME'].tolist()[0]
time_list = re.split('[-:]', time)
time_list
print(time_list)
x = []

def time_function(time_list):
    days = time_list[0]
    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    total = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
print(sec)
91/94:
time = df1['USEDTIME'].tolist()[0]
time_list = re.split('[-:]', time)
time_list
print(time_list)
x = []

def time_function(time_list):
    days = time_list[0]
    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    total = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
print(total)
91/95:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list
print(time_list)
x = []

def time_function(time_list):
    days = time_list[0]
    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    total = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
print(total)
91/96: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
91/97: df1['Time'].max()
91/98: **Trying to make a function to convert the USEDTIME into a seonds column.**
91/99: **Trying to make a function to convert the USEDTIME into a seonds column**
91/100:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
91/101: df1.head(10)
91/102: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
91/103:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
91/104: df1.head(10)
94/1: import pandas as pd
94/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
94/3: df1.head()
94/4: (mem  *  usedtime)/usedtime
94/5: df1.shape
95/1: import pandas as pd
95/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
95/3: df1.head()
95/4: #(mem  *  usedtime)/usedtime
95/5: df1.shape
95/6: df1.info()
95/7: df1['ACCOUNT'].nunique()
95/8: df1['ACCOUNT'].value_counts()
95/9: df1['USEDTIME'].value_counts()
95/10: df1['USEDMEM'].value_counts()
95/11: df1['REQTIME'].value_counts()
95/12: df1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index()
95/13: df1['STATE'].nunique()
95/14: df1['STATE'].value_counts()
95/15: df1['STATE'].unique()
95/16:
df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
df2.head(10)
95/17: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
95/18: df_State_comleted.shape
95/19: df_State_comleted['ACCOUNT'].value_counts()
95/20:
timeInterval ='00:35:01'
list = timeInterval.split(':')
hours = list[0]
minutes = list[1]
seconds = list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print("total = ", total)
95/21: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
95/22:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
95/23: df1.head(10)
95/24:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-',':')
time_list
95/25: import re
95/26: re.split('[-:]', time)
95/27:
time = df1['USEDTIME'].tolist()[6]
time_list = re.split('[-:]', time)
time_list
print(time_list)
x = []

def time_function(time_list):
    days = time_list[0]
    hours = time_list[1]
    minutes = time_list[2]
    seconds = time_list[3]
    total = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
print(total)
95/28:
time = df1['USEDTIME'].tolist()[6]
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):
    days = time_list1[0]
    hours = time_list1[1]
    minutes = time_list1[2]
    seconds = time_list1[3]
    total = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
print(total)
95/29:
time = df1['USEDTIME'].tolist()[6]
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):
    days = time_list1[0]
    hours = time_list1[1]
    minutes = time_list1[2]
    seconds = time_list1[3]
    total1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
print(total1)
95/30:
time = df1['USEDTIME'].tolist()[6]
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):
    days = time_list1[0]
    hours = time_list1[1]
    minutes = time_list1[2]
    seconds = time_list1[3]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total1)
95/31: time_function(time_list1)
95/32:
time_list1  = re.split('[-:]', time)
time_function(time_list1)
95/33:
time = df1['USEDTIME'].tolist()[6]
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):
    days = time_list1[0]
    hours = time_list1[1]
    minutes = time_list1[2]
    seconds = time_list1[3]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
95/34:
time_list1  = re.split('[-:]', time)
time_function(time_list1)
95/35:
time = df1['USEDTIME'].tolist()[2]
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):
    days = time_list1[0]
    hours = time_list1[1]
    minutes = time_list1[2]
    seconds = time_list1[3]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
95/36:
time_list1  = re.split('[-:]', time)
time_function(time_list1)
95/37:
time = df1['USEDTIME'].tolist()[6]
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):
    days = time_list1[0]
    hours = time_list1[1]
    minutes = time_list1[2]
    seconds = time_list1[3]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
95/38:
time_list1  = re.split('[-:]', time)
time_function(time_list1)
95/39:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
95/40: df_State_comleted['ACCOUNT'].nunique()
96/1:
time = df1['USEDTIME'].tolist()[0] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):         
    #days = time_list1[0]
    hours = time_list1[0]
    minutes = time_list1[1]
    seconds = time_list1[2]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
97/1: import pandas as pd
97/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
97/3: df1.head()
97/4: #(mem  *  usedtime)/usedtime
97/5: df1.shape
97/6: df1.info()
97/7: df1['ACCOUNT'].nunique()
97/8: df1['ACCOUNT'].value_counts()
97/9: df1['USEDTIME'].value_counts()
97/10: df1['USEDMEM'].value_counts()
97/11: df1['REQTIME'].value_counts()
97/12: df1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index()
97/13: df1['STATE'].nunique()
97/14: df1['STATE'].value_counts()
97/15: df1['STATE'].unique()
97/16:
df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
df2.head(10)
97/17: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
97/18: df_State_comleted.shape
97/19: df_State_comleted['ACCOUNT'].value_counts()
97/20: df_State_comleted['ACCOUNT'].nunique()
97/21:
timeInterval ='00:35:01'
list = timeInterval.split(':')
hours = list[0]
minutes = list[1]
seconds = list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print("total = ", total)
97/22: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
97/23:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
97/24: df1.head(10)
97/25:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-',':')
time_list
97/26: import re
97/27: re.split('[-:]', time)
97/28:
time = df1['USEDTIME'].tolist()[0] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):         
    #days = time_list1[0]
    hours = time_list1[0]
    minutes = time_list1[1]
    seconds = time_list1[2]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
97/29:
time = df1['USEDTIME'].tolist()[0] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):         
    days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
97/30:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
97/31:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):         
    days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
97/32:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
97/33:
time = df1['USEDTIME'].tolist()[0] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):         
    #days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
97/34:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
97/35:
time = df1['USEDTIME'].tolist()[0] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):         
    days = time_list1[0]
    hours = time_list1[1]
    minutes = time_list1[2]
    seconds = time_list1[3]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
97/36:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
97/37:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):         
    days = time_list1[0]
    hours = time_list1[1]
    minutes = time_list1[2]
    seconds = time_list1[3]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
97/38:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
97/39:
**Converting the usedtime column in seconds the function needs to be fixed we are going to use the timedelta package
df1.USEDTIME= df1.USEDTIME.str.replace('-','days ',regex=True)
97/40: df1.USEDTIME= df1.USEDTIME.str.replace('-','days ',regex=True)
97/41:
df1.USEDTIME= df1.USEDTIME.str.replace('-','days ',regex=True)
df1.head()
97/42:
df1.USEDTIME= df1.USEDTIME.str.replace('-','days ',regex=True)
df1.head(10)
97/43:
df1.USEDTIME1= df1.USEDTIME.str.replace('-','days ',regex=True)
df1.head(10)
97/44:
df1.USEDTIME = df1.USEDTIME.str.replace('-','days ',regex=True)
df1.head(10)
97/45:
df1['daytime'] = df1.USEDTIME.str.replace('-','days ',regex=True)
df1.head(10)
97/46: df1.head(10)
98/1: import pandas as pd
98/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
98/3: df1.head()
98/4: #(mem  *  usedtime)/usedtime
98/5: df1.shape
98/6: df1.info()
98/7: df1['ACCOUNT'].nunique()
98/8: df1['ACCOUNT'].value_counts()
98/9: df1['USEDTIME'].value_counts()
98/10: df1['USEDMEM'].value_counts()
98/11: df1['REQTIME'].value_counts()
98/12: df1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index()
98/13: df1['STATE'].nunique()
98/14: df1['STATE'].value_counts()
98/15: df1['STATE'].unique()
98/16:
df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
df2.head(10)
98/17: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
98/18: df_State_comleted.shape
98/19: df_State_comleted['ACCOUNT'].value_counts()
98/20: df_State_comleted['ACCOUNT'].nunique()
98/21:
timeInterval ='00:35:01'
list = timeInterval.split(':')
hours = list[0]
minutes = list[1]
seconds = list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print("total = ", total)
98/22: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
98/23:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
98/24: df1.head(10)
98/25:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-',':')
time_list
98/26:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-:')
time_list
98/27: import re
98/28:
import pandas as pd
import re
98/29: re.split('[-:]', time)
98/30:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):         
    days = time_list1[0]
    hours = time_list1[1]
    minutes = time_list1[2]
    seconds = time_list1[3]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
98/31:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
98/32: **Converting the usedtime column in seconds the function needs to be fixed we are going to use the timedelta package
98/33: df1.head(10)
98/34:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)
df1.head(10)
#df['USEDTIMEtd'] = pd.to_timedelta(df['USEDTIME'])
98/35:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)

df1['time_delta'] = pd.to_timedelta(df['time_day'])

df1.head(10)
98/36:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)

df1['time_delta'] = pd.to_timedelta(df1['time_day'])

df1.head(10)
98/37:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):         
    days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
98/38:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
98/39:
time = df1['USEDTIME'].tolist()[0] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1):         
    days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
98/40:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
98/41:
time = df1['USEDTIME'].tolist()[0] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1): 
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
98/42:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
98/43:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1): 
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
98/44:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
98/45:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)

x = []

def time_function(time): 
    time_list1 = re.split('[-:]', time)
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_sec)
98/46: time_function(time)
98/47:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)

x = []

def time_function(time): 
    time_list1 = re.split('[-:]', time)
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    return (total_sec)
98/48: time_function(time)
98/49: df1['USEDTIME']
98/50: df1['USEDTIME'].apply(time_function)
98/51:
df1['Usedtime_sec'] = df1['USEDTIME'].apply(time_function)
df1.head(10)
98/52: user_list = df1['ACCOUNT'].value_counts()
98/53:
user_list = df1['ACCOUNT'].value_counts()
print(user_list)
98/54: df1['ACCOUNT'].unique
98/55: df1['ACCOUNT'].value_counts()
98/56: df1['USEDMEM'].value_counts().tail(20)
98/57: df1['USEDMEM'].value_counts().head(20)
98/58: df1['JOBID'].nunique()
98/59: df1['REQTIME'].value_counts().tail(50)
98/60: df1['REQTIME'].value_counts()
98/61: 58747 - 51083
98/62: df1['GPU'].value_counts()
98/63: df1['GPUS'].value_counts()
98/64: df1['GPUS'].value_counts().plot
98/65: df1['GPUS'].value_counts().plot()
98/66: df1['GPUS'].value_counts()
98/67: df1['ACCOUNT'].value_counts().plot()
98/68: df1['ACCOUNT'].value_counts().box_plot()
98/69: df1['ACCOUNT'].value_counts()
98/70: df1['GPUS'].value_counts().tail()
98/71: df1['GPUS'].value_counts().tail().sort_index()
98/72: df1['GPUS'].value_counts().tail(20).sort_index()
98/73: df1['GPUS'].value_counts().tail(20).sort_index().plot()
98/74: df1['CPUS'].value_counts().tail(20).sort_index().plot()
98/75: df1['CPUS'].value_counts().tail(20).sort_index()
98/76: df1['CPUS'].value_counts().tail(20).sort_index().plot()
98/77: df1['PARTITION'].value_counts().tail(20).sort_index()
98/78: df1['PARTITION'].value_counts().tail(20).sort_index().plot()
98/79:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot()
98/80:
df1['USED_SEC'] = df1['USEDTIME'].apply(time_function)
df1['REQ_SEC'] = df1['REQTIME'].apply(time_function)
df1.head(10)
98/81:
mem = df1['USEDMEM'].tolist()[6]
list = mem.split('a-z')
list
98/82:
mem = df1['USEDMEM'].tolist()[6]
list = mem.split('[a-z]')
list
98/83:
mem = df1['USEDMEM'].tolist()[0]
list = mem.split('[a-z]')
list
98/84:
mem = df1['USEDMEM'].tolist()[0]
list = mem.split('a-z')
list
98/85:
mem = df1['USEDMEM'].tolist()[0]
list = mem.split('M')
list
98/86:
mem = df1['USEDMEM'].tolist()[0]
list = mem.str.replace('M','',regex=True)
list
98/87:
mem = df1['USEDMEM'].tolist()[0]
list = mem.split('M')
list
98/88:
df1['MEM] = df1['USEDMEM'].str.replace('M','',regex=True)
#list = mem.split('M')
#list
98/89:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True)
#list = mem.split('M')
#list
98/90:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True)
#list = mem.split('M')
#list

df1.head()
98/91:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list

df1.head()
98/92:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list

df1.info()
98/93:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df.astype({"MEM":'float'})
df1.info()
98/94:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df1.astype({"MEM":'float'})
df1.info()
98/95:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df1 = df1.astype({"MEM":'float'})
df1.info()
98/96: df1['GPUS'].value_counts().tail(20).sort_index()
98/97: df1['GPUS'].value_counts().tail(20).sort_index().plot()
98/98: df1.groupby(['ACCOUNT','PARTITION'])['MEM'].sum().sort_index()
98/99: df1.groupby(['PARTITION'])['MEM'].sum().sort_index()
98/100: df1.groupby(['PARTITION'])['MEM'].sum()
98/101: df1.groupby(['PARTITION', 'GPUS'])['MEM'].sum()
98/102:
df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
df3.head(10)
98/103:
df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
df3.head(50)
98/104:
#df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
#df3.head(50)
98/105: df1['GPUS'].value_counts().tail(20).sort_index()
98/106:
#f1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index() 
# this is not going to add the numbers in USEDMEM because it is an object, so we see the
98/107: df1['NODES'].value_counts()
98/108: df1.groupby(['PARTITION'])['MEM'].agg('sum')
98/109: df1.groupby(['PARTITION'])['GPU'].agg('sum')
98/110: df1.groupby(['PARTITION'])['GPUS'].agg('sum')
98/111:
x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
x
98/112: x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
98/113: y = df1.groupby(['PARTITION'])['GPUS'].agg('sum')
98/114: x/y
98/115:
#df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
#df2.head(10)
98/116: df1_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
98/117: df4_STATECOMPLETED = df1.loc[df1['STATE'] == 'COMPLETED']
98/118:
x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
x
98/119:
y = df1.groupby(['PARTITION'])['GPUS'].agg('sum')
y
98/120:
z = df1.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z
98/121: df4_SC = df1.loc[df1['STATE'] == 'COMPLETED']
98/122: df4_SC.shape
98/123: df4_SC['weighted_memorry'] = (df4_SC['MEM']/df4_SC['GPUS']))*df4_SC['USED_SEC']
98/124: df4_SC['weighted_memorry'] = (df4_SC['MEM']/df4_SC['GPUS'])*df4_SC['USED_SEC']
98/125: df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
98/126:
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
98/127:
#weighted_memorry/total used time in each partition
w/z
98/128:
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
98/129:
#weighted_memorry/total used time in each partition
w/z4
98/130: df4_SC['PARTITION'].value_counts()
98/131:
df4_SC.shape # (283333, 18)
df4_SC.describe()
98/132:
df4_SC.shape # (283333, 18)
df4_SC['ACCOUNT'].nunique()
98/133:
df4_SC.shape # (283333, 18)
df4_SC['ACCOUNT'].nunique() #27 
df4_SC['ACCOUNT'].value_counts()
99/1:
import pandas as pd
import re
99/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
99/3: df1.head()
99/4: df1.shape
99/5: df1.info()
99/6: df1['ACCOUNT'].nunique()
99/7: df1['ACCOUNT'].value_counts()
99/8: df1['JOBID'].nunique()
99/9: df1['USEDTIME'].value_counts()
99/10: df1['USEDMEM'].value_counts()
99/11: df1['GPUS'].value_counts().tail(20).sort_index()
99/12: df1['CPUS'].value_counts().tail(20).sort_index().plot()
99/13: df1['NODES'].value_counts()
99/14:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot()
99/15: df1['REQTIME'].value_counts()
99/16:
#f1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index() 
# this is not going to add the numbers in USEDMEM because it is an object, so we see the
99/17: df1['STATE'].nunique()
99/18: df1['STATE'].value_counts()
99/19: df1['STATE'].unique()
99/20:
#df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
#df2.head(10)
99/21: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
99/22: df_State_comleted.shape
99/23: df_State_comleted['ACCOUNT'].value_counts()
99/24:
timeInterval ='00:35:01'
list = timeInterval.split(':')
hours = list[0]
minutes = list[1]
seconds = list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print("total = ", total)
99/25: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
99/26:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
99/27: df1.head(10)
99/28:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-:')
time_list
99/29: re.split('[-:]', time)
99/30:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1): 
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
99/31:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
99/32:
def time_function(time): 
    time_list1 = re.split('[-:]', time)
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    return (total_sec)
99/33: time_function(time)
99/34:
df1['USED_SEC'] = df1['USEDTIME'].apply(time_function)
df1['REQ_SEC'] = df1['REQTIME'].apply(time_function)
df1.head(10)
99/35:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)

df1['time_delta'] = pd.to_timedelta(df1['time_day'])

df1.head(10)
99/36:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df1 = df1.astype({"MEM":'float'})
df1.info()
99/37: df1.groupby(['PARTITION', 'GPUS'])['MEM'].sum()
99/38:
#df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
#df3.head(50)
99/39:
x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
x
99/40:
y = df1.groupby(['PARTITION'])['GPUS'].agg('sum')
y
99/41:
z = df1.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z
99/42: x/y #average memorry usage per GPU in each partition.
99/43: df4_SC = df1.loc[df1['STATE'] == 'COMPLETED']
99/44:
df4_SC.shape # (283333, 18)
df4_SC['ACCOUNT'].nunique() #27 
df4_SC['ACCOUNT'].value_counts()
99/45: df4_SC['weighted_memorry'] = (df4_SC['MEM']/df4_SC['GPUS'])*df4_SC['USED_SEC']
99/46:
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
99/47:
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
99/48:
#weighted_memorry/total used time in each partition
w/z4

#The turning is using lot of memorry and shor
99/49: df4_SC['PARTITION'].value_counts()
99/50: df4_SC['PARTITION'].value_counts()
99/51:
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar')
99/52:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index()..plot(kind='bar')
99/53:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot(kind='bar')
99/54:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot()
99/55: df4_SC.head(50)
99/56: df1['EXITCODE'].value_counts()
99/57: df1['EXITCODE'].value_counts().plot(kind='bar')
99/58: df4_SC['EXITCODE'].value_counts()
99/59: df1['EXITCODE'].value_counts()
99/60:
par = df1['EXITCODE'].value_counts()
par
99/61: df1['EXITCODE'].value_counts()
99/62:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
99/63:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
99/64:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts().tail(20)
99/65:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].nunique()
99/66:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
len(df1['TIME_ESTIMATE'].value_counts())
99/67:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
99/68:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts().plot(kind='bar')
99/69:
#How far are the time estimates.
df1.head(20)
99/70:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
99/71:
#How far are the time estimates.
df1.head(20)
99/72:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
99/73:
#How far are the time estimates.
df1.head(20)
99/74: df1['TIME_ESTIMATE'].lt(0).sum()
99/75: lessTime = df1['TIME_ESTIMATE'].lt(0)
99/76:
lessTime = df1['TIME_ESTIMATE'].lt(0)
lessTime
99/77: df1['TIME_ESTIMATE'].lt(0).sum()
99/78:
df1['TIME_ESTIMATE'].lt(0).sum() #4247 rows have negative value where used time is more than requested time.

df1['TIME_ESTIMATE'](df1.loc[df1['STATE'] == 'COMPLETED']).lt(0).sum()
99/79:
df1['TIME_ESTIMATE'].lt(0).sum() #4247 rows have negative value where used time is more than requested time.

df1.loc[df1['STATE'] == 'COMPLETED'])['TIME_ESTIMATE'].lt(0).sum()
99/80:
df1['TIME_ESTIMATE'].lt(0).sum() #4247 rows have negative value where used time is more than requested time.

df1.loc[df1['STATE'] == 'COMPLETED']['TIME_ESTIMATE'].lt(0).sum()
101/1:
import pandas as pd
import re
101/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
101/3: df1.head()
101/4: df1.shape
101/5: df1.info()
101/6: df1['ACCOUNT'].nunique()
101/7: df1['ACCOUNT'].value_counts()
101/8: df1['JOBID'].nunique()
101/9: df1['USEDTIME'].value_counts()
101/10: df1['USEDMEM'].value_counts()
101/11: df1['GPUS'].value_counts().tail(20).sort_index()
101/12: df1['CPUS'].value_counts().tail(20).sort_index().plot()
101/13: df1['NODES'].value_counts()
101/14: df1['EXITCODE'].value_counts()
101/15:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot()
101/16: df1['REQTIME'].value_counts()
101/17:
#f1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index() 
# this is not going to add the numbers in USEDMEM because it is an object, so we see the
101/18: df1['STATE'].nunique()
101/19: df1['STATE'].value_counts()
101/20: df1['STATE'].unique()
101/21:
#df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
#df2.head(10)
101/22: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
101/23: df_State_comleted.shape
101/24: df_State_comleted['ACCOUNT'].value_counts()
101/25:
timeInterval ='00:35:01'
list = timeInterval.split(':')
hours = list[0]
minutes = list[1]
seconds = list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print("total = ", total)
101/26: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
101/27:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
101/28: df1.head(10)
101/29:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-:')
time_list
101/30: re.split('[-:]', time)
101/31:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1): 
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
101/32:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
101/33:
def time_function(time): 
    time_list1 = re.split('[-:]', time)
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    return (total_sec)
101/34: time_function(time)
101/35:
df1['USED_SEC'] = df1['USEDTIME'].apply(time_function)
df1['REQ_SEC'] = df1['REQTIME'].apply(time_function)
df1.head(10)
101/36:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)

df1['time_delta'] = pd.to_timedelta(df1['time_day'])

df1.head(10)
101/37:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df1 = df1.astype({"MEM":'float'})
df1.info()
101/38: df1.groupby(['PARTITION', 'GPUS'])['MEM'].sum()
101/39:
#df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
#df3.head(50)
101/40:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
101/41:
#How far are the time estimates.
df1.head(20)
101/42:
df1['TIME_ESTIMATE'].lt(0).sum() #4247 rows have negative value where used time is more than requested time.

df1.loc[df1['STATE'] == 'COMPLETED']['TIME_ESTIMATE'].lt(0).sum()  #there are 2873 jobID that got completed with more than requested time?
101/43:
x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
x
101/44:
y = df1.groupby(['PARTITION'])['GPUS'].agg('sum')
y
101/45:
z = df1.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z
101/46: x/y #average memorry usage per GPU in each partition.
101/47: df4_SC = df1.loc[df1['STATE'] == 'COMPLETED']
101/48:
df4_SC.shape # (283333, 18)
df4_SC['ACCOUNT'].nunique() #27 
df4_SC['ACCOUNT'].value_counts()
101/49: df4_SC['weighted_memorry'] = (df4_SC['MEM']/df4_SC['GPUS'])*df4_SC['USED_SEC']
101/50:
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
101/51:
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
101/52:
#weighted_memorry/total used time in each partition
w/z4

#The turning is using lot of memorry per unittime. 
#The maxwell have more jobs run in less time like the marathon runner while the turing are the marathon runner. May be the nature of job needs larger memmory and more time.
101/53:
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar')
101/54: df4_SC.head(50)
101/55: df4_SC['EXITCODE'].value_counts()
101/56: (df4_SC['TIME_ESTIMATE'] < 0).sum()
101/57: (df4_SC['TIME_ESTIMATE'] < 0).sum().plot()
101/58: (df4_SC['TIME_ESTIMATE'] < 0).sum()
101/59:
#Per partition what is the totalweighted meorry
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
101/60: df4_SC.head()
101/61:
#Per partition what is the total time used
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar')
101/62:
#Per partition what is the total time used
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar', title = 'Total Used seconds per partition');
101/63:
#Per partition what is the total time used
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
#z4.plot(kind='bar', title = 'Total Used seconds per partition');
101/64:
#Per partition what is the total time used
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar', title = 'Total Used seconds per partition');
101/65:
#Per partition what is the total time used
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar', x_title = 'Seonds * 10^9', title = 'Total Used seconds per partition');
101/66:
#Per partition what is the total time used
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar',  title = 'Total Used seconds per partition');
101/67:
#Per partition what is the total time used
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar',  xlabel = 'Seconds * 10^9', title = 'Total Used seconds per partition');
101/68:
#Per partition what is the total time used
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar',  ylabel = 'Seconds * 10^9', title = 'Total Used seconds per partition');
101/69:
#Per partition what is the total weighted meorry
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
w.plot(kind='bar',  ylabel = 'Seconds * 10^9', title = 'Total Used seconds per partition');
101/70:
#Per partition what is the total weighted meorry
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
w.plot(kind='bar',  ylabel = 'Weighted_memorry * 10^12', title = 'Total Weighted_memorry per partition');
101/71:
#What is the total distribution of the total number of partitions in completed jobs
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');)
101/72:
#What is the total distribution of the total number of partitions in completed jobs
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
101/73: df1.groupby(['Account', 'PARTITION'])['USED_SEC'].agg('sum')
101/74: df1.groupby(['ACCOUNT', 'PARTITION'])['USED_SEC'].agg('sum')
101/75: df1.groupby(['ACCOUNT', 'PARTITION'])['USED_SEC'].agg('sum').plot(kind = 'bar')
101/76: df1.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['USED_SEC'].agg('sum')
101/77: df1.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['USED_SEC'].agg('sum').head(50)
101/78: df4_SC.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['USED_SEC'].agg('sum').head(50)
101/79: df4.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['GPUS'].agg('sum').head(50)
101/80: df4_SC.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['GPUS'].agg('sum').head(50)
101/81: df4_SC.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['GPUS'].agg('sum').plot()
101/82: df4_SC.groupby(['ACCOUNT', 'PARTITION', 'STATE'])['GPUS'].agg('sum').plot(kind='bar')
101/83: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').plot(kind='bar')
101/84: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum')
101/85: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').head()
101/86: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_value()
101/87: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').head()
101/88: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
101/89: df4_underestimate = df4_SC[df4['TIME_ESTIMATE'] <0]
101/90: df4_underestimate = df4_SC[df4_SC['TIME_ESTIMATE'] <0]
101/91:
df4_underestimate = df4_SC[df4_SC['TIME_ESTIMATE'] <0]
df4_underestimate.head(10)
101/92:
df4_underestimate = df4_SC[df4_SC['TIME_ESTIMATE'] <0]
df4_underestimate.head(10)
df4_underestimate.shape
101/93: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum')
101/94: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum').sort_values()
101/95: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum').sort_values().plot(kind ='bar')
101/96: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum').sort_values()
101/97: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE', 'MEM'].agg('sum').sort_values()
101/98: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE', 'MEM'].agg('sum')
101/99: df4_underestimate.groupby(['ACCOUNT', 'PARTITION', 'USER'])['TIME_ESTIMATE', 'MEM'].agg('sum')
101/100: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE', 'MEM'].agg('sum')
101/101: df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts()
101/102: df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
101/103:
df4_underestimate = df4_SC[df4_SC['TIME_ESTIMATE'] <0]
df4_underestimate.head(10)
#df4_underestimate.shape #(2873, 19)
101/104: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum').sort_values()
101/105: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('mean').sort_values()
101/106: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum')
101/107: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
101/108: df4_underestimate.groupby(['ACCOUNT', 'PARTITION', 'USER'])['TIME_ESTIMATE', 'MEM'].agg('sum')
101/109:
import plotly.express as px
  
#df = px.data.iris()
  
fig = px.scatter(df4_SC, x="GPUS", y="MEM",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
101/110:
import plotly.express as px
  
#df = px.data.iris()
  
fig = px.scatter(df4_SC, x="MEM", y="GPUS",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
101/111:
fig = px.scatter(df1, x="MEM", y="USED_SEC",
                 color="ACCOUNT",
                 size='GPUS', 
                 hover_data=['STATE'])
  
fig.show()
101/112:
fig = px.scatter(df1, x="REQ_SEC", y="USED_SEC",
                 color="ACCOUNT",
                 size='GPUS', 
                 hover_data=['STATE'])
  
fig.show()
101/113:
fig = px.scatter(df1, x="USED_SEC", y="MEM",
                 color="PARTITION",
                 size='GPUS', 
                 hover_data=['STATE'])
  
fig.show()
101/114: df_glasshouse = df1[df1['ACCOUNT'] == 'glasshouse']
101/115:
df_glasshouse = df1[df1['ACCOUNT'] == 'glasshouse']
df_glasshouse.columns
101/116:
fig = px.scatter(df_glasshouse, x="USED_SEC", y="MEM",
                 color="PARTITION",
                 size='GPUS', 
                 hover_data=['STATE'])
  
fig.show()
101/117:
fig = px.scatter(df_glasshouse, x="MEM", y="GPUS",
                 color="USER",
                 size='USED_SEC', 
                 hover_data=['STATE'])
  
fig.show()
101/118: df1['USER'].nunique()
101/119: df1.groupby(['ACCOUNT'])['USER'].nunique()
101/120: df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values(ascending=TRUE)
101/121: df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values()
101/122: ((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100
101/123: df4_underestimate.groupby(['PARTITION'])['ACCOUNT'].value_counts()
101/124: df1[df1['TIME_ESTIMATE']  == 0].count()
101/125: df1.groupby(df1[df1['TIME_ESTIMATE'] == 0])['STATE'].value_counts()
101/126:

#df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
df1[df1['TIME_ESTIMATE'] == 0]).groupby(['PARTITION', 'USER'])['STATE'].value_counts()
101/127:

#df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
df1[df1['TIME_ESTIMATE'] == 0].groupby(['PARTITION', 'USER'])['STATE'].value_counts()
101/128:

#df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
df1[df1['TIME_ESTIMATE'] == 0].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts()
101/129: df1[df1['STATE'] == 'FAILED'].groupby(['PARTITION', 'ACCOUNT'])['USER'].value_counts()
101/130: df1[df1['STATE'] == 'FAILED']
101/131: df1[df1['STATE'] == 'FAILED'].count()
101/132:
#weighted_memorry/total used time in each partition
aa= w/z4
aa

#The turning is using lot of memorry per unittime. 
#The maxwell have more jobs run in less time like the marathon runner while the turing are the marathon runner. May be the nature of job needs larger memmory and more time.
101/133:
#weighted_memorry/total used time in each partition
aa= w/z4
aa
aa.plot(kind='bar',  ylabel = 'ratio', title = 'Total weighted-memmory/total_used sec per partition');
#The turning is using lot of memorry per unittime. 
#The maxwell have more jobs run in less time like the marathon runner while the turing are the marathon runner. May be the nature of job needs larger memmory and more time.
101/134:
#weighted_memorry/total used time in each partition
aa= w/z4
aa
aa.plot(kind='bar',  ylabel = 'ratio MEM_used/USED_SEC', title = 'Total weighted-memmory/total_used sec per partition');
#The turning is using lot of memorry per unittime. 
#The maxwell have more jobs run in less time like the marathon runner while the turing are the marathon runner. May be the nature of job needs larger memmory and more time.
101/135: df1[df1['GPUS'] > 4].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts()
101/136: df1[df1['GPUS'] > 4].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts().plot()
101/137: df1[df1['GPUS'] > 4].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts().head(50)
101/138: df1[df1['GPUS'] > 4].groupby(['PARTITION']).value_counts().head(50)
101/139: df1[df1['GPUS'] > 4].groupby(['PARTITION']).value_counts()
101/140: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()
101/141: df1[df1['GPUS'] > 4].count()
101/142: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()
102/1:
import pandas as pd
import re
102/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
102/3: df1.head()
102/4: df1.shape
102/5: df1.info()
102/6: df1['USER'].nunique()
102/7: df1['ACCOUNT'].nunique()
102/8: df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values()
102/9: ((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100
102/10: df1['ACCOUNT'].value_counts()
102/11: df1['JOBID'].nunique()
102/12: df1['USEDTIME'].value_counts()
102/13: df1['USEDMEM'].value_counts()
102/14: df1['GPUS'].value_counts().tail(20).sort_index()
102/15: df1['CPUS'].value_counts().tail(20).sort_index().plot()
102/16: df1['NODES'].value_counts()
102/17: df1['EXITCODE'].value_counts()
102/18:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot()
102/19: df1['REQTIME'].value_counts()
102/20:
#f1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index() 
# this is not going to add the numbers in USEDMEM because it is an object, so we see the
102/21: df1['STATE'].nunique()
102/22: df1['STATE'].value_counts()
102/23: df1['STATE'].unique()
102/24:
#df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
#df2.head(10)
102/25: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
102/26: df_State_comleted.shape
102/27: df_State_comleted['ACCOUNT'].value_counts()
102/28: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
102/29:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
102/30: df1.head(10)
102/31:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-:')
time_list
102/32: re.split('[-:]', time)
102/33:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1): 
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
102/34:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
102/35:
def time_function(time): 
    time_list1 = re.split('[-:]', time)
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    return (total_sec)
102/36: time_function(time)
102/37:
df1['USED_SEC'] = df1['USEDTIME'].apply(time_function)
df1['REQ_SEC'] = df1['REQTIME'].apply(time_function)
df1.head(10)
102/38:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)

df1['time_delta'] = pd.to_timedelta(df1['time_day'])

df1.head(10)
102/39:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df1 = df1.astype({"MEM":'float'})
df1.info()
102/40: df1.groupby(['PARTITION', 'GPUS'])['MEM'].sum()
102/41:
#df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
#df3.head(50)
102/42:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
102/43:
#How far are the time estimates.
df1.head(20)
102/44:
df1['TIME_ESTIMATE'].lt(0).sum() #4247 rows have negative value where used time is more than requested time.

df1.loc[df1['STATE'] == 'COMPLETED']['TIME_ESTIMATE'].lt(0).sum()  #there are 2873 jobID that got completed with more than requested time?
102/45:
x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
x
102/46:
y = df1.groupby(['PARTITION'])['GPUS'].agg('sum')
y
102/47:
z = df1.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z
102/48: x/y #average memorry usage per GPU in each partition.
102/49: df4_SC = df1.loc[df1['STATE'] == 'COMPLETED']
102/50:
df4_SC.shape # (283333, 18)
df4_SC['ACCOUNT'].nunique() #27 
df4_SC['ACCOUNT'].value_counts()
102/51: df4_SC['weighted_memorry'] = (df4_SC['MEM']/df4_SC['GPUS'])*df4_SC['USED_SEC']
102/52: df4_SC.head()
102/53: df4_SC['EXITCODE'].value_counts()
102/54:
#THere are lot of jobs that are UNDERESTIMATING the required time
(df4_SC['TIME_ESTIMATE'] < 0).sum()
102/55:
#Per partition what is the total weighted meorry
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
w.plot(kind='bar',  ylabel = 'Weighted_memorry * 10^12', title = 'Total Weighted_memorry per partition');
102/56:
#Per partition what is the total time used
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar',  ylabel = 'Seconds * 10^9', title = 'Total Used seconds per partition');
102/57:
#weighted_memorry/total used time in each partition
aa= w/z4
aa
aa.plot(kind='bar',  ylabel = 'MEM_used/USED_SEC', title = 'Total weighted-memmory/total_used sec per partition');
#The turning is using lot of memorry per unittime. 
#The maxwell have more jobs run in less time like the marathon runner while the turing are the marathon runner. May be the nature of job needs larger memmory and more time.
102/58:
#What is the total distribution of the total number of partitions in completed jobs
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
102/59: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
102/60: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
102/61:
df4_underestimate = df4_SC[df4_SC['TIME_ESTIMATE'] <0]
df4_underestimate.head(10)
#df4_underestimate.shape #(2873, 19)
102/62: df4_underestimate.groupby(['PARTITION'])['ACCOUNT'].value_counts()
102/63: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum').sort_values()
102/64: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('mean').sort_values()
102/65: df4_underestimate.groupby(['ACCOUNT', 'PARTITION', 'USER'])['TIME_ESTIMATE', 'MEM'].agg('sum')
102/66: df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
102/67:
import plotly.express as px
  
#df = px.data.iris()
  
fig = px.scatter(df4_SC, x="MEM", y="GPUS",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
102/68:
fig = px.scatter(df1, x="USED_SEC", y="MEM",
                 color="PARTITION",
                 size='GPUS', 
                 hover_data=['STATE'])
  
fig.show()
102/69:
df_glasshouse = df1[df1['ACCOUNT'] == 'glasshouse']
df_glasshouse.columns
102/70:
fig = px.scatter(df_glasshouse, x="MEM", y="GPUS",
                 color="USER",
                 size='USED_SEC', 
                 hover_data=['STATE'])
  
fig.show()
102/71: df1[df1['TIME_ESTIMATE']  == 0].count()
102/72:

#df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
df1[df1['TIME_ESTIMATE'] == 0].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts()
102/73: df1[df1['STATE'] == 'FAILED'].count()
102/74: df1[df1['GPUS'] > 4].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts().head(50)
102/75: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count())/df1[df1['GPUS'] > 4]
108/1:
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
108/2: cars = pd.read_csv('../data/auto_mpg_cleaned.csv')
108/3: cars.head()
108/4: cars['origin'].value_counts()
108/5: cars['origin'].value_counts().plot(kind = 'bar')
108/6: cars['origin'].value_counts().plot(kind = 'bar');
108/7:
plt.figure(figsize = (8,5))
cars['origin'].value_counts().plot(kind = 'bar');
108/8:
plt.figure(figsize = (8,5))
cars['origin'].value_counts().plot(kind = 'bar', color = 'slategray');
108/9:
plt.figure(figsize = (8,5))
cars['origin'].value_counts().plot(kind = 'bar', color = 'slategray')
plt.xticks(rotation = 45);
108/10:
plt.figure(figsize = (8,5))
cars['origin'].value_counts().plot(kind = 'bar', color = 'slategray')
plt.xticks(rotation = 0);
108/11:
plt.figure(figsize = (8,5))
cars['origin'].value_counts().plot(kind = 'bar', color = 'slategray')
plt.xticks(rotation = 0, fontsize = 14)
plt.yticks(fontsize = 14);
108/12:
plt.figure(figsize = (8,5))
cars['origin'].value_counts().plot(kind = 'bar', color = 'slategray')
plt.xticks(rotation = 0, fontsize = 14)
plt.yticks(fontsize = 14)
plt.ylabel('Count', fontsize = 14)
plt.title('Count of Cars by Origin', fontsize = 16, fontweight = 'bold');
108/13:
plt.figure(figsize = (8,5))
cars['origin'].value_counts().plot(kind = 'bar', color = 'slategray')
plt.xticks(rotation = 0, fontsize = 14, labels = ['US', 'Japan', 'Europe'], ticks = [0, 1, 2])
plt.yticks(fontsize = 14)
plt.ylabel('Count', fontsize = 14)
plt.title('Count of Cars by Origin', fontsize = 16, fontweight = 'bold');
108/14: cars.plot(kind = 'scatter', x = 'horsepower', y = 'mpg');
108/15: sns.scatterplot(data = cars, x = 'horsepower', y = 'mpg', hue = 'origin');
108/16:
sns.scatterplot(data = cars, x = 'horsepower', y = 'mpg', 
                hue = 'origin', palette = ['red', 'purple', 'skyblue']);
108/17:
sns.scatterplot(data = cars, x = 'horsepower', y = 'mpg', 
                hue = 'origin', palette = ['red', 'purple', 'skyblue'],
                alpha = 0.7, edgecolor = 'black');
108/18:
fontsize = 16

plt.figure(figsize = (12,7))
sns.scatterplot(data = cars, x = 'horsepower', y = 'mpg', 
                hue = 'origin', palette = ['red', 'purple', 'skyblue'],
                alpha = 0.7, edgecolor = 'black')
plt.xticks(fontsize = fontsize)
plt.xlabel('horsepower', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('mpg', fontsize = fontsize)
plt.title('MPG vs. Horsepower', fontsize = fontsize, fontweight = 'bold');
108/19:
fontsize = 16

plt.figure(figsize = (12,7))
sns.scatterplot(data = cars, x = 'horsepower', y = 'mpg', s = 50,
                hue = 'origin', palette = ['red', 'purple', 'skyblue'],
                alpha = 0.7, edgecolor = 'black')
plt.xticks(fontsize = fontsize)
plt.xlabel('horsepower', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('mpg', fontsize = fontsize)
plt.title('MPG vs. Horsepower', fontsize = fontsize, fontweight = 'bold');
108/20:
fontsize = 16

plt.figure(figsize = (12,7))
sns.scatterplot(data = cars, x = 'horsepower', y = 'mpg', s = 50,
                hue = 'origin', palette = ['red', 'purple', 'skyblue'],
                alpha = 0.7, edgecolor = 'black')
plt.xticks(fontsize = fontsize)
plt.xlabel('horsepower', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('mpg', fontsize = fontsize)
plt.title('MPG vs. Horsepower', fontsize = fontsize, fontweight = 'bold')
plt.legend(fontsize = fontsize - 2);
108/21: cars[(cars.horsepower > 125) & (cars.mpg > 30)]
108/22:
fontsize = 16

plt.figure(figsize = (12,7))
sns.scatterplot(data = cars, x = 'horsepower', y = 'mpg', s = 50,
                hue = 'origin', palette = ['red', 'purple', 'skyblue'],
                alpha = 0.7, edgecolor = 'black')
plt.xticks(fontsize = fontsize)
plt.xlabel('horsepower', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('mpg', fontsize = fontsize)
plt.title('MPG vs. Horsepower', fontsize = fontsize, fontweight = 'bold')
plt.legend(fontsize = fontsize - 2)

plt.annotate(text = 'Datsun 280ZX', xy = (132.5, 33), fontsize = 12,
             xytext = (150, 35), arrowprops=dict(facecolor='black', shrink=0.1));
108/23:
fontsize = 16

plt.figure(figsize = (12,7))
sns.scatterplot(data = cars, x = 'horsepower', y = 'mpg', s = 50,
                hue = 'origin', palette = ['red', 'purple', 'skyblue'],
                alpha = 0.7, edgecolor = 'black')
plt.xticks(fontsize = fontsize)
plt.xlabel('horsepower', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('mpg', fontsize = fontsize)
plt.title('MPG vs. Horsepower', fontsize = fontsize, fontweight = 'bold')
plt.legend(fontsize = fontsize - 2)

plt.annotate(text = 'Datsun 280ZX', xy = (132.5, 33), fontsize = 12,
             xytext = (150, 35), arrowprops=dict(facecolor='black', shrink=0.1))

plt.tight_layout()
plt.savefig('../assets/mpg_vs_horsepower.png', dpi = 150);
108/24: sns.stripplot(data = cars, x = 'model_year', y = 'mpg');
108/25:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = cars, x = 'model_year', y = 'mpg')
plt.xticks(fontsize = fontsize)
plt.xlabel('Model Year', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('mpg', fontsize = fontsize)
plt.title('MPG by Model Year', fontsize = fontsize, fontweight = 'bold');
108/26:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = cars, x = 'model_year', y = 'mpg')
plt.xticks(fontsize = fontsize)
plt.xlabel('Model Year', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('mpg', fontsize = fontsize)
plt.title('MPG by Model Year', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 27.5, xmin = xmin, xmax = xmax, linestyle = '--');
108/27:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = cars, x = 'model_year', y = 'mpg')
plt.xticks(fontsize = fontsize)
plt.xlabel('Model Year', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('mpg', fontsize = fontsize)
plt.title('MPG by Model Year', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 27.5, xmin = xmin, xmax = xmax, linestyle = '--', label = 'EPCA Requirement')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
110/1:
import plotly.express as px
  
#df = px.data.iris()
  
fig = px.scatter(df4_SC, x="MEM", y="GPUS",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
111/1:
import pandas as pd
import re
111/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
111/3: df1.head()
111/4: df1.shape
111/5: df1.info()
111/6: df1['USER'].nunique()
111/7: df1['ACCOUNT'].nunique()
111/8: df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values()
111/9: ((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100
111/10: df1['ACCOUNT'].value_counts()
111/11: df1['JOBID'].nunique()
111/12: df1['USEDTIME'].value_counts()
111/13: df1['USEDMEM'].value_counts()
111/14: df1['GPUS'].value_counts().tail(20).sort_index()
111/15: df1['CPUS'].value_counts().tail(20).sort_index().plot()
111/16: df1['NODES'].value_counts()
111/17: df1['EXITCODE'].value_counts()
111/18:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot()
111/19: df1['REQTIME'].value_counts()
111/20:
#f1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index() 
# this is not going to add the numbers in USEDMEM because it is an object, so we see the
111/21: df1['STATE'].nunique()
111/22: df1['STATE'].value_counts()
111/23: df1['STATE'].unique()
111/24:
#df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
#df2.head(10)
111/25: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
111/26: df_State_comleted.shape
111/27: df_State_comleted['ACCOUNT'].value_counts()
111/28: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
111/29:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
111/30: df1.head(10)
111/31:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-:')
time_list
111/32: re.split('[-:]', time)
111/33:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1): 
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
111/34:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
111/35:
def time_function(time): 
    time_list1 = re.split('[-:]', time)
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    return (total_sec)
111/36: time_function(time)
111/37:
df1['USED_SEC'] = df1['USEDTIME'].apply(time_function)
df1['REQ_SEC'] = df1['REQTIME'].apply(time_function)
df1.head(10)
111/38:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)

df1['time_delta'] = pd.to_timedelta(df1['time_day'])

df1.head(10)
111/39:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df1 = df1.astype({"MEM":'float'})
df1.info()
111/40: df1.groupby(['PARTITION', 'GPUS'])['MEM'].sum()
111/41:
#df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
#df3.head(50)
111/42:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
111/43:
#How far are the time estimates.
df1.head(20)
111/44:
df1['TIME_ESTIMATE'].lt(0).sum() #4247 rows have negative value where used time is more than requested time.

df1.loc[df1['STATE'] == 'COMPLETED']['TIME_ESTIMATE'].lt(0).sum()  #there are 2873 jobID that got completed with more than requested time?
111/45:
x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
x
111/46:
y = df1.groupby(['PARTITION'])['GPUS'].agg('sum')
y
111/47:
z = df1.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z
111/48: x/y #average memorry usage per GPU in each partition.
111/49: df4_SC = df1.loc[df1['STATE'] == 'COMPLETED']
111/50:
df4_SC.shape # (283333, 18)
df4_SC['ACCOUNT'].nunique() #27 
df4_SC['ACCOUNT'].value_counts()
111/51: df4_SC['weighted_memorry'] = (df4_SC['MEM']/df4_SC['GPUS'])*df4_SC['USED_SEC']
111/52: df4_SC.head()
111/53: df4_SC['EXITCODE'].value_counts()
111/54:
#THere are lot of jobs that are UNDERESTIMATING the required time
(df4_SC['TIME_ESTIMATE'] < 0).sum()
111/55:
#Per partition what is the total weighted meorry {(mem/gpus)*used_sec}
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
w.plot(kind='bar',  ylabel = 'Weighted_memorry * 10^12', title = 'Total Weighted_memorry per partition');
111/56:
#Per partition what is the total time used {(mem/gpus)*used_sec}
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar',  ylabel = 'Seconds * 10^9', title = 'Total Used seconds per partition');
111/57:
#weighted_memorry/total used time in each partition
aa= w/z4
aa
aa.plot(kind='bar',  ylabel = 'MEM_used/USED_SEC', title = 'Total weighted-memmory/total_used sec per partition');
#The turning is using lot of memorry per unittime. 
#The maxwell have more jobs run in less time like the marathon runner while the turing are the marathon runner. May be the nature of job needs larger memmory and more time.
111/58:
#What is the total distribution of the total number of partitions in completed jobs
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
111/59: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
111/60: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
111/61:
df4_underestimate = df4_SC[df4_SC['TIME_ESTIMATE'] <0]
df4_underestimate.head(10)
#df4_underestimate.shape #(2873, 19)
111/62: df4_underestimate.groupby(['PARTITION'])['ACCOUNT'].value_counts()
111/63: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum').sort_values()
111/64: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('mean').sort_values()
111/65: df4_underestimate.groupby(['ACCOUNT', 'PARTITION', 'USER'])['TIME_ESTIMATE', 'MEM'].agg('sum')
111/66: df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
111/67:
import plotly.express as px
  
#df = px.data.iris()
  
fig = px.scatter(df4_SC, x="MEM", y="GPUS",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
111/68:
fig = px.scatter(df1, x="USED_SEC", y="MEM",
                 color="PARTITION",
                 size='GPUS', 
                 hover_data=['STATE'])
  
fig.show()
111/69:
df_glasshouse = df1[df1['ACCOUNT'] == 'glasshouse']
df_glasshouse.columns
111/70:
fig = px.scatter(df_glasshouse, x="MEM", y="GPUS",
                 color="USER",
                 size='USED_SEC', 
                 hover_data=['STATE'])
  
fig.show()
111/71: df1[df1['TIME_ESTIMATE']  == 0].count()
111/72:

#df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
df1[df1['TIME_ESTIMATE'] == 0].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts()
111/73: df1[df1['STATE'] == 'FAILED'].count()
111/74: df1[df1['GPUS'] > 4].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts().head(50)
111/75: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count())/df1[df1['GPUS'] > 4]
111/76:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
111/77: df4_SC.head()
111/78:
df4_SC.head()
df4_SC.to_csv('./data/Completed_jobs.csv')
113/1: import pandas as pd
113/2: df4_SC = pd.read_csv('./data/Completed_jobs.csv')
114/1:
import pandas as pd
df4_SC = pd.read_csv('./data/Completed_jobs.csv')
114/2: df4_SC.head()
111/79:
df4_SC.head()
df4_SC.to_csv('./data/Completed_jobs.csv', index=False)
114/3:
import pandas as pd
df4_SC = pd.read_csv('./data/Completed_jobs.csv')
117/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
117/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
117/3: df1.head()
117/4: df1.shape
117/5: df1.info()
117/6: df1['USER'].nunique()
117/7: df1['ACCOUNT'].nunique()
117/8: df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values()
117/9: ((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100
117/10: df1['ACCOUNT'].value_counts()
117/11: df1['JOBID'].nunique()
117/12: df1['USEDTIME'].value_counts()
117/13: df1['USEDMEM'].value_counts()
117/14: df1['GPUS'].value_counts().tail(20).sort_index()
117/15: df1['CPUS'].value_counts().tail(20).sort_index().plot()
117/16: df1['NODES'].value_counts()
117/17: df1['EXITCODE'].value_counts()
117/18:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot()
117/19: df1['REQTIME'].value_counts()
117/20:
#f1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index() 
# this is not going to add the numbers in USEDMEM because it is an object, so we see the
117/21: df1['STATE'].nunique()
117/22: df1['STATE'].value_counts()
117/23: df1['STATE'].unique()
117/24:
#df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
#df2.head(10)
117/25: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
117/26: df_State_comleted.shape
117/27: df_State_comleted['ACCOUNT'].value_counts()
117/28: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
117/29:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
117/30: df1.head(10)
117/31:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-:')
time_list
117/32: re.split('[-:]', time)
117/33:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1): 
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
117/34:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
117/35:
def time_function(time): 
    time_list1 = re.split('[-:]', time)
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    return (total_sec)
117/36: time_function(time)
117/37:
df1['USED_SEC'] = df1['USEDTIME'].apply(time_function)
df1['REQ_SEC'] = df1['REQTIME'].apply(time_function)
df1.head(10)
117/38:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)

df1['time_delta'] = pd.to_timedelta(df1['time_day'])

df1.head(10)
117/39:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df1 = df1.astype({"MEM":'float'})
df1.info()
117/40: df1.groupby(['PARTITION', 'GPUS'])['MEM'].sum()
117/41:
#df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
#df3.head(50)
117/42:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
117/43:
#How far are the time estimates.
df1.head(20)
117/44:
df1['TIME_ESTIMATE'].lt(0).sum() #4247 rows have negative value where used time is more than requested time.

df1.loc[df1['STATE'] == 'COMPLETED']['TIME_ESTIMATE'].lt(0).sum()  #there are 2873 jobID that got completed with more than requested time?
117/45:
x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
x
117/46:
y = df1.groupby(['PARTITION'])['GPUS'].agg('sum')
y
117/47:
z = df1.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z
117/48: x/y #average memorry usage per GPU in each partition.
117/49: df4_SC = df1.loc[df1['STATE'] == 'COMPLETED']
117/50:
df4_SC.shape # (283333, 18)
df4_SC['ACCOUNT'].nunique() #27 
df4_SC['ACCOUNT'].value_counts()
117/51: df4_SC['weighted_memorry'] = (df4_SC['MEM']/df4_SC['GPUS'])*df4_SC['USED_SEC']
117/52: df4_SC.head()
117/53: df4_SC['EXITCODE'].value_counts()
117/54:
#THere are lot of jobs that are UNDERESTIMATING the required time
(df4_SC['TIME_ESTIMATE'] < 0).sum()
117/55:
#Per partition what is the total weighted meorry {(mem/gpus)*used_sec}
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
w.plot(kind='bar',  ylabel = 'Weighted_memorry * 10^12', title = 'Total Weighted_memorry per partition');
117/56:
#Per partition what is the total time used {(mem/gpus)*used_sec}
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar',  ylabel = 'Seconds * 10^9', title = 'Total Used seconds per partition');
117/57:
#weighted_memorry/total used time in each partition
aa= w/z4
aa
aa.plot(kind='bar',  ylabel = 'MEM_used/USED_SEC', title = 'Total weighted-memmory/total_used sec per partition');
#The turning is using lot of memorry per unittime. 
#The maxwell have more jobs run in less time like the marathon runner while the turing are the marathon runner. May be the nature of job needs larger memmory and more time.
117/58:
#What is the total distribution of the total number of partitions in completed jobs
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
117/59: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
117/60: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
117/61:
df4_underestimate = df4_SC[df4_SC['TIME_ESTIMATE'] <0]
df4_underestimate.head(10)
#df4_underestimate.shape #(2873, 19)
117/62: df4_underestimate.groupby(['PARTITION'])['ACCOUNT'].value_counts()
117/63: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum').sort_values()
117/64: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('mean').sort_values()
117/65: df4_underestimate.groupby(['ACCOUNT', 'PARTITION', 'USER'])['TIME_ESTIMATE', 'MEM'].agg('sum')
117/66: df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
117/67:
import plotly.express as px
  
#df = px.data.iris()
  
fig = px.scatter(df4_SC, x="MEM", y="GPUS",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
117/68:
fig = px.scatter(df1, x="USED_SEC", y="MEM",
                 color="PARTITION",
                 size='GPUS', 
                 hover_data=['STATE'])
  
fig.show()
117/69:
df_glasshouse = df1[df1['ACCOUNT'] == 'glasshouse']
df_glasshouse.columns
117/70:
fig = px.scatter(df_glasshouse, x="MEM", y="GPUS",
                 color="USER",
                 size='USED_SEC', 
                 hover_data=['STATE'])
  
fig.show()
117/71: df1[df1['TIME_ESTIMATE']  == 0].count()
117/72:

#df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
df1[df1['TIME_ESTIMATE'] == 0].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts()
117/73: df1[df1['STATE'] == 'FAILED'].count()
117/74: df1[df1['GPUS'] > 4].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts().head(50)
117/75:
df4_SC.head()
df4_SC.to_csv('./data/Completed_jobs.csv', index=False)
117/76: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count())/df1[df1['GPUS'] > 4]
117/77:
#import plotly.express as px
  
#df = px.data.iris()
  
fig = px.scatter(df4_SC, x="MEM", y="GPUS",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
117/78: df1.shape
117/79:
df1.shape #(336950, 18)

df1.columns()
117/80:
df1.shape #(336950, 18)

df1.columns
120/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
120/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
120/3: df1.head()
120/4: df1.shape
120/5: df1.info()
120/6: df1['USER'].nunique()
120/7: df1['ACCOUNT'].nunique()
120/8: df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values()
120/9: ((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100
120/10: df1['ACCOUNT'].value_counts()
120/11: df1['JOBID'].nunique()
120/12: df1['USEDTIME'].value_counts()
120/13: df1['USEDMEM'].value_counts()
120/14: df1['GPUS'].value_counts().tail(20).sort_index()
120/15: df1['CPUS'].value_counts().tail(20).sort_index().plot()
120/16: df1['NODES'].value_counts()
120/17: df1['EXITCODE'].value_counts()
120/18:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot()
120/19: df1['REQTIME'].value_counts()
120/20:
#f1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index() 
# this is not going to add the numbers in USEDMEM because it is an object, so we see the
120/21: df1['STATE'].nunique()
120/22: df1['STATE'].value_counts()
120/23: df1['STATE'].unique()
120/24:
#df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
#df2.head(10)
120/25: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
120/26: df_State_comleted.shape
120/27: df_State_comleted['ACCOUNT'].value_counts()
120/28: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
120/29:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
120/30: df1.head(10)
120/31:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-:')
time_list
120/32: re.split('[-:]', time)
120/33:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1): 
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
120/34:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
120/35:
def time_function(time): 
    time_list1 = re.split('[-:]', time)
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    return (total_sec)
120/36: time_function(time)
120/37:
df1['USED_SEC'] = df1['USEDTIME'].apply(time_function)
df1['REQ_SEC'] = df1['REQTIME'].apply(time_function)
df1.head(10)
120/38:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)

df1['time_delta'] = pd.to_timedelta(df1['time_day'])

df1.head(10)
120/39:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df1 = df1.astype({"MEM":'float'})
df1.info()
120/40: df1.groupby(['PARTITION', 'GPUS'])['MEM'].sum()
120/41:
#df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
#df3.head(50)
120/42:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
120/43:
#How far are the time estimates.
df1.head(20)
120/44:
df1.shape #(336950, 18)

df1.columns
120/45:
df1['TIME_ESTIMATE'].lt(0).sum() #4247 rows have negative value where used time is more than requested time.

df1.loc[df1['STATE'] == 'COMPLETED']['TIME_ESTIMATE'].lt(0).sum()  #there are 2873 jobID that got completed with more than requested time?
120/46:
x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
x
120/47:
y = df1.groupby(['PARTITION'])['GPUS'].agg('sum')
y
120/48:
z = df1.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z
120/49: x/y #average memorry usage per GPU in each partition.
120/50: df4_SC = df1.loc[df1['STATE'] == 'COMPLETED']
120/51:
df4_SC.shape # (283333, 18)
df4_SC['ACCOUNT'].nunique() #27 
df4_SC['ACCOUNT'].value_counts()
120/52: df4_SC['weighted_memorry'] = (df4_SC['MEM']/df4_SC['GPUS'])*df4_SC['USED_SEC']
120/53: df4_SC.head()
120/54: df4_SC['EXITCODE'].value_counts()
120/55:
#THere are lot of jobs that are UNDERESTIMATING the required time
(df4_SC['TIME_ESTIMATE'] < 0).sum()
120/56:
#Per partition what is the total weighted meorry {(mem/gpus)*used_sec}
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
w.plot(kind='bar',  ylabel = 'Weighted_memorry * 10^12', title = 'Total Weighted_memorry per partition');
120/57:
#Per partition what is the total time used {(mem/gpus)*used_sec}
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar',  ylabel = 'Seconds * 10^9', title = 'Total Used seconds per partition');
120/58:
#weighted_memorry/total used time in each partition MEMMORY INTENSIVE JOBS
aa= w/z4
aa
aa.plot(kind='bar',  ylabel = 'MEM_used/USED_SEC', title = 'Total weighted-memmory/total_used sec per partition');
#The turning is using lot of memorry per unittime. 
#The maxwell have more jobs run in less time like the marathon runner while the turing are the marathon runner. May be the nature of job needs larger memmory and more time.
120/59:
#What is the total distribution of the total number of partitions in completed jobs
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
120/60: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
120/61: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
120/62:
df4_underestimate = df4_SC[df4_SC['TIME_ESTIMATE'] <0]
df4_underestimate.head(10)
#df4_underestimate.shape #(2873, 19)
120/63: df4_underestimate.groupby(['PARTITION'])['ACCOUNT'].value_counts()
120/64: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum').sort_values()
120/65: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('mean').sort_values()
120/66: df4_underestimate.groupby(['ACCOUNT', 'PARTITION', 'USER'])['TIME_ESTIMATE', 'MEM'].agg('sum')
120/67: df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
120/68:
#import plotly.express as px
  
 
fig = px.scatter(df4_SC, x="MEM", y="GPUS",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
120/69:
import plotly.express as px
  
 
fig = px.scatter(df4_SC, x="MEM", y="GPUS",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
120/70:
fig = px.scatter(df1, x="USED_SEC", y="MEM",
                 color="PARTITION",
                 size='GPUS', 
                 hover_data=['STATE'])
  
fig.show()
120/71:
df_glasshouse = df1[df1['ACCOUNT'] == 'glasshouse']
df_glasshouse.columns
120/72:
fig = px.scatter(df_glasshouse, x="MEM", y="GPUS",
                 color="USER",
                 size='USED_SEC', 
                 hover_data=['STATE'])
  
fig.show()
120/73: df1[df1['TIME_ESTIMATE']  == 0].count()
120/74:

#df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
df1[df1['TIME_ESTIMATE'] == 0].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts()
120/75: df1[df1['STATE'] == 'FAILED'].count()
120/76: df1[df1['GPUS'] > 4].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts().head(50)
120/77:
df4_SC.head()
df4_SC.to_csv('./data/Completed_jobs.csv', index=False)
120/78: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count())/df1[df1['GPUS'] > 4]
120/79:
df4_SC.head()
df4_SC.to_csv('./data/Completed_jobs.csv', index=False)

df1.to_csv('./data/accre_job.csv', index=False)
121/1: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()/df1[df1['GPUS'] > 4]
122/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
122/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
122/3: df1.head()
122/4: df1.shape
122/5: df1.info()
122/6: df1['USER'].nunique()
122/7: df1['ACCOUNT'].nunique()
122/8: df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values()
122/9: ((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100
122/10: df1['ACCOUNT'].value_counts()
122/11: df1['JOBID'].nunique()
122/12: df1['USEDTIME'].value_counts()
122/13: df1['USEDMEM'].value_counts()
122/14: df1['GPUS'].value_counts().tail(20).sort_index()
122/15: df1['CPUS'].value_counts().tail(20).sort_index().plot()
122/16: df1['NODES'].value_counts()
122/17: df1['EXITCODE'].value_counts()
122/18:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot()
122/19: df1['REQTIME'].value_counts()
122/20:
#f1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index() 
# this is not going to add the numbers in USEDMEM because it is an object, so we see the
122/21: df1['STATE'].nunique()
122/22: df1['STATE'].value_counts()
122/23: df1['STATE'].unique()
122/24:
#df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
#df2.head(10)
122/25: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
122/26: df_State_comleted.shape
122/27: df_State_comleted['ACCOUNT'].value_counts()
122/28: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
122/29:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
122/30: df1.head(10)
122/31:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-:')
time_list
122/32: re.split('[-:]', time)
122/33:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1): 
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
122/34:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
122/35:
def time_function(time): 
    time_list1 = re.split('[-:]', time)
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    return (total_sec)
122/36: time_function(time)
122/37:
df1['USED_SEC'] = df1['USEDTIME'].apply(time_function)
df1['REQ_SEC'] = df1['REQTIME'].apply(time_function)
df1.head(10)
122/38:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)

df1['time_delta'] = pd.to_timedelta(df1['time_day'])

df1.head(10)
122/39:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df1 = df1.astype({"MEM":'float'})
df1.info()
122/40: df1.groupby(['PARTITION', 'GPUS'])['MEM'].sum()
122/41:
#df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
#df3.head(50)
122/42:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
122/43:
#How far are the time estimates.
df1.head(20)
122/44:
df1.shape #(336950, 18)

df1.columns
122/45:
df1['TIME_ESTIMATE'].lt(0).sum() #4247 rows have negative value where used time is more than requested time.

df1.loc[df1['STATE'] == 'COMPLETED']['TIME_ESTIMATE'].lt(0).sum()  #there are 2873 jobID that got completed with more than requested time?
122/46:
x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
x
122/47:
y = df1.groupby(['PARTITION'])['GPUS'].agg('sum')
y
122/48:
z = df1.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z
122/49: x/y #average memorry usage per GPU in each partition.
122/50: df4_SC = df1.loc[df1['STATE'] == 'COMPLETED']
122/51:
df4_SC.shape # (283333, 18)
df4_SC['ACCOUNT'].nunique() #27 
df4_SC['ACCOUNT'].value_counts()
122/52: df4_SC['weighted_memorry'] = (df4_SC['MEM']/df4_SC['GPUS'])*df4_SC['USED_SEC']
122/53: df4_SC.head()
122/54: df4_SC['EXITCODE'].value_counts()
122/55:
#THere are lot of jobs that are UNDERESTIMATING the required time
(df4_SC['TIME_ESTIMATE'] < 0).sum()
122/56:
#Per partition what is the total weighted meorry {(mem/gpus)*used_sec}
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
w.plot(kind='bar',  ylabel = 'Weighted_memorry * 10^12', title = 'Total Weighted_memorry per partition');
122/57:
#Per partition what is the total time used {(mem/gpus)*used_sec}
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar',  ylabel = 'Seconds * 10^9', title = 'Total Used seconds per partition');
122/58:
#weighted_memorry/total used time in each partition MEMMORY INTENSIVE JOBS
aa= w/z4
aa
aa.plot(kind='bar',  ylabel = 'MEM_used/USED_SEC', title = 'Total weighted-memmory/total_used sec per partition');
#The turning is using lot of memorry per unittime. 
#The maxwell have more jobs run in less time like the marathon runner while the turing are the marathon runner. May be the nature of job needs larger memmory and more time.
122/59:
#What is the total distribution of the total number of partitions in completed jobs
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
122/60: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
122/61: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
122/62:
df4_underestimate = df4_SC[df4_SC['TIME_ESTIMATE'] <0]
df4_underestimate.head(10)
#df4_underestimate.shape #(2873, 19)
122/63: df4_underestimate.groupby(['PARTITION'])['ACCOUNT'].value_counts()
122/64: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum').sort_values()
122/65: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('mean').sort_values()
122/66: df4_underestimate.groupby(['ACCOUNT', 'PARTITION', 'USER'])['TIME_ESTIMATE', 'MEM'].agg('sum')
122/67: df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
122/68:
import plotly.express as px
  
 
fig = px.scatter(df4_SC, x="MEM", y="GPUS",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
122/69:
fig = px.scatter(df1, x="USED_SEC", y="MEM",
                 color="PARTITION",
                 size='GPUS', 
                 hover_data=['STATE'])
  
fig.show()
122/70:
df_glasshouse = df1[df1['ACCOUNT'] == 'glasshouse']
df_glasshouse.columns
122/71:
fig = px.scatter(df_glasshouse, x="MEM", y="GPUS",
                 color="USER",
                 size='USED_SEC', 
                 hover_data=['STATE'])
  
fig.show()
122/72: df1[df1['TIME_ESTIMATE']  == 0].count()
122/73:

#df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
df1[df1['TIME_ESTIMATE'] == 0].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts()
122/74: df1[df1['STATE'] == 'FAILED'].count()
122/75: df1[df1['GPUS'] > 4].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts().head(50)
122/76:
df4_SC.head()
df4_SC.to_csv('./data/Completed_jobs.csv', index=False)

df1.to_csv('./data/accre_job.csv', index=False)
122/77: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()/df1[df1['GPUS'] > 4]
122/78: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()/df1[df1['GPUS'] > 4]
122/79: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()
122/80: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()/len(df.axes[0])
122/81: df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()/len(df1.axes[0])
122/82: df1.columns
122/83:
 
sns.violinplot(x='ACCOUNT', y='MEM',
                   data=df1, hue='PARTITION', palette='pastel')
122/84:
 
sns.violinplot(x='PARTITION', y='MEM',
                   data=df1, hue='USER', palette='pastel')
122/85:
 
sns.violinplot(x='PARTITION', y='MEM',
                   data=df1, palette='pastel')
122/86:
 
sns.violinplot(x='PARTITION', y=log'MEM',
                   data=df1, palette='pastel')
122/87:
 
sns.violinplot(x='USED_SEC', y= 'MEM',
                   data=df1, hue='PARTITION' palette='pastel')
122/88:
 
sns.violinplot(x='USED_SEC', y= 'MEM',
                   data=df1, hue='PARTITION', palette='pastel')
124/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df1 = pd.read_csv('./data/accre_job.csv')
#df4_SC = pd.read_csv('./data/Completed_jobs.csv')
124/2: df1.head()
122/89:
 
sns.violinplot(x='USED_SEC', y= 'REQ_SEC',
                   data=df1, hue='PARTITION', palette='pastel')
124/3: df1.describe()
124/4: df.info()
124/5: df1.info()
122/90:
 
#sns.violinplot(x='USED_SEC', y= 'REQ_SEC',
                   data=df1, hue='PARTITION', palette='pastel')
124/6: df1['STATE'].value_counts()
124/7: df['modSTATE'] = df['STATE']
124/8: df1['modSTATE'] = df1['STATE']
124/9:
df1['modSTATE'] = df1['STATE']
df1.head()
124/10: df1['mod_STATE'].str.split(' by', expand = True)[0]
124/11: df1['modSTATE'].str.split(' by', expand = True)[0]
124/12: df1['modSTATE'].str.split(' by', expand = True)[0].value_counts()
128/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
129/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
129/2: df1 = pd.read_csv('data/accre-gpu-jobs-2022-v2.csv', error_bad_lines=False)
129/3: df1.head()
129/4: df1.shape
129/5: df1.info()
129/6: df1['USER'].nunique()
129/7: df1['ACCOUNT'].nunique()
129/8: df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values()
129/9: ((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100
129/10: df1['ACCOUNT'].value_counts()
129/11: df1['JOBID'].nunique()
129/12: df1['USEDTIME'].value_counts()
129/13: df1['USEDMEM'].value_counts()
129/14: df1['GPUS'].value_counts().tail(20).sort_index()
129/15: df1['CPUS'].value_counts().tail(20).sort_index().plot()
129/16: df1['NODES'].value_counts()
129/17: df1['EXITCODE'].value_counts()
129/18:
df1['PARTITION'].value_counts().tail(20).sort_index()
df1['PARTITION'].value_counts().tail(20).sort_index().plot()
129/19: df1['REQTIME'].value_counts()
129/20:
#f1.groupby(['ACCOUNT','PARTITION'])['USEDMEM'].sum().sort_index() 
# this is not going to add the numbers in USEDMEM because it is an object, so we see the
129/21: df1['STATE'].nunique()
129/22: df1['STATE'].value_counts()
129/23: df1['STATE'].unique()
129/24:
#df2 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['USEDMEM', 'USEDTIME'].agg('sum')
#df2.head(10)
129/25: df_State_comleted = df1.loc[df1['STATE'] == 'COMPLETED']
129/26: df_State_comleted.shape
129/27: df_State_comleted['ACCOUNT'].value_counts()
129/28: #df1['Time'] = pd.to_timedelta(df1['USEDTIME']) #this is not needed we will split using the regullr expressions
129/29:
time = df1['USEDTIME'].tolist()[0] # converting the first row into the list and spliting on the :
time_list = time.split(':')
print(time_list)

hours = time_list[0]
minutes = time_list[1]
seconds = time_list[2]
total = (int(hours) * 3600 + int(minutes) * 60 + int(seconds))
print(total)
129/30: df1.head(10)
129/31:
time = df1['USEDTIME'].tolist()[6]
time_list = time.split('-:')
time_list
129/32: re.split('[-:]', time)
129/33:
time = df1['USEDTIME'].tolist()[6] # there is a day in the usedtime
time_list1 = re.split('[-:]', time)
print(time_list1)
x = []

def time_function(time_list1): 
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_1 = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    print(total_1)
129/34:
time_list1  = re.split('[-:]', time)
time_function(time_list1)

##it is working for the days in the column (d-hh:mm:ss) we need to modify the slplit arrgument so that the list item at index [0] becomes 0 if there are no days.
129/35:
def time_function(time): 
    time_list1 = re.split('[-:]', time)
    if len(time_list1) != 4:
        days = 0
    else:
        days = time_list1[-4]
    hours = time_list1[-3]
    minutes = time_list1[-2]
    seconds = time_list1[-1]
    total_sec = (int(days)* 86400 + int(hours) * 3600 + int(minutes) * 60 + int(seconds))
    #x.append(total)
    return (total_sec)
129/36: time_function(time)
129/37:
df1['USED_SEC'] = df1['USEDTIME'].apply(time_function)
df1['REQ_SEC'] = df1['REQTIME'].apply(time_function)
df1.head(10)
129/38:
df1['time_day'] = df1.USEDTIME.str.replace('-','days ',regex=True)

df1['time_delta'] = pd.to_timedelta(df1['time_day'])

df1.head(10)
129/39:
df1['MEM'] = df1['USEDMEM'].str.replace('M','',regex=True).str.strip()
#list = mem.split('M')
#list
df1 = df1.astype({"MEM":'float'})
df1.info()
129/40: df1.groupby(['PARTITION', 'GPUS'])['MEM'].sum()
129/41:
#df3 = df1.groupby(['ACCOUNT','PARTITION','STATE'])['MEM', 'USED_SEC'].agg('sum')
#df3.head(50)
129/42:
#time overestimate

df1['TIME_ESTIMATE'] = df1['REQ_SEC'] - df1['USED_SEC']
df1['TIME_ESTIMATE'].value_counts()
129/43:
#How far are the time estimates.
df1.head(20)
129/44:
df1.shape #(336950, 18)

df1.columns
129/45:
df1['TIME_ESTIMATE'].lt(0).sum() #4247 rows have negative value where used time is more than requested time.

df1.loc[df1['STATE'] == 'COMPLETED']['TIME_ESTIMATE'].lt(0).sum()  #there are 2873 jobID that got completed with more than requested time?
129/46:
x = df1.groupby(['PARTITION'])['MEM'].agg('sum')
x
129/47:
y = df1.groupby(['PARTITION'])['GPUS'].agg('sum')
y
129/48:
z = df1.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z
129/49: x/y #average memorry usage per GPU in each partition.
129/50: df4_SC = df1.loc[df1['STATE'] == 'COMPLETED']
129/51:
df4_SC.shape # (283333, 18)
df4_SC['ACCOUNT'].nunique() #27 
df4_SC['ACCOUNT'].value_counts()
129/52: df4_SC['weighted_memorry'] = (df4_SC['MEM']/df4_SC['GPUS'])*df4_SC['USED_SEC']
129/53: df4_SC.head()
129/54: df4_SC['EXITCODE'].value_counts()
129/55:
#THere are lot of jobs that are UNDERESTIMATING the required time
(df4_SC['TIME_ESTIMATE'] < 0).sum()
129/56:
#Per partition what is the total weighted meorry {(mem/gpus)*used_sec}
w= df4_SC.groupby(['PARTITION'])['weighted_memorry'].agg('sum')
w
w.plot(kind='bar',  ylabel = 'Weighted_memorry * 10^12', title = 'Total Weighted_memorry per partition');
129/57:
#Per partition what is the total time used {(mem/gpus)*used_sec}
z4 = df4_SC.groupby(['PARTITION'])['USED_SEC'].agg('sum')
z4
z4.plot(kind='bar',  ylabel = 'Seconds * 10^9', title = 'Total Used seconds per partition');
129/58:
#weighted_memorry/total used time in each partition MEMMORY INTENSIVE JOBS
aa= w/z4
aa
aa.plot(kind='bar',  ylabel = 'MEM_used/USED_SEC', title = 'Total weighted-memmory/total_used sec per partition');
#The turning is using lot of memorry per unittime. 
#The maxwell have more jobs run in less time like the marathon runner while the turing are the marathon runner. May be the nature of job needs larger memmory and more time.
129/59:
#What is the total distribution of the total number of partitions in completed jobs
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
129/60: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
129/61: df4_SC.groupby(['ACCOUNT', 'PARTITION'])['GPUS'].agg('sum').sort_values()
129/62:
df4_underestimate = df4_SC[df4_SC['TIME_ESTIMATE'] <0]
df4_underestimate.head(10)
#df4_underestimate.shape #(2873, 19)
129/63: df4_underestimate.groupby(['PARTITION'])['ACCOUNT'].value_counts()
129/64: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('sum').sort_values()
129/65: df4_underestimate.groupby(['ACCOUNT', 'PARTITION'])['TIME_ESTIMATE'].agg('mean').sort_values()
129/66: df4_underestimate.groupby(['ACCOUNT', 'PARTITION', 'USER'])['TIME_ESTIMATE', 'MEM'].agg('sum')
129/67: df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
129/68:
import plotly.express as px
  
 
fig = px.scatter(df4_SC, x="MEM", y="GPUS",
                 color="ACCOUNT",
                 size='USED_SEC', 
                 hover_data=['USED_SEC'])
  
fig.show()
129/69:
fig = px.scatter(df1, x="USED_SEC", y="MEM",
                 color="PARTITION",
                 size='GPUS', 
                 hover_data=['STATE'])
  
fig.show()
129/70:
df_glasshouse = df1[df1['ACCOUNT'] == 'glasshouse']
df_glasshouse.columns
129/71:
fig = px.scatter(df_glasshouse, x="MEM", y="GPUS",
                 color="USER",
                 size='USED_SEC', 
                 hover_data=['STATE'])
  
fig.show()
129/72: df1[df1['TIME_ESTIMATE']  == 0].count()
129/73:

#df1[df1['ACCOUNT'] == 'glasshouse'].groupby(['PARTITION', 'USER'])['STATE'].value_counts().sort_values()
df1[df1['TIME_ESTIMATE'] == 0].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts()
129/74: df1[df1['STATE'] == 'FAILED'].count()
129/75: df1[df1['GPUS'] > 4].groupby(['PARTITION', 'ACCOUNT', 'USER'])['STATE'].value_counts().head(50)
129/76:
df4_SC.head()
df4_SC.to_csv('./data/Completed_jobs.csv', index=False)

df1.to_csv('./data/accre_job.csv', index=False)
129/77:
df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()/len(df1.axes[0])

#total number of rows in dfrows = len(df.axes[0])
#total number of columns in dfcols = len(df.axes[1])
# slash and reading / forwardslash
129/78: df1.columns
129/79:
 
#sns.violinplot(x='USED_SEC', y= 'REQ_SEC',
                   data=df1, hue='PARTITION', palette='pastel')
129/80: ((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100.plot(kind='bar')
129/81: (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100).plot(kind='bar')
127/1: df1['ACCOUNT'].nunique()
131/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df1 = pd.read_csv('./data/accre_job.csv')
#df4_SC = pd.read_csv('./data/Completed_jobs.csv')
131/2: df1.head()
131/3: df1.describe()
131/4: df1.info()
131/5: df1['STATE'].value_counts()
131/6:
df1['modSTATE'] = df1['STATE']
df1.head()
131/7: df1['modSTATE'].str.split(' by', expand = True)[0].value_counts()
131/8: df1['ACCOUNT'].nunique()
131/9: df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values()
131/10: (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100).plot(kind='bar')
131/11:
(((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
    .plot(kind='bar')
131/12: (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100).plot(kind='bar')
131/13:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', xlabel='%User', ylabel='Account');


#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
131/14:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account');


#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
129/82:
#What is the total distribution of the total number of partitions in completed jobs
PAR = df4_SC['PARTITION'].value_counts()
PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition')
plt.xticks(rotation = 45, fontsize = 14);
131/15:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 45, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
131/16:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 45, fontsize = 10);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
131/17:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
129/83:
ab = df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()/len(df1.axes[0])

#total number of rows in dfrows = len(df.axes[0])
#total number of columns in dfcols = len(df.axes[1])
# slash and reading / forwardslash
129/84:
ab = df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()/len(df1.axes[0])
plt.figure(figsize = (8,5))
ab.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);


#total number of rows in dfrows = len(df.axes[0])
#total number of columns in dfcols = len(df.axes[1])
# slash and reading / forwardslash
129/85:
df1[df1['GPUS'] > 4].groupby(['PARTITION']).count()/len(df1.axes[0])

#total number of rows in dfrows = len(df.axes[0])
#total number of columns in dfcols = len(df.axes[1])
# slash and reading / forwardslash
131/18: df1['ACCOUNT'].value_counts()
131/19: df1['ACCOUNT'].value_counts()/len(df1.axes[0])
131/20: df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
131/21:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100

plt.figure(figsize = (8,5))
per_acc.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/22:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc
plt.figure(figsize = (8,5))
per_acc.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/23:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc
131/24:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values()
131/25:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(5)
131/26:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(5).plot
131/27:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(5).plot(kind='bar')
131/28:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(23).plot(kind='bar')
131/29:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(5).plot(kind='bar')
131/30:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(6).plot(kind='bar')
131/31:
#percent of the each ACCOUNT in the data set

per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar')
131/32:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar', title = 'Bottom 10 ACCUNT represented in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/33:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/34:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().tail(10).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/35:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().tail(5).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/36:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/37:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
#plt.xticks(rotation = 90, fontsize = 14);
131/38:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
#per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
#plt.xticks(rotation = 90, fontsize = 14);
131/39:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/40:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/41:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in dataset
', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/42:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in dataset', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
131/43:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in dataset', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 0, fontsize = 14);
131/44:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
131/45:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
131/46: df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values()
131/47: f1.groupby(['PARTITION'])['ACCOUNT'].value_counts()
131/48: df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()
131/49: df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/df1['USER'].nunique())*100)
131/50: (df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/df1['USER'].nunique())*100)
131/51: (df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/df1['USER'].nunique())*100
131/52: df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()
131/53: (df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100
131/54: df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100
131/55: df1.groupby(df1['PARTITION']== 'maxwell')['ACCOUNT'].value_counts()
131/56: df1.groupby(df1['PARTITION']== 'maxwell')['MEM'].agg('sum')
131/57: df1.groupby(df1['PARTITION']== 'maxwell')['MEM'].agg('sum')/df1['MEM'].sum()
131/58: (df1.groupby(df1['PARTITION']== 'maxwell')['MEM'].agg('sum')/df1['MEM'].sum())*100
131/59: (df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
131/60: (df1.groupby(df1['PARTITION'])['GPUS'].agg('sum')/df1['MEM'].sum())*100
131/61: (df1.groupby(df1['PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
131/62:
#What is the percentage of total GPU usgae by each partition
(df1.groupby(df1['PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
131/63: (df1.groupby(df1['PARTITION', 'ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
131/64: (df1.groupby(df1['PARTITION', 'ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
131/65: (df1.groupby(['PARTITION', 'ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
131/66: (df1.groupby(['PARTITION', 'ACCOUNT'])['STATE'].value_counts
131/67: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts()
131/68: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts()0
131/69: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts())
131/70: df1['modSTATE'] = df1['modSTATE'].str.split(' by', expand = True)[0].value_counts()
131/71: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts())
131/72:
df1['modSTATE'] = df1['modSTATE'].str.split(' by', expand = True)[0]
df1[]
131/73:
df1['modSTATE'] = df1['modSTATE'].str.split(' by', expand = True)[0]
df1['modSTATE'].value_counts()
131/74:
df1['modSTATE'].str.split(' by', expand = True)[0].value_counts()
#df1['modSTATE'].value_counts()
132/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df1 = pd.read_csv('./data/accre_job.csv')
#df4_SC = pd.read_csv('./data/Completed_jobs.csv')
132/2: df1.head()
132/3: df1.describe()
132/4: df1.info()
132/5: df1['STATE'].value_counts()
132/6:
df1['modSTATE'] = df1['STATE']
df1.head()
132/7:
df1['modSTATE'].str.split(' by', expand = True)[0].value_counts()
#df1['modSTATE'].value_counts()
132/8:
#what is the percentage distribution of the three partitions.
#what is the percentage distribution of the users across partitions
#what is the percentage distribution of the jobs STATE in data
#what is the percntage memmorry, GPU, USEDTIME REQTIME and timeestimate in data per partition and peruser.
132/9: df1['ACCOUNT'].nunique()
132/10:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
132/11:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
132/12:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
132/13: df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values()
132/14:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
132/15: df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values()
132/16: df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100
132/17: (df1.groupby(df1['PARTITION']== 'maxwell')['MEM'].agg('sum')/df1['MEM'].sum())*100
132/18:
#What percentage of total memorry is used by each partition
(df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
132/19:
#What is the percentage of total GPU usgae by each partition
(df1.groupby(df1['PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
132/20: (df1.groupby(['PARTITION', 'ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
132/21: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts())
132/22:
df1['modSTATE'].str.split(' by', expand = True)[0]
#df1['modSTATE'].value_counts()
132/23:
df1['modSTATE']=df1['modSTATE'].str.split(' by', expand = True)[0]
#df1['modSTATE'].value_counts()
132/24:
df1['modSTATE']=df1['modSTATE'].str.split(' by', expand = True)[0]
df1['modSTATE']
#df1['modSTATE'].value_counts()
132/25:
df1['modSTATE']=df1['modSTATE'].str.split(' by', expand = True)[0]
df1['modSTATE'].value_counts()
#df1['modSTATE'].value_counts()
132/26: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts())
132/27: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts()/len(df1.axes[0])*100
132/28: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts()/len(df1.axes[0])*100)
132/29:
g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g.map_dataframe(sns.boxplot, x="STATE", y="GPUS")
g.set_axis_labels("Status of the Jobs", "GPUS")
g.set_titles(col_template="{col_name} ", row_template="{row_name}")
g.set(xlim=(0, 60), ylim=(0, 12), xticks=[10, 30, 50], yticks=[2, 6, 10])
g.tight_layout()
g.savefig("facet_plot.png")
132/30:
g = sns.FacetGrid(df1, col="PARTITION", row="STATE", margin_titles=True)
g.map_dataframe(sns.boxplot, x="ACCOUNT", y="GPUS")
g.set_axis_labels("Status of the Jobs", "GPUS")
g.set_titles(col_template="{col_name} ", row_template="{row_name}")
g.set(xlim=(0, 60), ylim=(0, 12), xticks=[10, 30, 50], yticks=[0.5, 1, 2, 4, 6, 10])
g.tight_layout()
g.savefig("facet_plot.png")
132/31:
g = sns.FacetGrid(df1, col="PARTITION", row="STATE", margin_titles=True)
g.map_dataframe(sns.boxplot, x="ACCOUNT", y="GPUS")
g.set_axis_labels("Status of the Jobs", "GPUS")
g.set_titles(col_template="{col_name} ", row_template="{row_name}")
#g.set(xlim=(0, 60), ylim=(0, 60), xticks=[10, 30, 50], yticks=[0.5, 1, 2, 4, 6, 10])
g.tight_layout()
g.savefig("facet_plot.png")
132/32:
g = sns.FacetGrid(df1, col="PARTITION", row="STATE", margin_titles=True)
g.map_dataframe(sns.boxplot, x="ACCOUNT", y="GPUS")
g.set_axis_labels("Status of the Jobs", "GPUS")
g.set_titles(col_template="{col_name} ", row_template="{row_name}")
g.set(xlim=(0, 60), ylim=(0, 60), xticks=[10, 30, 50], yticks=[0.5, 1, 2, 4, 10, 20, 60])
g.tight_layout()
g.savefig("facet_plot.png")
133/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df1 = pd.read_csv('./data/accre_job.csv')
#df4_SC = pd.read_csv('./data/Completed_jobs.csv')
133/2: df1.head()
133/3: df1.describe()
133/4: df1.info()
133/5: df1['STATE'].value_counts()
133/6:
df1['modSTATE'] = df1['STATE']
df1.head()
133/7:
df1['modSTATE']=df1['modSTATE'].str.split(' by', expand = True)[0]
df1['modSTATE'].value_counts()
#df1['modSTATE'].value_counts()
133/8:
#what is the percentage distribution of the three partitions.
#what is the percentage distribution of the users across partitions
#what is the percentage distribution of the jobs STATE in data
#what is the percntage memmorry, GPU, USEDTIME REQTIME and timeestimate in data per partition and peruser.
133/9: df1['ACCOUNT'].nunique()
133/10:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
133/11:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
133/12:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
133/13: df1.groupby(['ACCOUNT'])['USER'].nunique()
133/14:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
133/15: df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values()
133/16: df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100
133/17: (df1.groupby(df1['PARTITION']== 'maxwell')['MEM'].agg('sum')/df1['MEM'].sum())*100
133/18:
#What percentage of total memorry is used by each partition
(df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
133/19:
#What is the percentage of total GPU usgae by each partition
(df1.groupby(df1['PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
133/20: (df1.groupby(['PARTITION', 'ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
133/21: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts()/len(df1.axes[0])*100)
133/22: df1.groupby(['ACCOUNT'])['USER'].nunique() ==1
133/23: (df1.groupby(['ACCOUNT'])['USER'].nunique() ==1).count()
133/24: (df1.groupby(['ACCOUNT'])['USER'].nunique()).count()
133/25: (df1.groupby(['ACCOUNT'])['USER'].nunique()
133/26: df1.groupby(['ACCOUNT'])['USER'].nunique()
133/27: df1.groupby(['ACCOUNT'])['USER'].nunique().plot()
133/28: df1.groupby(['ACCOUNT'])['USER'].nunique().plot(kind='bar')
133/29: df1.groupby(['ACCOUNT'])['USER'].nunique().plot(kind='bar', title='Number of total users per account', ylabel='Number of users')
133/30:
plt.figure(figsize = (8,5))
user_Count = df1.groupby(['ACCOUNT'])['USER'].nunique()
user_Count.plot(kind='bar', title='Number of total users per account', ylabel='Number of users')
plt.xticks(rotation = 90, fontsize = 14);
133/31:
plt.figure(figsize = (10,8))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
133/32:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
133/33: user_account
133/34:
plt.figure(figsize = (8,5))
user_Count = df1.groupby(['ACCOUNT'])['USER'].nunique()
user_Count.plot(kind='bar', title='Number of total users per account', ylabel='Number of users')
plt.xticks(rotation = 90, fontsize = 14);
user_Count
133/35:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
per_partition
133/36: #user_account
133/37: df1.groupby(['PARTITION'])['ACCOUNT', 'USER'].nunique().sort_values()
133/38: df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values()
133/39: df1.groupby(['PARTITION'])['USER'].nunique().sort_values()
133/40:
df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values().user_account.plot(kind='bar', title = 'Number total ACCOUNTS per PARTITION', ylabel='# of ACCOUNT', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/41:
df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values().plot(kind='bar', title = 'Number total ACCOUNTS per PARTITION', ylabel='# of ACCOUNT', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/42:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values().plot(kind='bar', title = 'Number total ACCOUNTS per PARTITION', ylabel='# of ACCOUNT', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/43:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['USER'].nunique().sort_values().plot(kind='bar', title = 'Number total USERS per PARTITION', ylabel='# of USER', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/44: df1.groupby(['PARTITION']== 'maxwell')['ACCOUNT'].value_counts()/len(df1.axes[0])*100
133/45: df1.groupby(df1['PARTITION']== 'maxwell')['ACCOUNT'].value_counts()/len(df1.axes[0])*100
133/46:
df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100.plot(kind='bar', title = 'Number total USERS per PARTITION', ylabel='# of USER', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/47:
plt.figure(figsize = (8,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = 'Number total USERS per PARTITION', ylabel='# of USER', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/48:
plt.figure(figsize = (8,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = '% distribution of each account per PARTITION', ylabel='% distribution, xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/49:
plt.figure(figsize = (8,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/50:
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/51:
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', hue = df1['PARTITION'], title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/52:
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', color = df1['PARTITION'], title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/53:
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).sns.barplot(kind='bar', hue = df1['PARTITION'], title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/54:
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', color='PARTITION', title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/55:
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/56: df1['PARTITION'] == 'maxwell')['ACCOUNT'].value_counts()/len(df1.axes[0])*100
133/57: (df1['PARTITION'] == 'maxwell')['ACCOUNT'].value_counts()/len(df1.axes[0])*100
133/58: df1.loc(df1['PARTITION'] == 'maxwell')['ACCOUNT'].value_counts()/len(df1.axes[0])*100
133/59: df1.loc[df1['PARTITION'] == 'maxwell']['ACCOUNT'].value_counts()/len(df1.axes[0])*100
133/60:
plt.figure(figsize = (10,5))
max_acc_count=df1.loc[df1['PARTITION'] == 'maxwell']['ACCOUNT'].value_counts()/len(df1.axes[0])*100

max_acc_count.plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/61:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  colors=colors, title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/62:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  color=colors, title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/63:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/64: (df1.loc(df1['PARTITION']== 'maxwell')['MEM'].agg('sum')/df1['MEM'].sum())*100
133/65: (df1.groupby(df1['PARTITION']== 'maxwell')['MEM'].agg('sum')/df1['MEM'].sum())*100
133/66: (df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
133/67: (df1.groupby(df1['PARTITION'])['USEDSEC'].agg('sum')/df1['USEDSEC'].sum())*100)
133/68: (df1.groupby(df1['PARTITION'])['USEDSEC'].agg('sum')/df1['USEDSEC'].sum())*100
133/69: (df1.groupby(df1['PARTITION'])['USED_SEC'].agg('sum')/df1['USED_SEC'].sum())*100
133/70: (df1.groupby(df1['PARTITION'])['REQ_SEC'].agg('sum')/df1['REQ_SEC'].sum())*100
133/71:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['STATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution Job status per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/72:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution Job status per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
133/73: df1.describe()
133/74: df1.describe().plot(kind='boxplot')
133/75: df1.describe().plot(kind='box')
133/76: df1.describe().plot(kind='bar')
133/77: df1.describe()
133/78: df1['NODES'].describe()
133/79: df1['NODES'].describe().plot(kind='bar')
133/80: df1['NODES'].describe().plot()
133/81: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
133/82: (df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
133/83: (df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].describe()
133/84: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].describe()
133/85: df1.groupby[df1['ACCOUNT']]['MEM'].boxplot()
133/86: df1['MEM'].boxplot()
133/87: df1['MEM'].describe.boxplot()
133/88: df1['MEM'].describe().boxplot()
133/89: (df1['MEM'].describe()).boxplot()
133/90: sns.FacetGrid(df1, col="PARTITION", margin_titles=True)
133/91: df1['modSTATE'].valuecounts()
133/92: df1['modSTATE'].value_counts()
133/93: sns.FacetGrid(df1, col="PARTITION", row="modSTATE", margin_titles=True)
133/94: df1(df1['modSTATE'] == 'COMPLETED', 'CANCELLED', 'RUNING', 'FAILED').value_counts()
133/95: df1[df1['modSTATE'] == 'COMPLETED', 'CANCELLED', 'RUNING', 'FAILED'].value_counts()
133/96:
sns.FacetGrid(df1, col="PARTITION", row="modSTATE", margin_titles=True)
g.map_dataframe(sns.boxplot, y="ACCOUNT", x="GPUS")
133/97:
g = sns.FacetGrid(df1, col="PARTITION", row="modSTATE", margin_titles=True)
g.map_dataframe(sns.boxplot, y="ACCOUNT", x="GPUS")
133/98:
g = sns.FacetGrid(df1, col="PARTITION", row="modSTATE", margin_titles=True)
g.map_dataframe(sns.boxplot, x="ACCOUNT", y="GPUS")
133/99:
g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g.map_dataframe(sns.boxplot, x="modSTATE", y="GPUS")
133/100:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)

g = sns.FacetGrid(df1, col="PARTITION", margin_titles=True)
g.map_dataframe(sns.boxplot, x="modSTATE", y= df1['modSTATE'].value_counts())
133/101:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)

g = sns.FacetGrid(df1, col="PARTITION", margin_titles=True)
g.map_dataframe(sns.boxplot, x="ACCOUNT", y= df1['modSTATE'].value_counts())
133/102:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)

g = sns.FacetGrid(df1, col="PARTITION", margin_titles=True)
g.map_dataframe(sns.boxplot, x="ACCOUNT", y="modSTATE")
133/103:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)

g = sns.FacetGrid(df1, col="PARTITION", row = "modSTATE", margin_titles=True)
g.map_dataframe(sns.boxplot, x="ACCOUNT")
133/104:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)

g = sns.FacetGrid(df1, col="PARTITION", row = "modSTATE", margin_titles=True)
g.map_dataframe(sns.boxplot, x="ACCOUNT", y="MEM")
133/105:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
per_acc
133/106: df1['modSTATE'].value_counts()
133/107: df1.groupby(['PARTITION'])['modSTATE'].value_counts()
133/108: df1.groupby(['PARTITION'])['modSTATE'].value_counts().toframe()
133/109: df1.groupby(['PARTITION'])['modSTATE'].value_counts()
133/110:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
y=df1.groupby(['PARTITION'])['modSTATE'].value_counts()
g = sns.FacetGrid(df1, col="PARTITION",  margin_titles=True)
g.map_dataframe(sns.boxplot, x="modSTATE", y=y)
133/111: df1.groupby(['PARTITION'])['modSTATE'].value_counts().reset_index()
133/112: df1.groupby(['PARTITION'])['modSTATE'].value_counts()
133/113: df1.groupby(['PARTITION'])['modSTATE']..transform(lambda x: x.value_counts().idxmax()))
133/114: df1.groupby(['PARTITION'])['modSTATE'].transform(lambda x: x.value_counts().idxmax()))
133/115: df1.groupby(['PARTITION'])['modSTATE'].transform(lambda x: x.value_counts().idxmax())
133/116: df1.groupby(['PARTITION'])['modSTATE'].transform(lambda x: x.value_counts())
133/117: df1.groupby(['PARTITION'])['modSTATE'].value_counts().DataFrame()
133/118: df1.groupby(['PARTITION'])['modSTATE'].value_counts()
133/119:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION",  margin_titles=True)
g.map(sns.boxplot, x="modSTATE", y=1)
133/120: df1.head()
133/121:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION",  margin_titles=True)
g.map(sns.boxplot, x="modSTATE", y= "NODES")
133/122:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION", row="modSTATE" margin_titles=True)
#g.map_dataframe(sns.boxplot, x="ACCOUNT", y="GPUS")
133/123:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION", row="modSTATE"< margin_titles=True)
#g.map_dataframe(sns.boxplot, x="ACCOUNT", y="GPUS")
133/124:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION", row="modSTATE", margin_titles=True)
#g.map_dataframe(sns.boxplot, x="ACCOUNT", y="GPUS")
133/125:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION", row="modSTATE", margin_titles=True)
g.map_dataframe(sns.boxplot, x="ACCOUNT", y="GPUS")
133/126: df1.groupby(['PARTITION'])['modSTATE'].value_counts().reset_index()
133/127: df1.groupby(['PARTITION'])['modSTATE'].value_counts().reset_index(name='test')
133/128: df1.groupby(['PARTITION'])['modSTATE'].value_counts().reset_index(name='value_counts')
133/129: plotdf1= df1.groupby(['PARTITION'])['modSTATE'].value_counts().reset_index(name='value_counts')
133/130:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)
g.map_dataframe(sns.boxplot, x="modSTATE", y="value_counts")
133/131:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)
g.map_dataframe(sns.barplot, x="modSTATE", y="value_counts")
133/132:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)
g.map_dataframe(sns.barplot, x="modSTATE", y="value_counts")
plt.xticks(rotation = 90, fontsize = 14);
133/133:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)
g.map_dataframe(sns.barplot, y="modSTATE", x="value_counts")
#plt.xticks(rotation = 90, fontsize = 14);
133/134:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)
g.map_dataframe(sns.barplot, x="modSTATE", y="value_counts")
#plt.xticks(rotation = 90, fontsize = 14);
133/135:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="modSTATE", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/136:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION", row="modSTATE" margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/137:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION", row="modSTATE", margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/138: df1['ACCOUNT'].unique()
133/139: sns.barplot(data=df1, x="ACCOUNT", y="GPUS")
133/140:
sns.barplot(data=df1, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/141:
plot= sns.barplot(data=df1, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/142:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/143: df1['modSTATE'].value_counts()
133/144: df1.groupby(['ACCOUNT'])['modSTATE'].value_counts()
133/145: df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().retset_index(name='value_counts')
133/146: df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
133/147: plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
133/148:
g = sns.FacetGrid(plotdf2, col="PARTITION", row='modSTATE' margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/149:
g = sns.FacetGrid(plotdf2, col="PARTITION", row='modSTATE', margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/150:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()
133/151:
g = sns.FacetGrid(plotdf2, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/152:
g = sns.FacetGrid(plotdf2, row="modSTATE",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/153:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

plotdf2[['modSTATE']].drop_duplicates()
133/154:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

plotdf2[['modSTATE']].drop_duplicates()
plotdf2[['ACCOUNT']].drop_duplicates()
133/155:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

plotdf2[['modSTATE']].drop_duplicates()
plotdf2[['ACCOUNT']].drop_duplicates()
pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(), right=plotdf2[['ACCOUNT']].drop_duplicates(), how='cross')
133/156:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

pd.merge(left=plotdf2, right=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(), right=plotdf2[['ACCOUNT']].drop_duplicates(), how='cross'), how='outer')
133/157:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer')
133/158:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer').fillna(0)
133/159:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

plotdf2 = pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer').fillna(0)
133/160:
g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
133/161:
g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=8)
133/162:
g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, sharey=False)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=8)
133/163:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

plotdf2 = pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer').fillna(0)
plotdf2.shape
#plotdf2.loc[~plotdf2['modSTATE'].isin(['RUNNING','PENDING'])]
133/164:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

plotdf2 = pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer').fillna(0)
plotdf2.shape #(486, 4)
plotdf2.loc[~plotdf2['modSTATE'].isin(['RUNNING','PENDING'])]
133/165:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

plotdf2 = pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer').fillna(0)
plotdf2.shape #(486, 4)
plotdf2 = plotdf2.loc[~plotdf2['modSTATE'].isin(['RUNNING','PENDING'])]
133/166:
g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, sharey=False)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=8)
133/167:
g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, sharey=False)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=6)
133/168:
g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, sharey=False)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=8)
133/169:
g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, sharey=False)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts", hue="ACCOUNT")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=7)
133/170:
g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, sharey=False)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=7)
133/171:
plt.figure(figsize = (10,5))
g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, sharey=False)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=7)
133/172:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, sharey=False, height=5)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=7)
133/173:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=5, aspect=0.5)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=8)
133/174:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=5, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=7)
133/175:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=1, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=7)
133/176:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=7)
133/177:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=60, fontsize=8)
133/178:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=10)
133/179:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=12)
133/180:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=14)
133/181:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
133/182:
plot= sns.barplot(data=df1, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
133/183: np.percentile(df1, [75 ,25])
133/184:
import numpy as np
np.percentile(df1, [75 ,25])
133/185:
import numpy as np
np.percentile(df1['NODES'], [75 ,25])
133/186: sns.pairplot(plotdf2)
133/187: sns.pairplot(df1)
133/188:
#plt.hist(x)
#plt.show()


x = df1["GPUS"]

plt.title("Distribution of GPUS accross all the jobs and partitions")
plt.xlabel("GPUS")
plt.ylabel("GPUS")

plt.hist(x)

plt.grid()

plt.show()
133/189: x = df1['GPUS']
133/190: df1.head()
134/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df1 = pd.read_csv('./data/accre_job.csv')
#df4_SC = pd.read_csv('./data/Completed_jobs.csv')
134/2: df1.head()
134/3: df1.describe()
134/4: df1.info()
134/5: df1['STATE'].value_counts()
134/6:
df1['modSTATE'] = df1['STATE']
df1.head()
134/7:
df1['modSTATE']=df1['modSTATE'].str.split(' by', expand = True)[0]
df1['modSTATE'].value_counts()
#df1['modSTATE'].value_counts()
134/8: df1['NODES'].describe().plot(kind=)
134/9: df1['NODES'].describe()
134/10:
#what is the percentage distribution of the three partitions.
#what is the percentage distribution of the users across partitions
#what is the percentage distribution of the jobs STATE in data
#what is the percntage memmorry, GPU, USEDTIME REQTIME and timeestimate in data per partition and peruser.
134/11: df1['ACCOUNT'].nunique()
134/12:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
per_acc
134/13:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
134/14:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
per_partition
134/15:
plt.figure(figsize = (8,5))
user_Count = df1.groupby(['ACCOUNT'])['USER'].nunique()
user_Count.plot(kind='bar', title='Number of total users per account', ylabel='Number of users')
plt.xticks(rotation = 90, fontsize = 14);
user_Count
134/16:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
134/17: #user_account
134/18:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values().plot(kind='bar', title = 'Number total ACCOUNTS per PARTITION', ylabel='# of ACCOUNT', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
134/19:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['USER'].nunique().sort_values().plot(kind='bar', title = 'Number total USERS per PARTITION', ylabel='# of USER', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
134/20:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
134/21:
plt.figure(figsize = (10,5))
max_acc_count=df1.loc[df1['PARTITION'] == 'maxwell']['ACCOUNT'].value_counts()/len(df1.axes[0])*100

max_acc_count.plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
134/22: (df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
134/23: (df1.groupby(df1['PARTITION'])['USED_SEC'].agg('sum')/df1['USED_SEC'].sum())*100
134/24: (df1.groupby(df1['PARTITION'])['REQ_SEC'].agg('sum')/df1['REQ_SEC'].sum())*100
134/25:
#What percentage of total memorry is used by each partition
(df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
134/26:
#What is the percentage of total GPU usgae by each partition
(df1.groupby(df1['PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
134/27: (df1.groupby(['PARTITION', 'ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
134/28: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts()/len(df1.axes[0])*100)
134/29:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution Job status per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
134/30: (df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
134/31: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].describe()
134/32: plotdf1= df1.groupby(['PARTITION'])['modSTATE'].value_counts().reset_index(name='value_counts')
134/33:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="modSTATE", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
134/34: df1['modSTATE'].value_counts()
134/35:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
134/36: df1['ACCOUNT'].unique()
134/37:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

plotdf2 = pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer').fillna(0)
plotdf2.shape #(486, 4)
plotdf2 = plotdf2.loc[~plotdf2['modSTATE'].isin(['RUNNING','PENDING'])]
134/38:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
134/39:
plot= sns.barplot(data=df1, x="ACCOUNT", y="GPUS")
#for axes in plot.axes.flat:
 #   _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
134/40: plt.hist(df1['GPUS'], bins=9)
134/41: plt.hist(df1['GPUS'], bins=[0,0.05, 0.1, 0.5, 1.0, 5.0, 10, 20, 40, 60]))
134/42: plt.hist(df1['GPUS'], bins=[0,0.05, 0.1, 0.5, 1.0, 5.0, 10, 20, 40, 60])
134/43: plt.hist(df1['MEM'], bins=[0,0.05, 0.1, 0.5, 1.0, 5.0, 10, 20, 40, 60])
134/44: plt.hist(df1['MEM']/1000, bins=[0,0.05, 0.1, 0.5, 1.0, 5.0, 10, 20, 40, 60])
134/45: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].describe().boxplot()
134/46: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].describe()
134/47: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('mean')
134/48: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('mean').boxplot()
134/49: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('mean').plot()
134/50: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('mean').plot(kind='box')
134/51: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('mean').plot()
134/52: plt.hist(df1['MEM']len(df1.axes[0])*100, bins=[0,0.05, 0.1, 0.5, 1.0, 5.0, 10, 20, 40, 60])
134/53: plt.hist(df1['MEM']/len(df1.axes[0])*100, bins=[0,0.05, 0.1, 0.5, 1.0, 5.0, 10, 20, 40, 60])
134/54: plt.hist(df1['MEM']/df1['MEM'].sum())*100, bins=[0,0.05, 0.1, 0.5, 1.0, 5.0, 10, 20, 40, 60])
134/55: plt.hist(df1['MEM']/(df1['MEM'].sum())*100, bins=[0,0.05, 0.1, 0.5, 1.0, 5.0, 10, 20, 40, 60])
134/56: plt.hist((df1['MEM']/df1['MEM'].sum())*100, bins=[0,0.05, 0.1, 0.5, 1.0, 5.0, 10, 20, 40, 60])
134/57: plt.hist((df1['USED_SEC'])
134/58: plt.hist(df1['USED_SEC'])
134/59: sns.stripplot(data = df1, x = 'GPUS', y = 'MEM');
134/60:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 3.5, xmin = xmin, xmax = xmax, linestyle = '--', label = 'EPCA Requirement')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/61:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
#plt.hlines(y = 3.5, xmin = xmin, xmax = xmax, linestyle = '--', label = 'EPCA Requirement')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/62:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
#plt.hlines(y = 3.5, xmin = xmin, xmax = xmax, linestyle = '--', label = 'EPCA Requirement')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/63:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/64:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df1 = pd.read_csv('./data/accre_job.csv')
df4_SC = pd.read_csv('./data/Completed_jobs.csv')
134/65:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/66: df4_SC['TIME_ESTIMATE'].describe()
134/67: df4_SC['TIME_ESTIMATE']=df4_SC['TIME_ESTIMATE']/86400
134/68:
df4_SC['TIME_ESTIMATE']=df4_SC['TIME_ESTIMATE']/86400
df4_SC["TIME_ESTIMATE"].describe()
134/69:
df4_SC['TIME_ESTIMATE']=df4_SC['TIME_ESTIMATE']/3600
df4_SC["TIME_ESTIMATE"].describe()
134/70:
df4_SC['TIME_ESTIMATE']=df4_SC['TIME_ESTIMATE']/86400
df4_SC["TIME_ESTIMATE"].describe()
134/71:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/72:
df4_SC['TIME_ESTIMATE']=df4_SC['TIME_ESTIMATE']/3600
df4_SC["TIME_ESTIMATE"].describe()
134/73:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Hours', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/74:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Hours', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/75:
df4_SC['TIME_ESTIMATE']=df4_SC['TIME_ESTIMATE']/60
df4_SC["TIME_ESTIMATE"].describe()
134/76:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Hours', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/77:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Minutes', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/78:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Minutes', fontsize = fontsize)
plt.title('Time estimate for completed jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/79: df4_SC.groupby(['PARTITION', 'ACCOUNT'])["TIME_ESTIMATE"] <0.value_counts
134/80: (df4_SC.groupby(['PARTITION', 'ACCOUNT'])["TIME_ESTIMATE"] <0).value_counts
134/81: (df4_SC.groupby(['PARTITION', 'ACCOUNT'])["TIME_ESTIMATE"] <0).value_counts()
134/82: [df4_SC.groupby(['PARTITION', 'ACCOUNT'])["TIME_ESTIMATE"] <0].value_counts()
134/83:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT',
              y = 'MEM')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Megabytes', fontsize = fontsize)
plt.title('Time estimate for completed jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/84:
fontsize = 16
mem_per_GPU = df1['MEM']/df1['GPUS']
plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT',
              y = mem_per_GPU)
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Megabytes/GPUS', fontsize = fontsize)
plt.title('Memmory per GPU usage by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/85:
fontsize = 16
mem_per_GPU = ln(df1['MEM']/df1['GPUS'])
plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT',
              y = mem_per_GPU)
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Megabytes/GPUS', fontsize = fontsize)
plt.title('Memmory per GPU usage by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/86:
fontsize = 16
mem_per_GPU = (df1['MEM']/df1['GPUS'])
plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT',
              y = mem_per_GPU)
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Megabytes/GPUS', fontsize = fontsize)
plt.title('Memmory per GPU usage by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/87:
fontsize = 16
mem_per_GPU = (df1['MEM']/df1['GPUS'])/10000
plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT',
              y = mem_per_GPU)
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Megabytes/GPUS', fontsize = fontsize)
plt.title('Memmory per GPU usage by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
134/88:
fontsize = 16
mem_per_GPU = (df1['MEM']/df1['GPUS'])/1000000
plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT',
              y = mem_per_GPU)
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Gigabytes/GPUS', fontsize = fontsize)
plt.title('Memmory per GPU usage by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
137/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df1 = pd.read_csv('./data/accre_job.csv')
df4_SC = pd.read_csv('./data/Completed_jobs.csv')
137/2: df1.head()
137/3: df1.describe()
137/4: df1.info()
137/5: df1['STATE'].value_counts()
137/6:
df1['modSTATE'] = df1['STATE']
df1.head()
137/7:
df1['modSTATE']=df1['modSTATE'].str.split(' by', expand = True)[0]
df1['modSTATE'].value_counts()
#df1['modSTATE'].value_counts()
137/8: df1['NODES'].describe()
137/9:
#what is the percentage distribution of the three partitions.
#what is the percentage distribution of the users across partitions
#what is the percentage distribution of the jobs STATE in data
#what is the percntage memmorry, GPU, USEDTIME REQTIME and timeestimate in data per partition and peruser.
137/10: df1['ACCOUNT'].nunique()
137/11:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
per_acc
137/12:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
137/13:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
per_partition
137/14:
plt.figure(figsize = (8,5))
user_Count = df1.groupby(['ACCOUNT'])['USER'].nunique()
user_Count.plot(kind='bar', title='Number of total users per account', ylabel='Number of users')
plt.xticks(rotation = 90, fontsize = 14);
user_Count
137/15:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
137/16: #user_account
137/17:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values().plot(kind='bar', title = 'Number total ACCOUNTS per PARTITION', ylabel='# of ACCOUNT', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
137/18:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['USER'].nunique().sort_values().plot(kind='bar', title = 'Number total USERS per PARTITION', ylabel='# of USER', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
137/19:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
137/20:
plt.figure(figsize = (10,5))
max_acc_count=df1.loc[df1['PARTITION'] == 'maxwell']['ACCOUNT'].value_counts()/len(df1.axes[0])*100

max_acc_count.plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
137/21: (df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
137/22: (df1.groupby(df1['PARTITION'])['USED_SEC'].agg('sum')/df1['USED_SEC'].sum())*100
137/23: (df1.groupby(df1['PARTITION'])['REQ_SEC'].agg('sum')/df1['REQ_SEC'].sum())*100
137/24:
#What percentage of total memorry is used by each partition
(df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
137/25:
#What is the percentage of total GPU usgae by each partition
(df1.groupby(df1['PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
137/26: (df1.groupby(['PARTITION', 'ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
137/27: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts()/len(df1.axes[0])*100)
137/28:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution Job status per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
137/29: (df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
137/30: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('mean').plot()
137/31: plotdf1= df1.groupby(['PARTITION'])['modSTATE'].value_counts().reset_index(name='value_counts')
137/32:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="modSTATE", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
137/33: df1['modSTATE'].value_counts()
137/34:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
137/35: df1['ACCOUNT'].unique()
137/36:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

plotdf2 = pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer').fillna(0)
plotdf2.shape #(486, 4)
plotdf2 = plotdf2.loc[~plotdf2['modSTATE'].isin(['RUNNING','PENDING'])]
137/37:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
137/38:
plot= sns.barplot(data=df1, x="ACCOUNT", y="GPUS")
#for axes in plot.axes.flat:
 #   _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
137/39: plt.hist(df1['USED_SEC'])
137/40: # = df1['GPUS']
137/41: df1.head()
137/42: sns.stripplot(data = df1, x = 'GPUS', y = 'MEM');
137/43:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
137/44:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
137/45:
df4_SC['TIME_ESTIMATE']=df4_SC['TIME_ESTIMATE']/60
df4_SC["TIME_ESTIMATE"].describe()
137/46:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Minutes', fontsize = fontsize)
plt.title('Time estimate for completed jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
137/47:
fontsize = 16
mem_per_GPU = (df1['MEM']/df1['GPUS'])/1000000
plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT',
              y = mem_per_GPU)
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Gigabytes/GPUS', fontsize = fontsize)
plt.title('Memmory per GPU usage by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
137/48: df1['modSTATE'].value_counts()
137/49: df1.groupby(['PARTITION'])['modSTATE'].value_counts()
137/50: df1.groupby(['PARTITION'])['modSTATE'].value_counts()/len(df1.axes[0])*100
137/51: df1['modSTATE'].value_counts()/len(df1.axes[0])*100
137/52: df1.groupby(['modSTATE'])['PARTITION'].value_counts()/len(df1.axes[0])*100
137/53: df1.groupby(['modSTATE','PARTITION'])['USED_SEC'].value_counts()/len(df1.axes[0])*100
137/54: df1.groupby(['modSTATE','PARTITION'])['GPUS'].value_counts()/len(df1.axes[0])*100
137/55: df1.groupby(['modSTATE','PARTITION'])['GPUS'].value_counts()/len(df1.axes[0])*100.plot()
137/56: (df1.groupby(['modSTATE','PARTITION'])['GPUS'].value_counts()/len(df1.axes[0])*100).plot()
137/57: df1.groupby(['modSTATE','PARTITION'])['GPUS'].value_counts()/len(df1.axes[0])*100
137/58: (df1.groupby(['PARTITION', 'modSTATE'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100
137/59: (df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100
137/60: (df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100.reset_index(name='%GPUS')
137/61: ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
137/62: df1.groupby(['modSTATE'])['PARTITION'].value_counts()/len(df1.axes[0])*100).reset_index(name='Jobs_per_partition')
137/63: (df1.groupby(['modSTATE'])['PARTITION'].value_counts()/len(df1.axes[0])*100).reset_index(name='Jobs_per_partition')
137/64: pdf = (df1.groupby(['modSTATE'])['PARTITION'].value_counts()/len(df1.axes[0])*100).reset_index(name='Jobs_per_partition')
137/65: gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
137/66: pd.merge(left = pdf1, right = gpudf, how=inner)
137/67: pd.merge(left = pdf, right = gpudf, how=inner)
137/68: pd.merge(left = pdf, right = gpudf, how='outer')
137/69:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf1.head()
137/70:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()
137/71:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
137/72:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
memdf.head()
137/73: per = pd.merge(left = pdf, right = gpudf, how='outer')
137/74: gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
137/75:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
137/76:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem.head()
137/77:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
memdf.head()

timest = ((df1.groupby(['modSTATE', 'PARTITION'])['TIME_ESTIMATE'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='%GPUS')
137/78:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
memdf.head()

timest = ((df1.groupby(['modSTATE', 'PARTITION'])['TIME_ESTIMATE'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='%GPUS')

timeest.head()
137/79:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
memdf.head()

timest = ((df1.groupby(['modSTATE', 'PARTITION'])['TIME_ESTIMATE'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='%GPUS')

timest.head()
137/80:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_time = pd.merge(left =gpu_par_mem, right = timest, how='outer')
gpu_par_mem_time.head()
137/81:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
memdf.head()

timest = ((df1.groupby(['modSTATE', 'PARTITION'])['TIME_ESTIMATE'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='timest')

timest.head()
137/82:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_time = pd.merge(left =gpu_par_mem, right = timest, how='outer')
gpu_par_mem_time.head()
137/83:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
memdf.head()

timest = ((df1.groupby(['modSTATE', 'PARTITION'])['TIME_ESTIMATE'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='timest')
timest.head()

timest = ((df1.groupby(['modSTATE', 'PARTITION'])['USED_SEC'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='usedtime')
timest.head()
137/84:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
memdf.head()

timest = ((df1.groupby(['modSTATE', 'PARTITION'])['TIME_ESTIMATE'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='timest')
timest.head()

usedsec = ((df1.groupby(['modSTATE', 'PARTITION'])['USED_SEC'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='usedtime')
usedsec.head()
137/85:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
memdf.head()

timest = ((df1.groupby(['modSTATE', 'PARTITION'])['TIME_ESTIMATE'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='timest')
timest.head()

usedsec = ((df1.groupby(['modSTATE', 'PARTITION'])['USED_SEC'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='usedtime')
usedsec.head()

reqsec = ((df1.groupby(['modSTATE', 'PARTITION'])['REQ_SEC'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='reqtime')
reqsec.head()
137/86:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_reqsec = pd.merge(left =gpu_par_mem, right = reqsec how='outer')
gpu_par_mem_reqsec.head()
137/87:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_reqsec = pd.merge(left =gpu_par_mem, right = reqsec, how='outer')
gpu_par_mem_reqsec.head()
137/88:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_reqsec = pd.merge(left =gpu_par_mem, right = reqsec, how='outer')
gpu_par_mem_reqsec_usedsec = pd.merge(left=gpu_par_mem_reqsec, right=usedsec, how='outer')

gpu_par_mem_reqsec_usedsec.head()
137/89:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_reqsec = pd.merge(left =gpu_par_mem, right = reqsec, how='outer')
gpu_par_mem_reqsec_usedsec = pd.merge(left=gpu_par_mem_reqsec, right=usedsec, how='outer')
gpu_par_mem_reqsec_usedsec_timest = pd.merge (left=gpu_par_mem_reqsec_usedsec, right=timest, how='outer')
gpu_par_mem_reqsec_usedsec_timest.head()
137/90:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_reqsec = pd.merge(left =gpu_par_mem, right = reqsec, how='outer')
gpu_par_mem_reqsec_usedsec = pd.merge(left=gpu_par_mem_reqsec, right=usedsec, how='outer')
gpu_par_mem_reqsec_usedsec_timest = pd.merge (left=gpu_par_mem_reqsec_usedsec, right=timest, how='outer')
gpu_par_mem_reqsec_usedsec_timest.shape()
137/91:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_reqsec = pd.merge(left =gpu_par_mem, right = reqsec, how='outer')
gpu_par_mem_reqsec_usedsec = pd.merge(left=gpu_par_mem_reqsec, right=usedsec, how='outer')
gpu_par_mem_reqsec_usedsec_timest = pd.merge (left=gpu_par_mem_reqsec_usedsec, right=timest, how='outer')
gpu_par_mem_reqsec_usedsec_timest.shape
137/92:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_reqsec = pd.merge(left =gpu_par_mem, right = reqsec, how='outer')
gpu_par_mem_reqsec_usedsec = pd.merge(left=gpu_par_mem_reqsec, right=usedsec, how='outer')
gpu_par_mem_reqsec_usedsec_timest = pd.merge (left=gpu_par_mem_reqsec_usedsec, right=timest, how='outer')
gpu_par_mem_reqsec_usedsec_timest.head(20)
137/93: gpu_par_mem_reqsec_usedsec_timest.iloc[:-6]
137/94: mer_per_df = gpu_par_mem_reqsec_usedsec_timest.iloc[:-6]
137/95:
mer_per_df = gpu_par_mem_reqsec_usedsec_timest.iloc[:-6]
mer_per_df.head(10)
137/96: plt.hist(mer_per_df)
137/97:
pdf = (df1.groupby(['modSTATE'])['PARTITION'].value_counts()/len(df1.axes[0])*100).reset_index(name='Jobs_per_partition')

accdf = (df1.groupby(['modSTATE','PARTITION']['ACCOUNT'].value_counts()/len(df1.axes[0])*100).reset_index(name='Jobs_per_partition')
137/98:
pdf = (df1.groupby(['modSTATE'])['PARTITION'].value_counts()/len(df1.axes[0])*100).reset_index(name='Jobs_per_partition')

accdf = (df1.groupby(['modSTATE','PARTITION']['ACCOUNT'].value_counts()/len(df1.axes[0])*100).reset_index(name='account')
137/99:
pdf = (df1.groupby(['modSTATE'])['PARTITION'].value_counts()/len(df1.axes[0])*100).reset_index(name='Jobs_per_partition')

accdf = (df1.groupby(['modSTATE','PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).
         reset_index(name='account')
137/100:
pdf = (df1.groupby(['modSTATE'])['PARTITION'].value_counts()/len(df1.axes[0])*100).reset_index(name='Jobs_per_partition')

(df1.groupby(['modSTATE','PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).reset_index(name='account')
137/101: sns.pairplot(mer_per_df)
137/102: sns.pairplot(mer_per_df, hue="PARTITION")
137/103: sns.pairplot(mer_per_df, hue="PARTITION", diag_kind="hist");
137/104: sns.pairplot(mer_per_df, hue="PARTITION");
137/105: sns.pairplot(mer_per_df, hue="modSTATE");
137/106: sns.pairplot(mer_per_df, hue="PARTITION");
137/107: sns.displot(data=mer_per_df, x="Jobs_per_partition", kde=True)
137/108: sns.violinplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
137/109: sns.boxplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
137/110: sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
137/111:
ax = sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
ax.bar_label(ax.containers[0])
137/112:
ax = sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
ax.bar_label(ax.containers)
137/113:
sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
#ax.bar_label(ax.containers)
137/114:
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
137/115:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
    for axes in ax.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
137/116:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
    for axes in ax.axes.flat:
        _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
137/117:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
for axes in ax.axes.flat:
     _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
137/118:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
for axes in plot.axes.flat:
     _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
137/119:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
137/120:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(fontsize = 16, rotation=90)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('% of Time Estimated for total job in each partition', fontsize = 16, fontweight = 'bold')
137/121:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION")
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('% of Time Estimated for total job in each partition', fontsize = 16, fontweight = 'bold')
137/122:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION");
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('% of Time Estimated for total job in each partition', fontsize = 16, fontweight = 'bold')
139/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df1 = pd.read_csv('./data/accre_job.csv')
#df4_SC = pd.read_csv('./data/Completed_jobs.csv')
139/2: df1.head()
139/3: df1.describe()
139/4: df1.info()
139/5: df1['STATE'].value_counts()
139/6:
df1['modSTATE'] = df1['STATE']
df1.head()
139/7:
df1['modSTATE']=df1['modSTATE'].str.split(' by', expand = True)[0]
df1['modSTATE'].value_counts()
#df1['modSTATE'].value_counts()
139/8: df1['NODES'].describe()
139/9:
#what is the percentage distribution of the three partitions.
#what is the percentage distribution of the users across partitions
#what is the percentage distribution of the jobs STATE in data
#what is the percntage memmorry, GPU, USEDTIME REQTIME and timeestimate in data per partition and peruser.
139/10: df1['ACCOUNT'].nunique()
139/11:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
per_acc
139/12:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
139/13:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
per_partition
139/14:
plt.figure(figsize = (8,5))
user_Count = df1.groupby(['ACCOUNT'])['USER'].nunique()
user_Count.plot(kind='bar', title='Number of total users per account', ylabel='Number of users')
plt.xticks(rotation = 90, fontsize = 14);
user_Count
139/15:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
139/16: #user_account
139/17:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values().plot(kind='bar', title = 'Number total ACCOUNTS per PARTITION', ylabel='# of ACCOUNT', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
139/18:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['USER'].nunique().sort_values().plot(kind='bar', title = 'Number total USERS per PARTITION', ylabel='# of USER', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
139/19:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
139/20:
plt.figure(figsize = (10,5))
max_acc_count=df1.loc[df1['PARTITION'] == 'maxwell']['ACCOUNT'].value_counts()/len(df1.axes[0])*100

max_acc_count.plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
139/21: (df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
139/22: (df1.groupby(df1['PARTITION'])['USED_SEC'].agg('sum')/df1['USED_SEC'].sum())*100
139/23: (df1.groupby(df1['PARTITION'])['REQ_SEC'].agg('sum')/df1['REQ_SEC'].sum())*100
139/24:
#What percentage of total memorry is used by each partition
(df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
139/25:
#What is the percentage of total GPU usgae by each partition
(df1.groupby(df1['PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
139/26: (df1.groupby(['PARTITION', 'ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
139/27: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts()/len(df1.axes[0])*100)
139/28:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution Job status per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
139/29: (df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
139/30: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('mean').plot()
139/31: plotdf1= df1.groupby(['PARTITION'])['modSTATE'].value_counts().reset_index(name='value_counts')
139/32:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="modSTATE", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
139/33: df1['modSTATE'].value_counts()
139/34:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
139/35: df1['ACCOUNT'].unique()
139/36:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

plotdf2 = pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer').fillna(0)
plotdf2.shape #(486, 4)
plotdf2 = plotdf2.loc[~plotdf2['modSTATE'].isin(['RUNNING','PENDING'])]
139/37:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
139/38:
plot= sns.barplot(data=df1, x="ACCOUNT", y="GPUS")
#for axes in plot.axes.flat:
 #   _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
139/39: plt.hist(df1['USED_SEC'])
139/40: # = df1['GPUS']
139/41: df1.head()
139/42: df1['NODES'].describe().plot(kind='box')
139/43:
df1['modSTATE']=df1['modSTATE'].str.split(' by', expand = True)[0]
(df1['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = 'Status of Jobs in data', ylabel='Percentage of total JOBS', xlabel='JOB STATE')
plt.xticks(rotation = 90, fontsize = 14);
#per_acc
#df1['modSTATE'].value_counts()
142/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df1 = pd.read_csv('./data/accre_job.csv')
df4_SC = pd.read_csv('./data/Completed_jobs.csv')
142/2: df1.head()
142/3: df1.describe()
142/4: df1.info()
142/5: df1['STATE'].value_counts()
142/6:
df1['modSTATE'] = df1['STATE']
df1.head()
142/7:
df1['modSTATE']=df1['modSTATE'].str.split(' by', expand = True)[0]
df1['modSTATE'].value_counts()
#df1['modSTATE'].value_counts()
142/8: df1['NODES'].describe()
142/9:
#what is the percentage distribution of the three partitions.
#what is the percentage distribution of the users across partitions
#what is the percentage distribution of the jobs STATE in data
#what is the percntage memmorry, GPU, USEDTIME REQTIME and timeestimate in data per partition and peruser.
142/10: df1['ACCOUNT'].nunique()
142/11:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
per_acc
142/12:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
142/13:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
per_partition
142/14:
plt.figure(figsize = (8,5))
user_Count = df1.groupby(['ACCOUNT'])['USER'].nunique()
user_Count.plot(kind='bar', title='Number of total users per account', ylabel='Number of users')
plt.xticks(rotation = 90, fontsize = 14);
user_Count
142/15:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
142/16: #user_account
142/17:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values().plot(kind='bar', title = 'Number total ACCOUNTS per PARTITION', ylabel='# of ACCOUNT', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
142/18:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['USER'].nunique().sort_values().plot(kind='bar', title = 'Number total USERS per PARTITION', ylabel='# of USER', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
142/19:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
142/20:
plt.figure(figsize = (10,5))
max_acc_count=df1.loc[df1['PARTITION'] == 'maxwell']['ACCOUNT'].value_counts()/len(df1.axes[0])*100

max_acc_count.plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
142/21: (df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
142/22: (df1.groupby(df1['PARTITION'])['USED_SEC'].agg('sum')/df1['USED_SEC'].sum())*100
142/23: (df1.groupby(df1['PARTITION'])['REQ_SEC'].agg('sum')/df1['REQ_SEC'].sum())*100
142/24:
#What percentage of total memorry is used by each partition
(df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
142/25:
#What is the percentage of total GPU usgae by each partition
(df1.groupby(df1['PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
142/26: (df1.groupby(['PARTITION', 'ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
142/27: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts()/len(df1.axes[0])*100)
142/28:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution Job status per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
142/29: (df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
142/30: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('mean').plot()
142/31: plotdf1= df1.groupby(['PARTITION'])['modSTATE'].value_counts().reset_index(name='value_counts')
142/32:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="modSTATE", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
142/33: df1['modSTATE'].value_counts()
142/34:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
142/35: df1['ACCOUNT'].unique()
142/36:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

plotdf2 = pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer').fillna(0)
plotdf2.shape #(486, 4)
plotdf2 = plotdf2.loc[~plotdf2['modSTATE'].isin(['RUNNING','PENDING'])]
142/37:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
142/38:
plot= sns.barplot(data=df1, x="ACCOUNT", y="GPUS")
#for axes in plot.axes.flat:
 #   _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
142/39: plt.hist(df1['USED_SEC'])
142/40: # = df1['GPUS']
142/41: df1.head()
142/42: sns.stripplot(data = df1, x = 'GPUS', y = 'MEM');
142/43:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
142/44:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
142/45:
df4_SC['TIME_ESTIMATE']=df4_SC['TIME_ESTIMATE']/60
df4_SC["TIME_ESTIMATE"].describe()
142/46:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Minutes', fontsize = fontsize)
plt.title('Time estimate for completed jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
142/47:
fontsize = 16
mem_per_GPU = (df1['MEM']/df1['GPUS'])/1000000
plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT',
              y = mem_per_GPU)
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Gigabytes/GPUS', fontsize = fontsize)
plt.title('Memmory per GPU usage by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
142/48: df1['modSTATE'].value_counts()/len(df1.axes[0])*100
142/49:
pdf = (df1.groupby(['modSTATE'])['PARTITION'].value_counts()/len(df1.axes[0])*100).reset_index(name='Jobs_per_partition')

(df1.groupby(['modSTATE','PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).reset_index(name='account')
142/50:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
memdf.head()

timest = ((df1.groupby(['modSTATE', 'PARTITION'])['TIME_ESTIMATE'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='timest')
timest.head()

usedsec = ((df1.groupby(['modSTATE', 'PARTITION'])['USED_SEC'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='usedtime')
usedsec.head()

reqsec = ((df1.groupby(['modSTATE', 'PARTITION'])['REQ_SEC'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='reqtime')
reqsec.head()
142/51:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_reqsec = pd.merge(left =gpu_par_mem, right = reqsec, how='outer')
gpu_par_mem_reqsec_usedsec = pd.merge(left=gpu_par_mem_reqsec, right=usedsec, how='outer')
gpu_par_mem_reqsec_usedsec_timest = pd.merge (left=gpu_par_mem_reqsec_usedsec, right=timest, how='outer')
gpu_par_mem_reqsec_usedsec_timest.head(20)
142/52:
mer_per_df = gpu_par_mem_reqsec_usedsec_timest.iloc[:-6]
mer_per_df.head(10)
142/53: sns.pairplot(mer_per_df, hue="PARTITION");
142/54:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION");
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('% of Time Estimated for total job in each partition', fontsize = 16, fontweight = 'bold')
142/55:
(df1['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = 'Status of Jobs in data', ylabel='Percentage of total JOBS', xlabel='JOB STATE')
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(rotation = 90, fontsize = 14);
142/56:
ax = (df1['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = 'Status of Jobs in data', ylabel='Percentage of total JOBS', xlabel='JOB STATE')
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(rotation = 90, fontsize = 14);
142/57:
plt.subplots(figsize = (14, 6))
ax = (df1['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = 'Status of Jobs in data', ylabel='Percentage of total JOBS', xlabel='JOB STATE')
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(rotation = 90, fontsize = 14);
142/58:
plt.subplots(figsize = (14, 6))
ax = (df1['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = 'Status of Jobs in data', ylabel='Percentage of total JOBS', xlabel='JOB STATE')
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(rotation = 90, fontsize = 22);
142/59:
plt.subplots(figsize = (14, 6))
ax = (df1['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = 'Status of Jobs in data', ylabel='Percentage of total JOBS', xlabel='JOB STATE')
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(rotation = 90, fontsize = 14);
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('Percentage of total JOBS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('Status of the Jobs representated as % of total job in each partition', fontsize = 16, fontweight = 'bold')
142/60:
plt.subplots(figsize = (14, 6))
ax = (df1['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = 'Status of Jobs in data', ylabel='Percentage of total JOBS', xlabel='JOB STATE')
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(rotation = 90, fontsize = 14);
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('Percentage of total JOBS', fontsize = 16)
plt.title('Status of the Jobs representated as % of total job in each partition', fontsize = 16, fontweight = 'bold')
142/61: df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')
142/62: df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/360
142/63: df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')
142/64: df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC'])
142/65: (df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100
142/66:
(df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100.reset_index(name='reqtime')
reqsec.head()
142/67: ((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100).reset_index(name='reqtime')
142/68: req1=((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100).reset_index(name='Used_Sec%')
142/69:
req1=((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100).reset_index(name='Used_Sec%')
req1
142/70:
use1=((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100).reset_index(name='Used_Sec%')
req1
142/71:
use1=((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100).reset_index(name='Used_Sec%')
use1
142/72:
use1=((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100)
    .reset_index(name='Used_Sec%')
142/73:
use1=((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100).reset_index(name='Used_Sec%')
use1

use1=((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100).reset_index(name='Used_Sec%')
use1
142/74:
use1=((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100).reset_index(name='Used_Sec%')
use1

req1=((df1.groupby(['modSTATE'])['REQ_SEC'].agg('sum')/sum(df1['REQ_SEC']))*100).reset_index(name='Req_Sec%')
req1
142/75:
use1=((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100).reset_index(name='Used_Sec%')
use1

req1=((df1.groupby(['modSTATE'])['REQ_SEC'].agg('sum')/sum(df1['REQ_SEC']))*100).reset_index(name='Req_Sec%')
req1


pd.merge(req1, use1, how='outer')
142/76: df1.columns
142/77: df1[['TIME_ESTIMATE']==0][modSTATE].value_counts()
142/78: df1[df1['TIME_ESTIMATE']==0][modSTATE].value_counts()
142/79: df1[df1['TIME_ESTIMATE']==0]['modSTATE'].value_counts()
142/80: df1[df1['TIME_ESTIMATE']>0]['modSTATE'].value_counts()
142/81: df1[df1['TIME_ESTIMATE']<0]['modSTATE'].value_counts()
142/82: df1[df1['TIME_ESTIMATE']==0]['modSTATE'].value_counts().reset_index(name='Exact Estimator')
142/83: oves= df1[df1['TIME_ESTIMATE']>0]['modSTATE'].value_counts().reset_index(name="Over Estimator")
142/84: exes= df1[df1['TIME_ESTIMATE']==0]['modSTATE'].value_counts().reset_index(name='Exact Estimator')
142/85: unes=df1[df1['TIME_ESTIMATE']<0]['modSTATE'].value_counts().reset_index(name="Under Estimator")
142/86: pd.merge(exes, oves, how='outer')
142/87: exov = pd.merge(exes, oves, how='outer')
142/88:
exov = pd.merge(exes, oves, how='outer')
exonvunex = pd.merge(exov, unes, how='outer')
142/89:
exov = pd.merge(exes, oves, how='outer')
exonvunex = pd.merge(exov, unes, how='outer')
exonvunex
142/90:
exov = pd.merge(exes, oves, how='outer')
exonvunex = pd.merge(exov, unes, how='outer').fillna()
exonvunex
142/91:
exov = pd.merge(exes, oves, how='outer')
exonvunex = pd.merge(exov, unes, how='outer').fillna
exonvunex
142/92:
exov = pd.merge(exes, oves, how='outer')
exonvunex = pd.merge(exov, unes, how='outer')
142/93:
exov = pd.merge(exes, oves, how='outer')
exovunex = pd.merge(exov, unes, how='outer').fillna(0)
exovunex
142/94:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=exovunex, x="index", y=["Exact Estimator", "Over Estimator", "Under Estimatot"], hue="index");
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('% of Time Estimated for total job in each partition', fontsize = 16, fontweight = 'bold')
142/95:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=exovunex, x="index", y="Exact Estimator", hue="index");
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('% of Time Estimated for total job in each partition', fontsize = 16, fontweight = 'bold')
142/96:
plt.subplots(figsize = (14, 6))
ax=exovunex.plot( x="index", y=["Exact Estimator", "Over Estimator", "Under Estimator"], hue="index");
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('% of Time Estimated for total job in each partition', fontsize = 16, fontweight = 'bold')
142/97: exovunex.plot( x="index", y=["Exact Estimator", "Over Estimator", "Under Estimator"], kind="bar")
142/98:
plt.subplots(figsize = (14, 6))
ax=exovunex.plot( x="index", y=["Exact Estimator", "Over Estimator", "Under Estimator"], kind="bar");
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('% of Time Estimated for total job in each partition', fontsize = 16, fontweight = 'bold')
142/99:
exes= df1[df1['TIME_ESTIMATE']==0]['modSTATE'].value_counts().reset_index(name='Exact Estimator')
df1[df1['TIME_ESTIMATE']==0]['ACCOUNT'].value_counts().reset_index(name='Exact Estimator Account')
142/100:
oves= df1[df1['TIME_ESTIMATE']>0]['modSTATE'].value_counts().reset_index(name="Over Estimator")
df1[df1['TIME_ESTIMATE']>0]['ACCOUNT'].value_counts().reset_index(name='Exact Estimator Account')
142/101:
unes=df1[df1['TIME_ESTIMATE']<0]['modSTATE'].value_counts().reset_index(name="Under Estimator")
df1[df1['TIME_ESTIMATE']<0]['ACCOUNT'].value_counts().reset_index(name="Under Estimator")
142/102:
unes=df1[df1['TIME_ESTIMATE']<0]['modSTATE'].value_counts().reset_index(name="Under Estimator")
df1[df1['TIME_ESTIMATE']<0][df1["modSTATE"]=="COMPLETED"]['ACCOUNT'].value_counts().reset_index(name="Under Estimator")
#glasshouse(1714), virgini(1139),prince(669)
142/103:
unes=df1[df1['TIME_ESTIMATE']<0]['modSTATE'].value_counts().reset_index(name="Under Estimator")
df1[df1['TIME_ESTIMATE']<0][df1["modSTATE"]=="COMPLETED"]['ACCOUNT'].value_counts().reset_index(name="Under Estimator").plot(kind='bar')
#glasshouse(1714), virgini(1139),prince(669)
142/104:
unes=df1[df1['TIME_ESTIMATE']<0]['modSTATE'].value_counts().reset_index(name="Under Estimator")
df1[df1['TIME_ESTIMATE']<0][df1["modSTATE"]=="COMPLETED"]['ACCOUNT'].value_counts().reset_index(name="Under Estimator")
#glasshouse(1714), virgini(1139),prince(669)
142/105: df1[[df1["modSTATE"]=="COMPLETED"][df1['USE_SEC']=0][df1['MEM']==0]].value_counts()
142/106: df1[[df1["modSTATE"]=="COMPLETED"][df1['USE_SEC']=+0][df1['MEM']==0]].value_counts()
142/107: df1[[df1["modSTATE"]=="COMPLETED"][df1['USE_SEC']==0][df1['MEM']==0]].value_counts()
142/108: df1[[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0][df1['MEM']==0]].value_counts()
142/109: df1[[df1["modSTATE"]=="COMPLETED"] & [df1['USED_SEC']==0] &[df1['MEM']==0]].value_counts()
142/110: df1[[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0]][df1['MEM']==0].value_counts()
142/111: df1[[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0]].value_counts()
142/112: df1[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0].value_counts()
142/113: df1[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0][df1['MEM']==0].agg('sum')
142/114: df1[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0][df1['MEM']==0].counts
142/115: df1[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0][df1['MEM']==0].counts()
142/116: df1[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0][df1['MEM']==0].count()
142/117: df1[df1["modSTATE"]=="CANCELED"][df1['USED_SEC']==0][df1['MEM']==0].count()
142/118: df1[df1["modSTATE"]=="FAILED"][df1['USED_SEC']==0][df1['MEM']==0].count()
142/119: df1[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0][df1['MEM']==0].count()
142/120: df1[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0][df1['MEM']==0][df1['PARTITION'].value_counts()
142/121: df1[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0][df1['MEM']==0]['PARTITION'].value_counts()
145/1:
import pandas as pd
import re
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df1 = pd.read_csv('./data/accre_job.csv')
df4_SC = pd.read_csv('./data/Completed_jobs.csv')
145/2: df1.head()
145/3: df1.describe()
145/4: df1.info()
145/5: df1['STATE'].value_counts()
145/6:
df1['modSTATE'] = df1['STATE']
df1.head()
145/7:
df1['modSTATE']=df1['modSTATE'].str.split(' by', expand = True)[0]
df1['modSTATE'].value_counts()
#df1['modSTATE'].value_counts()
145/8: df1['NODES'].describe()
145/9:
#what is the percentage distribution of the three partitions.
#what is the percentage distribution of the users across partitions
#what is the percentage distribution of the jobs STATE in data
#what is the percntage memmorry, GPU, USEDTIME REQTIME and timeestimate in data per partition and peruser.
145/10: df1['ACCOUNT'].nunique()
145/11:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().head(10).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
per_acc
145/12:
#percent of the each ACCOUNT in the data set
plt.figure(figsize = (8,5))
per_acc= df1['ACCOUNT'].value_counts()/len(df1.axes[0])*100
per_acc.sort_values().tail(10).plot(kind='bar', title = 'TOP 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
145/13:
#percent of the each PARTITION in the data set
plt.figure(figsize = (8,5))
per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
per_partition
per_partition.sort_values().tail(10).plot(kind='bar', title = '% Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
per_partition
145/14:
plt.figure(figsize = (8,5))
user_Count = df1.groupby(['ACCOUNT'])['USER'].nunique()
user_Count.plot(kind='bar', title='Number of total users per account', ylabel='Number of users')
plt.xticks(rotation = 90, fontsize = 14);
user_Count
145/15:
plt.figure(figsize = (8,5))
user_account = (((df1.groupby(['ACCOUNT'])['USER'].nunique().sort_values())/df1['USER'].nunique())*100)
user_account.plot(kind='bar', title = '% of total users per account', ylabel='%User', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);

#PAR = df4_SC['PARTITION'].value_counts()
#PAR.plot(kind='bar', ylabel = 'counts of completed jobs', title = 'Total Number of completed jobs per partition');
145/16: #user_account
145/17:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['ACCOUNT'].nunique().sort_values().plot(kind='bar', title = 'Number total ACCOUNTS per PARTITION', ylabel='# of ACCOUNT', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
145/18:
plt.figure(figsize = (8,5))
df1.groupby(['PARTITION'])['USER'].nunique().sort_values().plot(kind='bar', title = 'Number total USERS per PARTITION', ylabel='# of USER', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
145/19:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
145/20:
plt.figure(figsize = (10,5))
max_acc_count=df1.loc[df1['PARTITION'] == 'maxwell']['ACCOUNT'].value_counts()/len(df1.axes[0])*100

max_acc_count.plot(kind='bar',  title = '% distribution of each account per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
145/21: (df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
145/22: (df1.groupby(df1['PARTITION'])['USED_SEC'].agg('sum')/df1['USED_SEC'].sum())*100
145/23: (df1.groupby(df1['PARTITION'])['REQ_SEC'].agg('sum')/df1['REQ_SEC'].sum())*100
145/24:
#What percentage of total memorry is used by each partition
(df1.groupby(df1['PARTITION'])['MEM'].agg('sum')/df1['MEM'].sum())*100
145/25:
#What is the percentage of total GPU usgae by each partition
(df1.groupby(df1['PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
145/26: (df1.groupby(['PARTITION', 'ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
145/27: (df1.groupby(['PARTITION', 'ACCOUNT'])['modSTATE'].value_counts()/len(df1.axes[0])*100)
145/28:
colors= [ '#0000FF', '#FF4500', '#03A89E']
plt.figure(figsize = (10,5))
(df1.groupby(['PARTITION'])['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar',  title = '% distribution Job status per PARTITION', ylabel='% distribution', xlabel='PARTITION')
plt.xticks(rotation = 90, fontsize = 14);
145/29: (df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('sum')/df1['GPUS'].sum())*100
145/30: df1[df1['PARTITION']=='maxwell'].groupby(['ACCOUNT'])['GPUS'].agg('mean').plot()
145/31: plotdf1= df1.groupby(['PARTITION'])['modSTATE'].value_counts().reset_index(name='value_counts')
145/32:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(plotdf1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="modSTATE", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
145/33: df1['modSTATE'].value_counts()
145/34:
#g = sns.FacetGrid(df1, col="PARTITION", row="ACCOUNT", margin_titles=True)
g = sns.FacetGrid(df1, col="PARTITION",  margin_titles=True)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="GPUS")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)
145/35: df1['ACCOUNT'].unique()
145/36:
plotdf2 = df1.groupby(['PARTITION','ACCOUNT'])['modSTATE'].value_counts().reset_index(name='value_counts')
plotdf2.head()

#plotdf2['modSTATE']].drop_duplicates()
#plotdf2[['ACCOUNT']].drop_dup[licates()

plotdf2 = pd.merge(left=plotdf2,
         right=pd.merge(left=pd.merge(left=plotdf2[['modSTATE']].drop_duplicates(),
                        right=plotdf2[['ACCOUNT']].drop_duplicates(),
                        how='cross'),
                        right=plotdf2[['PARTITION']].drop_duplicates(),
                        how='cross'),
         how='outer').fillna(0)
plotdf2.shape #(486, 4)
plotdf2 = plotdf2.loc[~plotdf2['modSTATE'].isin(['RUNNING','PENDING'])]
145/37:

g = sns.FacetGrid(plotdf2, col="PARTITION", row="modSTATE",  margin_titles=True, 
                  sharey=False, height=4, aspect=2)

plot = g.map_dataframe(sns.barplot, x="ACCOUNT", y="value_counts")
for axes in plot.axes.flat:
    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
145/38:
plot= sns.barplot(data=df1, x="ACCOUNT", y="GPUS")
#for axes in plot.axes.flat:
 #   _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90, fontsize=20)
145/39: plt.hist(df1['USED_SEC'])
145/40: # = df1['GPUS']
145/41: df1.head()
145/42: sns.stripplot(data = df1, x = 'GPUS', y = 'MEM');
145/43:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
145/44:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('SECONDS', fontsize = fontsize)
plt.title('Time estimate for jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
145/45:
df4_SC['TIME_ESTIMATE']=df4_SC['TIME_ESTIMATE']/60
df4_SC["TIME_ESTIMATE"].describe()
145/46:
fontsize = 16

plt.subplots(figsize = (14, 6))
sns.stripplot(data = df4_SC, x = 'ACCOUNT', y = 'TIME_ESTIMATE')
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Minutes', fontsize = fontsize)
plt.title('Time estimate for completed jobs by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
145/47:
fontsize = 16
mem_per_GPU = (df1['MEM']/df1['GPUS'])/1000000
plt.subplots(figsize = (14, 6))
sns.stripplot(data = df1, x = 'ACCOUNT',
              y = mem_per_GPU)
plt.xticks(fontsize = fontsize, rotation=90)
plt.xlabel('ACCOUNT', fontsize = fontsize)
plt.yticks(fontsize = fontsize)
plt.ylabel('Gigabytes/GPUS', fontsize = fontsize)
plt.title('Memmory per GPU usage by different accounts', fontsize = fontsize, fontweight = 'bold')

xmin, xmax = plt.xlim()
plt.hlines(y = 0.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Exact estimate of time')
#plt.hlines(y = 1.0, xmin = xmin, xmax = xmax, linestyle = '--', label = 'Over estimate of time')
plt.legend(loc = 'upper left', fontsize = fontsize - 2);
145/48: df1['modSTATE'].value_counts()/len(df1.axes[0])*100
145/49:
pdf = (df1.groupby(['modSTATE'])['PARTITION'].value_counts()/len(df1.axes[0])*100).reset_index(name='Jobs_per_partition')

(df1.groupby(['modSTATE','PARTITION'])['ACCOUNT'].value_counts()/len(df1.axes[0])*100).reset_index(name='account')
145/50:
gpudf = ((df1.groupby(['modSTATE', 'PARTITION'])['GPUS'].agg('sum')/df1['GPUS'].agg('sum'))*100).reset_index(name='%GPUS')
gpudf.head()

memdf = ((df1.groupby(['modSTATE', 'PARTITION'])['MEM'].agg('sum')/df1['MEM'].agg('sum'))*100).reset_index(name='%mem used')
memdf.head()

timest = ((df1.groupby(['modSTATE', 'PARTITION'])['TIME_ESTIMATE'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='timest')
timest.head()

usedsec = ((df1.groupby(['modSTATE', 'PARTITION'])['USED_SEC'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='usedtime')
usedsec.head()

reqsec = ((df1.groupby(['modSTATE', 'PARTITION'])['REQ_SEC'].agg('sum')/df1['TIME_ESTIMATE'].agg('sum'))*100).reset_index(name='reqtime')
reqsec.head()
145/51:
gpu_par = pd.merge(left = pdf, right = gpudf, how='outer')
gpu_par_mem = pd.merge(left= gpu_par, right = memdf, how='outer')
gpu_par_mem_reqsec = pd.merge(left =gpu_par_mem, right = reqsec, how='outer')
gpu_par_mem_reqsec_usedsec = pd.merge(left=gpu_par_mem_reqsec, right=usedsec, how='outer')
gpu_par_mem_reqsec_usedsec_timest = pd.merge (left=gpu_par_mem_reqsec_usedsec, right=timest, how='outer')
gpu_par_mem_reqsec_usedsec_timest.head(20)
145/52:
mer_per_df = gpu_par_mem_reqsec_usedsec_timest.iloc[:-6]
mer_per_df.head(10)
145/53: sns.pairplot(mer_per_df, hue="PARTITION");
145/54:
plt.subplots(figsize = (14, 6))
ax=sns.barplot(data=mer_per_df, x="modSTATE", y="timest", hue="PARTITION");
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('% of Time Estimated for total job in each partition', fontsize = 16, fontweight = 'bold')
145/55:
plt.subplots(figsize = (14, 6))
ax = (df1['modSTATE'].value_counts()/len(df1.axes[0])*100).plot(kind='bar', title = 'Status of Jobs in data', ylabel='Percentage of total JOBS', xlabel='JOB STATE')
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(rotation = 90, fontsize = 14);
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('Percentage of total JOBS', fontsize = 16)
plt.title('Status of the Jobs representated as % of total job in each partition', fontsize = 16, fontweight = 'bold')
145/56:
use1=((df1.groupby(['modSTATE'])['USED_SEC'].agg('sum')/sum(df1['USED_SEC']))*100).reset_index(name='Used_Sec%')
use1

req1=((df1.groupby(['modSTATE'])['REQ_SEC'].agg('sum')/sum(df1['REQ_SEC']))*100).reset_index(name='Req_Sec%')
req1


pd.merge(req1, use1, how='outer')
145/57: df1.columns
145/58:
exes= df1[df1['TIME_ESTIMATE']==0]['modSTATE'].value_counts().reset_index(name='Exact Estimator')
df1[df1['TIME_ESTIMATE']==0]['ACCOUNT'].value_counts().reset_index(name='Exact Estimator Account')
#prince(23), virginia (12), glasshouse(8)
145/59:
oves= df1[df1['TIME_ESTIMATE']>0]['modSTATE'].value_counts().reset_index(name="Over Estimator")
df1[df1['TIME_ESTIMATE']>0]['ACCOUNT'].value_counts().reset_index(name='Over Estimator Account')
#glasshouse(245403), malanga(51676), virgini(9191)
145/60:
unes=df1[df1['TIME_ESTIMATE']<0]['modSTATE'].value_counts().reset_index(name="Under Estimator")
df1[df1['TIME_ESTIMATE']<0][df1["modSTATE"]=="COMPLETED"]['ACCOUNT'].value_counts().reset_index(name="Under Estimator")
#glasshouse(1714 (1038 completed), virgini(1139),prince(669)
145/61:
exov = pd.merge(exes, oves, how='outer')
exovunex = pd.merge(exov, unes, how='outer').fillna(0)
exovunex
145/62:
plt.subplots(figsize = (14, 6))
ax=exovunex.plot( x="index", y=["Exact Estimator", "Over Estimator", "Under Estimator"], kind="bar");
#ax.bar_label(ax.containers)
for i in ax.containers:
    ax.bar_label(i,)
plt.xticks(fontsize = 16, rotation=0)
plt.xlabel('JOB STATUS', fontsize = 16)
plt.yticks(fontsize = 16)
plt.ylabel('% of total time estimated (req-used)', fontsize = 16)
plt.title('% of Time Estimated for total job in each partition', fontsize = 16, fontweight = 'bold')
145/63: exovunex.plot( x="index", y=["Exact Estimator", "Over Estimator", "Under Estimator"], kind="bar")
145/64: df1[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0][df1['MEM']==0].count()
145/65: df1[df1["modSTATE"]=="COMPLETED"][df1['USED_SEC']==0][df1['MEM']==0]['PARTITION'].value_counts()
145/66: df1.head()
145/67:
#creating weighted columns
df1['memory_per_gpu'] = df1['MEM']/df1['GPUS']
145/68:
#creating weighted columns
df1['memory_per_gpu'] = df1['MEM']/df1['GPUS']
df1['weight'] = df1['GPUS'] * df1['USED_SEC']
145/69:
#creating weighted columns
df1['memory_per_gpu'] = df1['MEM']/df1['GPUS']
df1['weight'] = df1['GPUS'] * df1['USED_SEC'] 
df1['weighted_memory_gpu'] = df1['memory_per_gpu'] * df1['weight']
145/70:
#creating weighted columns
df1['memory_per_gpu'] = df1['MEM']/df1['GPUS'] #Unit will be Megabytes/GPUS
df1['weight'] = df1['GPUS'] * df1['USED_SEC'] #Unit will be GPUS sec
df1['weighted_memory_gpu'] = df1['memory_per_gpu'] * df1['weight'] #unit is megabytes sec
df1.columns
145/71: df1['memory_per_gpu'].describe()
145/72: df1[df1['memory_per_gpu'] == 0].count
145/73: df1[df1['memory_per_gpu'] == 0].count()
145/74: df1[df1['memory_per_gpu'] == 0]['PARTITION'].count()  #58764
145/75: df1[df1['memory_per_gpu'] == 0]['PARTITION'].value_counts()  #58764
147/1: TNDA_pop_inc['tract'].nunique()
147/2: TNDA_pop_inc['tract'].nunique()
148/1:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
148/2:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
response
res = response.json() 
res
#response.text #this is not needed as the website sends a json object rather than the url object
res #it is a list of dictionary

#However, it will be easier to work with as a json. We can use the json method to convert the results to a dictionary.
#res.keys() #so this will not work

#as this is a list of dictionaries it can be directly converted to data frame using the pd.DataFrame code.
df = pd.DataFrame(res)
df.head(2)
148/3:
endpoint1 = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T12:00:00' and '2022-09-30T00:00:00'",
    #'zip_code' : '37207' #we can add any parameter here
}

response1 = requests.get(endpoint1, params = params)

response1
148/4:
res1 = response1.json()
#res1
148/5:
agg_burg = pd.DataFrame(res1)
agg_burg.head(3)
agg_burg.shape

#We only have 1000 rows we need to increase this becuase this is restricted by the API and 
##there is a parameter that can be set to go upto 2000
148/6:
agg_burg.columns
#agg_burg["offense_description"].value_counts()
148/7: agg_burg["offense_description"].value_counts() #looking for the description it should only be BURGLARY- AGGRAVATED
148/8: agg_burg.offense_description.unique
148/9:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'


response_pop = requests.get(endpoint_pop)
, 
response_pop
148/10:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)
, 
response_pop
148/11:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
#TNDA_pop.shape
148/12:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TNDA_pop.iloc[0] #pull the row 1 in a list called header
TNDA_pop = TNDA_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_pop.columns = header
TNDA_pop.head()
148/13:
#Renaming the column B01001_00IE as population

TNDA_pop = TNDA_pop.rename(columns={'B01001_001E':'Population'})
TNDA_pop.head()
148/14:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TNDA_inc = pd.DataFrame(res_inc)
TNDA_inc.head()
#TNDA_inc.shape (175, 5)
148/15:
## Naming the columns same as done for the pop because the header row is missing.
header = TNDA_inc.iloc[0] #pull the row 1 in a list called header
TNDA_inc = TNDA_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_inc.columns = header
TNDA_inc.head()
148/16:
#Renaming the column S1901_C01_012E as population

TNDA_inc = TNDA_inc.rename(columns={'S1901_C01_012E':'MedianIncome'})
TNDA_inc.head()
148/17:
TNDA_pop_inc = pd.merge(TNDA_pop, TNDA_inc, on=['NAME', 'state', 'county', 'tract'])
TNDA_pop_inc.shape #(174, 6)
TNDA_pop_inc.head(3)
148/18: TNDA_pop_inc['tract'].nunique()
148/19: import geopandas as gpd
148/20:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
#TN_shape.shape #(1701, 13)
148/21:
agg_burg.head
agg_burg.columns
148/22:
agg_burg['geometry'] = gpd.points_from_xy(agg_burg['longitude'], agg_burg['latitude'])
agg_burg.head()
148/23:
TN_BURG_GEO = gpd.GeoDataFrame(agg_burg, 
                           crs = TN_shape.crs, 
                           geometry = agg_burg['geometry'])
148/24: type(TN_BURG_GEO)
148/25: TN_BURG_GEO.plot()
148/26:
BURG_TNDA = gpd.sjoin(TN_BURG_GEO, TN_shape, predicate = 'within')
BURG_TNDA.shape
148/27:
BURG_TNDA.head()
BURG_TNDA.columns
148/28: BURG_TNDA.head()
148/29: BURG_TNDA.columns()
148/30: BURG_TNDA.columns
148/31: BURG_TNDA.groupby(['TRACTCE'])['TRACTCE'].count().sort
148/32: BURG_TNDA.groupby(['TRACTCE'])['TRACTCE'].count()
148/33: BURG_TNDA.groupby(['TRACTCE'])['TRACTCE'].count().sort_index()
148/34: BURG_TNDA['incident_number'].nunique()
148/35: BURG_TNDA['incident_number'].value_counts()
148/36: TNDABURG = BURG_TNDA.drop_duplicate('incident_number')
148/37: BURG_TNDA.drop_duplicates('incident_number')
148/38: TNDABURG = BURG_TNDA.drop_duplicates('incident_number')
148/39:
TNDABURG = BURG_TNDA.drop_duplicates('incident_number')
TNDABURG.shape()
148/40:
TNDABURG = BURG_TNDA.drop_duplicates('incident_number')
TNDABURG.shape
148/41: TNDABURG.colums
148/42: TNDABURG.columns
148/43:
TNDABURG = BURG_TNDA.drop_duplicates('incident_number')
TNDABURG.shape
TNDABURG['incident_number'].value_counts()
148/44: BURG_TNDA['incident_number'].nunique()
148/45:
TNDABURG = BURG_TNDA.drop_duplicates('incident_number')
TNDABURG.shape
#TNDABURG['incident_number'].value_counts()
148/46:
type(TN_BURG_GEO)
TN_BURG_GEO.shape
148/47:
type(TN_BURG_GEO)
#TN_BURG_GEO.shape #(1000, 32)
149/1:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
149/2:
endpoint = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

response = requests.get(endpoint)
response
res = response.json() 
res
#response.text #this is not needed as the website sends a json object rather than the url object
res #it is a list of dictionary

#However, it will be easier to work with as a json. We can use the json method to convert the results to a dictionary.
#res.keys() #so this will not work

#as this is a list of dictionaries it can be directly converted to data frame using the pd.DataFrame code.
df = pd.DataFrame(res)
df.head(2)
149/3:
endpoint1 = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T00:00:00' and '2022-09-30T23:59:59'",
    #'zip_code' : '37207' #we can add any parameter here
}

response1 = requests.get(endpoint1, params = params)

response1
149/4:
res1 = response1.json()
#res1
149/5:
agg_burg = pd.DataFrame(res1)
agg_burg.head(3)
agg_burg.shape

#We only have 1000 rows we need to increase this becuase this is restricted by the API and 
##there is a parameter that can be set to go upto 2000
149/6:
agg_burg.columns
#agg_burg["offense_description"].value_counts()
149/7: agg_burg["offense_description"].value_counts() #looking for the description it should only be BURGLARY- AGGRAVATED
149/8: agg_burg.offense_description.unique
149/9:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'


response_pop = requests.get(endpoint_pop)
, 
response_pop
149/10:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)
, 
response_pop
149/11:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
#TNDA_pop.shape
149/12:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TNDA_pop.iloc[0] #pull the row 1 in a list called header
TNDA_pop = TNDA_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_pop.columns = header
TNDA_pop.head()
149/13:
#Renaming the column B01001_00IE as population

TNDA_pop = TNDA_pop.rename(columns={'B01001_001E':'Population'})
TNDA_pop.head()
149/14:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TNDA_inc = pd.DataFrame(res_inc)
TNDA_inc.head()
#TNDA_inc.shape (175, 5)
149/15:
## Naming the columns same as done for the pop because the header row is missing.
header = TNDA_inc.iloc[0] #pull the row 1 in a list called header
TNDA_inc = TNDA_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_inc.columns = header
TNDA_inc.head()
149/16:
#Renaming the column S1901_C01_012E as population

TNDA_inc = TNDA_inc.rename(columns={'S1901_C01_012E':'MedianIncome'})
TNDA_inc.head()
149/17:
TNDA_pop_inc = pd.merge(TNDA_pop, TNDA_inc, on=['NAME', 'state', 'county', 'tract'])
TNDA_pop_inc.shape #(174, 6)
TNDA_pop_inc.head(3)
149/18: TNDA_pop_inc['tract'].nunique()
149/19: import geopandas as gpd
149/20:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
#TN_shape.shape #(1701, 13)
149/21:
agg_burg.head
agg_burg.columns
149/22:
agg_burg['geometry'] = gpd.points_from_xy(agg_burg['longitude'], agg_burg['latitude'])
agg_burg.head()
149/23:
TN_BURG_GEO = gpd.GeoDataFrame(agg_burg, 
                           crs = TN_shape.crs, 
                           geometry = agg_burg['geometry'])
149/24:
type(TN_BURG_GEO)
#TN_BURG_GEO.shape #(1000, 32)
149/25: TN_BURG_GEO.plot()
149/26:
BURG_TNDA = gpd.sjoin(TN_BURG_GEO, TN_shape, predicate = 'within')
BURG_TNDA.shape
149/27:
BURG_TNDA.head()
BURG_TNDA.columns
149/28: BURG_TNDA.head()
149/29: BURG_TNDA.columns
149/30: BURG_TNDA.groupby(['TRACTCE'])['TRACTCE'].count().sort_index()
149/31: BURG_TNDA['incident_number'].nunique()
149/32:
TNDABURG = BURG_TNDA.drop_duplicates('incident_number')
TNDABURG.shape
#TNDABURG['incident_number'].value_counts()
149/33: TNDABURG.columns
149/34:
endpoint1 = 'https://data.nashville.gov/resource/2u6v-ujjs.json'

params = {
    
    'offense_description' : 'BURGLARY- AGGRAVATED',
    '$where' : "incident_occurred between '2022-01-01T00:00:00' and '2022-09-30T23:59:59'",
    '$limit' : 10000
    #'zip_code' : '37207' #we can add any parameter here
}

response1 = requests.get(endpoint1, params = params)

response1
149/35:
res1 = response1.json()
#res1
149/36:
agg_burg = pd.DataFrame(res1)
agg_burg.head(3)
agg_burg.shape

#We only have 1000 rows we need to increase this becuase this is restricted by the API and 
##there is a parameter that can be set to go upto 2000
149/37:
agg_burg.columns
#agg_burg["offense_description"].value_counts()
149/38: agg_burg["offense_description"].value_counts() #looking for the description it should only be BURGLARY- AGGRAVATED
149/39: agg_burg.offense_description.unique
149/40:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'


response_pop = requests.get(endpoint_pop)
, 
response_pop
149/41:
endpoint_pop = 'https://api.census.gov/data/2020/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)
, 
response_pop
149/42:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
#TNDA_pop.shape
149/43:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TNDA_pop.iloc[0] #pull the row 1 in a list called header
TNDA_pop = TNDA_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_pop.columns = header
TNDA_pop.head()
149/44:
#Renaming the column B01001_00IE as population

TNDA_pop = TNDA_pop.rename(columns={'B01001_001E':'Population'})
TNDA_pop.head()
149/45:
endpoint_inc = 'https://api.census.gov/data/2020/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'tract',
    'in' : 'county:037 state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TNDA_inc = pd.DataFrame(res_inc)
TNDA_inc.head()
#TNDA_inc.shape (175, 5)
149/46:
## Naming the columns same as done for the pop because the header row is missing.
header = TNDA_inc.iloc[0] #pull the row 1 in a list called header
TNDA_inc = TNDA_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_inc.columns = header
TNDA_inc.head()
149/47:
#Renaming the column S1901_C01_012E as population

TNDA_inc = TNDA_inc.rename(columns={'S1901_C01_012E':'MedianIncome'})
TNDA_inc.head()
149/48:
TNDA_pop_inc = pd.merge(TNDA_pop, TNDA_inc, on=['NAME', 'state', 'county', 'tract'])
TNDA_pop_inc.shape #(174, 6)
TNDA_pop_inc.head(3)
149/49: TNDA_pop_inc['tract'].nunique()
149/50: import geopandas as gpd
149/51:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
#TN_shape.shape #(1701, 13)
149/52:
agg_burg.head
agg_burg.columns
149/53:
agg_burg['geometry'] = gpd.points_from_xy(agg_burg['longitude'], agg_burg['latitude'])
agg_burg.head()
149/54:
TN_BURG_GEO = gpd.GeoDataFrame(agg_burg, 
                           crs = TN_shape.crs, 
                           geometry = agg_burg['geometry'])
149/55:
type(TN_BURG_GEO)
#TN_BURG_GEO.shape #(1000, 32)
149/56: TN_BURG_GEO.plot()
149/57:
BURG_TNDA = gpd.sjoin(TN_BURG_GEO, TN_shape, predicate = 'within')
BURG_TNDA.shape
149/58:
BURG_TNDA.head()
BURG_TNDA.columns
149/59: BURG_TNDA.head()
149/60: BURG_TNDA.columns
149/61: BURG_TNDA.groupby(['TRACTCE'])['TRACTCE'].count().sort_index()
149/62: BURG_TNDA['incident_number'].nunique()
149/63:
TNDABURG = BURG_TNDA.drop_duplicates('incident_number')
TNDABURG.shape
#TNDABURG['incident_number'].value_counts()
149/64: TNDABURG.columns
149/65: TNDABURG['TRACTCE'].value_counts()
149/66: burOnly = TNDABURG['TRACTCE'].value_counts().reset_index(name='Burg_count')
149/67:
burOnly = TNDABURG['TRACTCE'].value_counts().reset_index(name='Burg_count')
burOnly.shape
149/68:
burOnly = TNDABURG['TRACTCE'].value_counts().reset_index(name='Burg_count')
burOnly.shape
burOnly.head()
149/69:
burOnly = TNDABURG['TRACTCE'].value_counts().reset_index(name='Burg_count')
burOnly.shape
burOnly.head()
burOnly.rename(columns = {'index':"TRACT"})
149/70:
burOnly = TNDABURG['TRACTCE'].value_counts().reset_index(name='Burg_count')
burOnly.shape
burOnly.head()
burOnly = burOnly.rename(columns = {'index':"TRACT"})
149/71:
burOnly = TNDABURG['TRACTCE'].value_counts().reset_index(name='Burg_count')
burOnly.shape
burOnly.head()
burOnly = burOnly.rename(columns = {'index':"TRACT"})
burOnly.head()
149/72:
burOnly = TNDABURG['TRACTCE'].value_counts().reset_index(name='Burg_count')
burOnly.shape
burOnly.head()
burOnly = burOnly.rename(columns = {'index':"tract"})
burOnly.head()
149/73: pd.merge(left=TNDA_pop_inc, right=burOnly, on=['tract'], how='outer')
149/74: TNDABur_pop_inc = pd.merge(left=TNDA_pop_inc, right=burOnly, on=['tract'], how='outer')
149/75:
TNDABur_pop_inc = pd.merge(left=TNDA_pop_inc, right=burOnly, on=['tract'], how='outer')
TNDABur_pop_inc.shape
149/76: TNDABur['tract'].nunique()
149/77: TNDABur_pop_inc['tract'].nunique()
149/78:
TNDABur_pop_inc = pd.merge(left=TNDA_pop_inc, right=burOnly, on=['tract'], how='left')
TNDABur_pop_inc.shape
149/79: TNDA_pop_inc["tract"].nunique()
149/80:
TNDABur_pop_inc = pd.merge(left=TNDA_pop_inc, right=burOnly, on=['tract'], how='outer')
TNDABur_pop_inc.shape
150/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
150/2: pd.read_csv('../data/gourds.csv')
150/3: pd.read_csv('/data/gourds.csv')
150/4: pd.read_csv('data/gourds.csv')
150/5: df_gourd = pd.read_csv('data/gourds.csv')
150/6:
df_gourd = pd.read_csv('data/gourds.csv')
df_gourd.head(5)
150/7: df_gourd.shape
150/8:
df_gourd.shape #(28011, 16)
df_gourd.describe()
150/9:
df_gourd.shape #(28011, 16)
df_gourd.info()
150/10:
df_gourd.shape #(28011, 16)
df_gourd.info()
df_gourd.columns
150/11:
plt.figure(figsize = (8,5))
#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
df_gourd.hist('weight_lbs)
              #'(kind='hist', title = ' Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/12:
plt.figure(figsize = (8,5))
#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
df_gourd.hist('weight_lbs')
              #'(kind='hist', title = ' Distribution of each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/13:
plt.figure(figsize = (8,5))
#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
df_gourd.hist('weight_lbs',
             title = ' Distribution of the weight of pumpkin in the gourd dataset',
             ylabel = 'number of pumpkins'
             xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/14:
plt.figure(figsize = (8,5))
#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
df_gourd.hist('weight_lbs',
             title = ' Distribution of the weight of pumpkin in the gourd dataset',
             ylabel = 'number of pumpkins',
             xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/15:
plt.figure(figsize = (8,5))
#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
df_gourd.hist('weight_lbs',
             title = ' Distribution of the weight of pumpkin in the gourd dataset', ylabel = 'number of pumpkins', xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/16:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black')

#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
#df_gourd.hist('weight_lbs',
             title = ' Distribution of the weight of pumpkin in the gourd dataset', ylabel = 'number of pumpkins', xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/17:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black')

#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
#df_gourd.hist('weight_lbs',
            # title = ' Distribution of the weight of pumpkin in the gourd dataset', ylabel = 'number of pumpkins', xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/18:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', ylabel='Frequency')

#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
#df_gourd.hist('weight_lbs',
            # title = ' Distribution of the weight of pumpkin in the gourd dataset', ylabel = 'number of pumpkins', xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/19:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency')

#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
#df_gourd.hist('weight_lbs',
            # title = ' Distribution of the weight of pumpkin in the gourd dataset', ylabel = 'number of pumpkins', xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/20:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')

#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
#df_gourd.hist('weight_lbs',
            #  ylabel = 'number of pumpkins', xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/21:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs'
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')

#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
#df_gourd.hist('weight_lbs',
            #  ylabel = 'number of pumpkins', xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/22:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')

#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
#df_gourd.hist('weight_lbs',
            #  ylabel = 'number of pumpkins', xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
#plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/23:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')

#per_partition= df1['PARTITION'].value_counts()/len(df1.axes[0])*100
#per_partition
#df_gourd.hist('weight_lbs',
            #  ylabel = 'number of pumpkins', xlabel = 'lbs')
              #'(kind='hist',  each partition in the dataset', ylabel='Percentage of total JOBS', xlabel='Partition')
plt.xticks(rotation = 0, fontsize = 14);
#per_partition
150/24:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
df_gourd['weigt_lbs'].plot(kind='kde')
150/25:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import statistics
150/26:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
150/27: df_gourd["weight_lbs"].plot(kind='kde')
150/28:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
df_gourd["weight_lbs"].plot(kind='kde')
150/29:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
#df_gourd["weight_lbs"].plot(kind='kde')
150/30:
df_gourd["weight_lbs"].plot(kind='kde',
                           ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')
150/31: df_gourd['place'].type()
150/32: df_gourd['place'].info()
150/33: df_gourd['place'].unique()
150/34: df_gourd['place'].value_counts()
150/35:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

df_gourd['place'].replace('EXH', np.nan, inplace=True)
150/36:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

df_gourd['place'].replace('EXH', np.nan, inplace=True).value_counts()
150/37:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

df_gourds['place'] = df_gourd['place'].replace('EXH', np.nan, inplace=True)
150/38:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

df_gourd['place'] = df_gourd['place'].replace('EXH', np.nan, inplace=True)
150/39:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

df_gourd['place'] = df_gourd['place'].replace('EXH', np.nan, inplace=True)
df_gourd['place'].value_counts()
150/40:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].value_counts()
150/41:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].counts
150/42:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].counts()
150/43:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].count()
150/44:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

#df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].count()
150/45:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import statistics
150/46:
df_gourd = pd.read_csv('data/gourds.csv')
df_gourd.head(5)
150/47:
df_gourd.shape #(28011, 16)
df_gourd.info()
df_gourd.columns
150/48:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
#df_gourd["weight_lbs"].plot(kind='kde')
150/49:
df_gourd["weight_lbs"].plot(kind='kde',
                           ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')
150/50:
df_gourd['place'].value_counts() #EXH=1862, DMG=465

#df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
#df_gourd['place'].count()
150/51:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info()
#df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
#df_gourd['place'].count()
150/52:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].info()
150/53:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].info()
df_gourd['place'].astype(int)
150/54:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].astype(int)
150/55:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import statistics
150/56:
df_gourd = pd.read_csv('data/gourds.csv')
df_gourd.head(5)
150/57:
df_gourd.shape #(28011, 16)
df_gourd.info()
df_gourd.columns
150/58:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
#df_gourd["weight_lbs"].plot(kind='kde')
150/59:
df_gourd["weight_lbs"].plot(kind='kde',
                           ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')
150/60:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
#df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].astype(int)
150/61:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].astype(int)
150/62:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import statistics
150/63:
df_gourd = pd.read_csv('data/gourds.csv')
df_gourd.head(5)
150/64:
df_gourd.shape #(28011, 16)
df_gourd.info()
df_gourd.columns
150/65:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
#df_gourd["weight_lbs"].plot(kind='kde')
150/66:
df_gourd["weight_lbs"].plot(kind='kde',
                           ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')
150/67:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
#df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
#df_gourd['place'].astype(int)
150/68:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
#df_gourd['place'].astype(int)

#df['place'].fillna(0).astype(int)
150/69:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
#df_gourd['place'].astype(int)
df['place'].info()

#df['place'].fillna(0).astype(int)
150/70:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
#df_gourd['place'].astype(int)
df_gourd['place'].info()

#df['place'].fillna(0).astype(int)
150/71:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True).fillna(0).astype(int)
#df_gourd['place'].astype(int)
df_gourd['place'].info()

#df['place'].fillna(0).astype(int)
150/72:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
df_gourd['place'].info()

#df['place'].fillna(0).astype(int)
150/73:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import statistics
150/74:
df_gourd = pd.read_csv('data/gourds.csv')
df_gourd.head(5)
150/75:
df_gourd.shape #(28011, 16)
df_gourd.info()
df_gourd.columns
150/76:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
#df_gourd["weight_lbs"].plot(kind='kde')
150/77:
df_gourd["weight_lbs"].plot(kind='kde',
                           ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')
150/78:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
df_gourd.info() #object 28011
#df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
#df_gourd['place'].info()
df_gourd['place']
#df['place'].fillna(0).astype(int)
150/79:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
#df_gourd.info() #object 28011
#df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
#df_gourd['place'].info()
df_gourd['place']
#df['place'].fillna(0).astype(int)
150/80:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
#df_gourd.info() #object 28011
#df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
#df_gourd['place'].info()
df_gourd['place']
#df['place'].fillna(0).astype(int)

df_gourd['place'] = df_gourd['place'].apply(pd.to_numeric,errors='coerce')
150/81:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
#df_gourd.info() #object 28011
#df_gourd['place'] = df_gourd['place'].replace(['EXH', 'DMG'], np.nan, inplace=True)
#df_gourd['place'].info()
df_gourd['place']
#df['place'].fillna(0).astype(int)

df_gourd['place'] = df_gourd['place'].apply(pd.to_numeric,errors='coerce')
df_gourd['place'].info()
150/82:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
#df_gourd.info() #object 28011

df_gourd['place'] = df_gourd['place'].apply(pd.to_numeric,errors='coerce')
df_gourd['place'].info()
df_gourd.info()
150/83: df_gourd['country'].value_counts()
150/84: df_gourd['country'].value_counts().head(5)
150/85: df_gourd['country'].value_counts().head(5).plot(kind='bar')
150/86:
df_gourd['country'].value_counts().head(5).plot(kind='bar', title = 'Bottom 10 ACCOUNT_users in data', ylabel='Percentage of total JOBS', xlabel='Account')
plt.xticks(rotation = 90, fontsize = 14);
150/87:
plt.figure(figsize = (8,5))
df_gourd['country'].value_counts().head(5).plot(kind='bar', 
                                                title = 'Top 5 countries in gourd dataset', 
                                                ylabel='Number of Appearance in data', 
                                                xlabel='Country')
plt.xticks(rotation = 90, fontsize = 14);
150/88:
plt.figure(figsize = (8,5))
df_gourd['country'].value_counts().head(5).plot(kind='bar', 
                                                title = 'Top 5 countries in gourd dataset', 
                                                ylabel='Number of Appearance in data', 
                                                xlabel='Country')
plt.xticks(rotation = 0, fontsize = 14);
150/89: df_gourd['id']
150/90: df[['year','type']] = df['id'].Name.str.split("_",expand=True)
150/91: df[['year','type']] = df_gourd['id'].Name.str.split("_",expand=True)
150/92: df[['year','type']] = df_gourd['id'].str.split("_",expand=True)
150/93: df_gourd[['year','type']] = df_gourd['id'].Name.str.split("_",expand=True)
150/94: df_gourd[['year','type']] = df_gourd['id'].str.split("_",expand=True)
150/95: df_gourd['id'].str.split("_",expand=True)
150/96: df_gourd.id.str.split("_",expand=True)
150/97: df[['year','type']] = df.id.str.split("_",expand=True)
150/98: df_gourd[['year','type']] = df_gourd.id.str.split("_",expand=True)
150/99: len(df_gourd['id'])
150/100:
len(df_gourd['id'])
df_gourd['id'].unique
150/101:
len(df_gourd['id'])
df_gourd['id'].nunique()
150/102: df_gourd[['year','type']] = df_gourd['id'].str.split("_",expand=True)
150/103: df_gourd['id'].str.split("_",expand=True)
150/104: df_gourd['id'].str.split("_")
150/105: df_gourd[['year','type']] = df_gourd['id'].str.split("_")
150/106: #df_gourd[['year','type']] = df_gourd['id'].str.split("_")
150/107:
#df_gourd[['year','type']] = df_gourd['id'].str.split("_")

df_gourd.id.str.split('_',1).tolist(),
                         columns = ['first','Last']))
150/108:
#df_gourd[['year','type']] = df_gourd['id'].str.split("_")

df_gourd.id.str.split('_',1).tolist(),
                         columns = ['first','Last']
150/109:
#df_gourd[['year','type']] = df_gourd['id'].str.split("_")

df_gourd(df_gourd['id'].str.split('_',1).tolist(),
                         columns = ['first','Last'])
150/110:
#df_gourd[['year','type']] = df_gourd['id'].str.split("_")

df_gourd[['year','type']] = df.id.apply(lambda x: pd.Series(str(x).split("_")))
150/111:
#df_gourd[['year','type']] = df_gourd['id'].str.split("_")

df_gourd[['year','type']] = df_gourd.id.apply(lambda x: pd.Series(str(x).split("_")))
150/112:
df_gourd[['year','type']] = df_gourd['id'].str.split("_", expand=True, n=1)

#df_gourd[['year','type']] = df_gourd.id.apply(lambda x: pd.Series(str(x).split("_")))
150/113:
#df_gourd[['year','type']] = df_gourd['id'].str.split("_", expand=True, n=1)

df_gourd[['year','type']] = df_gourd.id.apply(lambda x: pd.Series(str(x).split("_"), axis=1))
150/114:
#df_gourd[['year','type']] = df_gourd['id'].str.split("_", expand=True, n=1)

#df_gourd[['year','type']] = df_gourd.id.apply(lambda x: pd.Series(str(x).split("_"), axis=1))

print(df_gourd.id.apply(lambda x: pd.Series(str(x).split("_"))))
150/115:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)

#df_gourd[['year','type']] = df_gourd.id.apply(lambda x: pd.Series(str(x).split("_"), axis=1))

#print(df_gourd.id.apply(lambda x: pd.Series(str(x).split(''))))
150/116:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)
df_gourd.head()

#df_gourd[['year','type']] = df_gourd.id.apply(lambda x: pd.Series(str(x).split("_"), axis=1))

#print(df_gourd.id.apply(lambda x: pd.Series(str(x).split(''))))
150/117:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)
df_gourd.head()
df_gourd.shape
150/118:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)
df_gourd.head()
df_gourd.shape #(28011, 18)
df_gourd.columns
150/119:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)
df_gourd.head()
df_gourd.shape #(28011, 18)
df_gourd.columns
df_gourd.info()
150/120:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)
df_gourd.head()
df_gourd.shape #(28011, 18)
df_gourd.columns
df_gourd.info()
df_gourd.describe()
150/121: df_gourd['year']
150/122: df_gourd['year'].value_counts()
150/123:
df_gourd['year'].value_counts()
df_gourd.groupby(['year'])['weight_lbs'].max()
150/124:
df_gourd['year'].value_counts()
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line')
150/125:
df_gourd['year'].value_counts()
#df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line')
150/126:
df_gourd['year'].value_counts().sort_index()
#df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line')
150/127:
df_gourd['year'].value_counts().sort_index() #2013 to 2021
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line')
150/128:
plt.figure(figsize = (8,5))
df_gourd['year'].value_counts().sort_index() #2013 to 2021
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line', 
                                                    title = 'Trend in pumpkin weight over the year', 
                                                    ylabel='Weight of the pumpkin (lbs)', 
                                                    xlabel='Year')
plt.xticks(rotation = 0, fontsize = 14);
150/129:
plt.figure(figsize = (8,5))
df_gourd['year'].value_counts().sort_index() #2013 to 2021
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line', 
                                                    title = 'Trend in pumpkin weight over the year', 
                                                    ylabel='Weight of the pumpkin (lbs)', 
                                                    xlabel='Year')
plt.xticks(rotation = 0, fontsize = 20);
150/130:
plt.figure(figsize = (8,5))
df_gourd['year'].value_counts().sort_index() #2013 to 2021
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line', 
                                                    title = 'Trend in pumpkin weight over the year', 
                                                    ylabel='Weight of the pumpkin (lbs)', 
                                                    xlabel='Year')
plt.xticks(rotation = 0, fontsize = 14);
150/131:
plt.figure(figsize = (8,5))
df_gourd['year'].value_counts().sort_index() #2013 to 2021
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line', 
                                                    title = 'Trend in pumpkin weight over the year', 
                                                    ylabel='Weight of the pumpkin (lbs)', 
                                                    xlabel='Year', fontsize=14)
plt.xticks(rotation = 0, fontsize = 14);
150/132: df_gourd.groupby(['type'])['weight_lbs'].agg('mean')
150/133:
df_gourd.groupby(['type'])['weight_lbs'].agg('mean').plot(kind='bar', 
                                                    title = 'Average pumpkin weight among different types', 
                                                    ylabel='Average weight (lbs)', 
                                                    xlabel='TYPE')
plt.xticks(rotation = 0, fontsize = 14);
150/134:
plt.figure(figsize = (8,5))
df_gourd.groupby(['type'])['weight_lbs'].agg('mean').plot(kind='bar', 
                                                    title = 'Average pumpkin weight among different types', 
                                                    ylabel='Average weight (lbs)', 
                                                    xlabel='TYPE')
plt.xticks(rotation = 0, fontsize = 14);
150/135: df_gourd.plot(x = 'est_year', y = 'weight_lbs');
150/136: df_gourd.plot(x ='est_year', y = 'weight_lbs')
150/137: df_gourd.plot(kind="scatter", x='est_weight', y='weight_lbs')
150/138:
plt.scatter(y = df_gourd['est_weight'], x = df_gourd['weight_lbs'], 
            s=df_gourd['year'], c=df_gourd['type'], alpha=0.5)
plt.show()
150/139:
df_gourd(plt.scatter(y = df_gourd['est_weight'], x = df_gourd['weight_lbs'], 
            s=df_gourd['year'], c=df_gourd['type'], alpha=0.5))
plt.show()
150/140: df_gourd['weight_error'] = df_gourd['est_weight']-df['weight_lbs']
150/141: df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
150/142:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
150/143:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
[df_gourd['weight_error'] == 0].value_counts()
150/144:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
[df_gourd['weight_error'] == 0].counts()
150/145:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
[df_gourd['weight_error'] == 0].counts
150/146:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
[df_gourd['weight_error'] == 0].count
150/147:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
[df_gourd['weight_error'] == 0].count()
150/148:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
[df_gourd['weight_error'] > 0].value_counts()
150/149:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
[df_gourd['weight_error'] > 0]['year'].value_counts()
150/150:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
df_gourd[df_gourd['weight_error'] > 0]['year'].value_counts()
150/151:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
#df_gourd[df_gourd['weight_error'] > 0]['year'].value_counts()
150/152:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
df_gourd[df_gourd['weight_error'] > 0]['year'].value_counts()
150/153:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
df_gourd[df_gourd['weight_error'] > 0].sum()
150/154:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
df_gourd[df_gourd['weight_error'] > 0].agg(sum)
150/155:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
df_gourd[df_gourd['weight_error'] > 0].value_counts()
150/156:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
df_gourd[df_gourd['weight_error'] > 0].count
150/157:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
df_gourd[df_gourd['weight_error'] > 0].count()
150/158:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
(df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
150/159:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
150/160:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
df_gourd[df_gourd['weight_error'] > 0].max()
150/161: df_gourd.head()
156/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import statistics
156/2:
df_gourd = pd.read_csv('data/gourds.csv')
df_gourd.head(5)
156/3:
df_gourd.shape #(28011, 16)
df_gourd.info()
df_gourd.columns
156/4:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
#df_gourd["weight_lbs"].plot(kind='kde')
156/5:
df_gourd["weight_lbs"].plot(kind='kde',
                           ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')
156/6:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
#df_gourd.info() #object 28011

df_gourd['place'] = df_gourd['place'].apply(pd.to_numeric,errors='coerce')
df_gourd['place'].info()
df_gourd.info()
156/7:
plt.figure(figsize = (8,5))
df_gourd['country'].value_counts().head(5).plot(kind='bar', 
                                                title = 'Top 5 countries in gourd dataset', 
                                                ylabel='Number of Appearance in data', 
                                                xlabel='Country')
plt.xticks(rotation = 0, fontsize = 14);
156/8:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)
df_gourd.head()
df_gourd.shape #(28011, 18)
df_gourd.columns
df_gourd.info()
df_gourd.describe()
156/9:
plt.figure(figsize = (8,5))
df_gourd['year'].value_counts().sort_index() #2013 to 2021
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line', 
                                                    title = 'Trend in pumpkin weight over the year', 
                                                    ylabel='Weight of the pumpkin (lbs)', 
                                                    xlabel='Year')
plt.xticks(rotation = 0, fontsize = 14);
156/10:
plt.figure(figsize = (8,5))
df_gourd.groupby(['type'])['weight_lbs'].agg('mean').plot(kind='bar', 
                                                    title = 'Average pumpkin weight among different types', 
                                                    ylabel='Average weight (lbs)', 
                                                    xlabel='TYPE')
plt.xticks(rotation = 0, fontsize = 14);
156/11: df_gourd.plot(kind="scatter", x='est_weight', y='weight_lbs', hue)
156/12:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
156/13: df_gourd.plot(kind="scatter", x='est_weight', y='weight_lbs')
156/14:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
df_gourd['weight_error'].describe()
#df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
156/15:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
print(df_gourd['weight_error'].describe())
#df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
156/16:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
print(df_gourd['weight_error'].describe())
df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
156/17: df_gourd['grower'].value_counts()
156/18: df_gourd['grower_name'].value_counts()
156/19: df_gourd['grower_name'].value_counts().sort_values()
156/20: df_gourd['grower_name'].value_counts().sort_values(ascending = False)
156/21: df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
156/22:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['grower_name'].value_counts()
156/23:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['grower_name'].value_counts().head(10)
156/24:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
#df_gourd.groupby(df_gourd['place'])['grower_name'].value_counts().head(10)
156/25:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['grower_name'].value_counts().head(10)
156/26:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place', 'variety'])['grower_name'].value_counts().head(10)
156/27:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place', 'variety'])['grower_name'].value_counts().head(10)
156/28:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name', 'variety'])['place'].value_counts().head(10)
156/29:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name', 'variety'])['place'].value_counts()
156/30:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name', 'variety'])
156/31:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['variety'])['grower_name'].value_counts()
156/32:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['type'])['grower_name'].value_counts()
156/33: df_gourd['type'].value_counts()
156/34:
dict = { "F": "Field Pumpkin", "P": "Giant Pumpkin", "S": "Giant Squash", "W": "Giant Watermelon", "T": "Tomato", "L": "Long Gourd"}
df_gourd['type']=df_gourd.replace({"type": dict})
df_gourd['type'].value_counts()
156/35:
dict = { "F": "Field Pumpkin", "P": "Giant Pumpkin", "S": "Giant Squash", "W": "Giant Watermelon", "T": "Tomato", "L": "Long Gourd"}
df_gourd['type']=df_gourd['type'].replace({"type": dict})
df_gourd['type'].value_counts()
156/36:
dict = { "F": "Field Pumpkin", "P": "Giant Pumpkin", "S": "Giant Squash", "W": "Giant Watermelon", "T": "Tomato", "L": "Long Gourd"}
df_gourd['type']=df_gourd['type'].replace({'type': dict})
df_gourd['type'].value_counts()
156/37:
dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Long Gourd"}
df_gourd['type']=df_gourd['type'].replace({'type': dict})
df_gourd['type'].value_counts()
156/38:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Long Gourd"}
df_gourd['type']=df_gourd['type'].replace({'type' : { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 
                                                     'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Long Gourd"}})
df_gourd['type'].value_counts()
156/39:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Long Gourd"}
df_gourd['type']=df_gourd['type'].replace({'type' : { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 
                                                     'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Long Gourd"}})
#df_gourd['type'].value_counts()
156/40: #dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Long Gourd"}
156/41:
df_gourd['type']=df['gourd'].replace[{"F": "Field Pumpkin",
    "P": "Giant Pumpkin",
    "S": "Giant Squash",
    "W": "Giant Watermelon",
    "T": "Tomato",
    "L": "Long Gourd"} ]
156/42:
df_gourd['type']=df_gourd['type'].replace[{"F": "Field Pumpkin",
    "P": "Giant Pumpkin",
    "S": "Giant Squash",
    "W": "Giant Watermelon",
    "T": "Tomato",
    "L": "Long Gourd"} ]
156/43:
df_gourd['type']=df_gourd['type'].replace['type' = {"F": "Field Pumpkin",
    "P": "Giant Pumpkin",
    "S": "Giant Squash",
    "W": "Giant Watermelon",
    "T": "Tomato",
    "L": "Long Gourd"} ]
156/44:
df_gourd['type']=df_gourd['type'].replace['type': {"F": "Field Pumpkin",
    "P": "Giant Pumpkin",
    "S": "Giant Squash",
    "W": "Giant Watermelon",
    "T": "Tomato",
    "L": "Long Gourd"} ]
156/45: df_gourd['type']=df_gourd['type'].replace["F": "Field Pumpkin"]
156/46: df_gourd['type']=df_gourd['type'].replace["F": "Field Pumpkin"]
156/47: df_gourd['type']=df_gourd['type.str.replace["F": "Field Pumpkin"]
156/48:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name'])['type'].value_counts()
156/49:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name'])['type'].value_counts().sort_index()
156/50:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name'])['type'].value_counts().sort_index(ascendig = FALSE)
156/51:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name'])['type'].counts()
156/52:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name'])['type'].count()
156/53:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['type'])['grower_name'].count()
156/54:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name', 'type'])['place'].count()
156/55:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name', 'type'])['place'].value_counts()
156/56:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['grower_name'])['place'].count()
156/57:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['type','place'])['grower_name'].count()
156/58:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['grower_name'].count()
156/59:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['grower_name'].value_counts()
156/60:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['grower_name'].value_counts().head(20)
156/61:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place', 'type'])['grower_name'].value_counts()
156/62: df_gourd['type'].str.replace['F' : "Field Pumpkin"]
156/63: df_gourd[df_gourd['type'].str.replace['F' : "Field Pumpkin"]]
156/64:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['grower_name'].value_counts()
156/65:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place']==1)['type', 'grower_name'].value_counts()
156/66:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts()
156/67:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().sort_index()
156/68:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().sort_index(ascending = True)
156/69:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().sort_index(ascending = False)
156/70:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts()
156/71:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().reset_index()
156/72:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().reset_index(name = winner_times)
156/73:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().reset_index(name = winner)
156/74:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().reset_index(name = value_counts)
156/75:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().reset_index(name = 'Winner_number_of_times')
156/76:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
156/77: df_gourd.groupby(df_gourd['grower_name'] == "Jutras, Joe")
156/78: [df_gourd.groupby(df_gourd['grower_name'] == "Jutras, Joe")]
156/79: df_gourd.groupby(df_gourd['grower_name'] == "Jutras, Joe")['weight_lbs'].value_counts()
158/1:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Lon{g Gourd"}

df_goud['type'].replace ({'F': "Field Pumpkin", 
                          'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 
                                                        'T': "Tomato", 'L': "Long Gourd"})
159/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import statistics
159/2:
df_gourd = pd.read_csv('data/gourds.csv')
df_gourd.head(5)
159/3:
df_gourd.shape #(28011, 16)
df_gourd.info()
df_gourd.columns
159/4:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
#df_gourd["weight_lbs"].plot(kind='kde')
159/5:
df_gourd["weight_lbs"].plot(kind='kde',
                           ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')
159/6:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
#df_gourd.info() #object 28011

df_gourd['place'] = df_gourd['place'].apply(pd.to_numeric,errors='coerce')
df_gourd['place'].info()
df_gourd.info()
159/7:
plt.figure(figsize = (8,5))
df_gourd['country'].value_counts().head(5).plot(kind='bar', 
                                                title = 'Top 5 countries in gourd dataset', 
                                                ylabel='Number of Appearance in data', 
                                                xlabel='Country')
plt.xticks(rotation = 0, fontsize = 14);
159/8:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)
df_gourd.head()
df_gourd.shape #(28011, 18)
df_gourd.columns
df_gourd.info()
df_gourd.describe()
159/9:
plt.figure(figsize = (8,5))
df_gourd['year'].value_counts().sort_index() #2013 to 2021
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line', 
                                                    title = 'Trend in pumpkin weight over the year', 
                                                    ylabel='Weight of the pumpkin (lbs)', 
                                                    xlabel='Year')
plt.xticks(rotation = 0, fontsize = 14);
159/10: df_gourd['type'].value_counts()
159/11: df_gourd[df_gourd['type'].replace['F' : "Field Pumpkin"]]
159/12:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Lon{g Gourd"}

df_goud['type'].replace ({'F': "Field Pumpkin", 
                          'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 
                                                        'T': "Tomato", 'L': "Long Gourd"})
159/13:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Lon{g Gourd"}

df_gourd['type'].replace ({'F': "Field Pumpkin", 
                          'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 
                                                        'T': "Tomato", 'L': "Long Gourd"})
159/14:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Lon{g Gourd"}

df['type'] = df_gourd['type'].replace ({'F': "Field Pumpkin", 
                          'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 
                                                        'T': "Tomato", 'L': "Long Gourd"})
159/15:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Lon{g Gourd"}

df_gourd['type'] = df_gourd['type'].replace ({'F': "Field Pumpkin", 
                          'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 
                                                        'T': "Tomato", 'L': "Long Gourd"})
159/16:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Lon{g Gourd"}

df_gourd['type'] = df_gourd['type'].replace ({'F': "Field Pumpkin", 
                          'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 
                                                        'T': "Tomato", 'L': "Long Gourd"})
df_gourd['type'].value_counts()
159/17:
plt.figure(figsize = (8,5))
df_gourd.groupby(['type'])['weight_lbs'].agg('mean').plot(kind='bar', 
                                                    title = 'Average pumpkin weight among different types', 
                                                    ylabel='Average weight (lbs)', 
                                                    xlabel='TYPE')
plt.xticks(rotation = 0, fontsize = 14);
159/18:
plt.figure(figsize = (8,5))
df_gourd.groupby(['type'])['weight_lbs'].agg('mean').plot(kind='bar', 
                                                    title = 'Average pumpkin weight among different types', 
                                                    ylabel='Average weight (lbs)', 
                                                    xlabel='TYPE')
plt.xticks(rotation = 45, fontsize = 14);
159/19:
plt.figure(figsize = (8,5))
df_gourd.groupby(['type'])['weight_lbs'].agg('mean').plot(kind='box', 
                                                    title = 'Average pumpkin weight among different types', 
                                                    ylabel='Average weight (lbs)', 
                                                    xlabel='TYPE')
plt.xticks(rotation = 45, fontsize = 14);
159/20:
plt.figure(figsize = (8,5))
df_gourd.groupby(['type'])['weight_lbs'].agg('mean').plot(kind='bar', 
                                                    title = 'Average pumpkin weight among different types', 
                                                    ylabel='Average weight (lbs)', 
                                                    xlabel='TYPE')
plt.xticks(rotation = 45, fontsize = 14);
159/21: df_gourd.plot(kind="scatter", x='est_weight', y='weight_lbs')
159/22:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
print(df_gourd['weight_error'].describe())
df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
159/23:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/24: df_gourd.groupby(df_gourd['grower_name'] == "Jutras, Joe")['weight_lbs'].value_counts()
159/25:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place', 'type'])['grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/26:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place'])['type', 'grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/27:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['place', 'type'])['grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/28: df_gourd.groupby(df_gourd['place', 'type'])
159/29: df_gourd['place', 'type'])
159/30: df_gourd['place', 'type']
159/31: df_gourd[['place', 'type']]
159/32:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd[['place', 'type']])['grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/33:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(['place', 'type'])['grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/34: df_gourd.groupby(df_gourd['grower_name'] == "Jutras, Joe")['weight_lbs'].value_counts()
159/35: plt.boxplot(data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean'))
159/36: plt.boxplot(x = df_gourd.groupby(['type'])['weight_lbs'].agg('mean'))
159/37:
plt.boxplot(data = df_gourds,
            x = df_gourd['type'],
            y = df_gourd.groupby(['type'])['weight_lbs'].agg('mean'))
159/38:
plt.boxplot(data = df_gourd,
            x = df_gourd['type'],
            y = df_gourd.groupby(['type'])['weight_lbs'].agg('mean'))
159/39:
plt.boxplot(data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean'),
            x = df_gourd['type'])
159/40: data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean')
159/41:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean')
data
159/42:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name=mean)
data
sns.boxplot(data=data, x="type", y="class", hue="alive")
159/43:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data
sns.boxplot(data=data, x="type", y="class", hue="alive")
159/44:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data
#sns.boxplot(data=data, x="type", y="class", hue="alive")
159/45:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data
sns.boxplot(data=data, x="type", y="lbs", hue="type")
159/46:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data
sns.boxplot(data=data, x="type", y="Average_weight_lbs", hue="type")
159/47:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data
sns.boxplot(data=df_gourds, x="type", y="weight_lbs")
159/48:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data
sns.boxplot(data=df_gourd, x="type", y="weight_lbs")
159/49:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data

plt.figure(figsize = (8,5))
sns.boxplot(data=df_gourd, x="type", y="weight_lbs")
159/50:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data

plt.figure(figsize = (8,5))
sns.boxplot(data=df_gourd, x="type", y="weight_lbs") 
plt.xticks(rotation = 45, fontsize = 14);
159/51:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data

plt.figure(figsize = (8,5))
plt.boxplot(data=df_gourd, x="type", y="weight_lbs") 
plt.xticks(rotation = 45, fontsize = 14);
159/52:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data

plt.figure(figsize = (8,5))
sns.boxplot(data=df_gourd, x="type", y="weight_lbs") 
plt.xticks(rotation = 45, fontsize = 14);
159/53:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby(df_gourd['type'] & df_gourd['place'] < 10)['grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/54:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd['type']) & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/55:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd.groupby[(df_gourd['type']) & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/56:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd(df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/57:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd([df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/58:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/59:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd['place'] < 10)]['type', 'grower_name'].value_counts().reset_index(name = 'Winner_number_of_times').head(15)
159/60:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd['place'] < 10)]['type', 'grower_name'].reset_index(name = 'Winner_number_of_times').head(15)
159/61:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd['place'] < 10)]['grower_name'].reset_index(name = 'Winner_number_of_times').head(15)
159/62:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd[type]), (df_gourd['place'] < 10)]['grower_name'].reset_index(name = 'Winner_number_of_times').head(15)
159/63:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd[type] == 'Field Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].reset_index(name = 'Winner_number_of_times').head(15)
159/64:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd[type] == 'Field Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].head(15)
159/65:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd['type'] == 'Field Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].head(15)
159/66:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd['type'] == 'Field Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
159/67: df_gourd[(df_gourd['type'] == 'Field Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
159/68: df_gourd[(df_gourd['type'] == 'Giant Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
159/69: df_top10 = df_gourd[df_gourd['place'] < 10)]
159/70: df_top10 = df_gourd[(df_gourd['place'] < 10)]
159/71:
df_top10 = df_gourd[(df_gourd['place'] < 10)]
df_top10.head()
159/72:
df_top10 = df_gourd[(df_gourd['place'] < 10)]
df_top10.head()
df_top10.shape
159/73:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(488, 19)
159/74:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(542, 19)
159/75:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(542, 19)
df_top10.groupby(df_top10['type'])['grower_name'].value_counts()
159/76:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(542, 19)
df_top10.groupby(df_top10['grower_name'])['type'].value_counts()
159/77:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(542, 19)
df_top10.groupby(df_top10['type'])['grower_name'].value_counts()>reset_index(name = 'count').head(20)
159/78:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(542, 19)
df_top10.groupby(df_top10['type'])['grower_name'].value_counts().reset_index(name = 'count').head(20)
159/79:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(542, 19)
df_top10.groupby(df_top10['type'])['grower_name'].value_counts().reset_index(name = 'count').head(50)
162/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import statistics
162/2:
df_gourd = pd.read_csv('data/gourds.csv')
df_gourd.head(5)
162/3:
df_gourd.shape #(28011, 16)
df_gourd.info()
df_gourd.columns
162/4:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
#df_gourd["weight_lbs"].plot(kind='kde')
162/5:
df_gourd["weight_lbs"].plot(kind='kde',
                           ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')
162/6:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
#df_gourd.info() #object 28011

df_gourd['place'] = df_gourd['place'].apply(pd.to_numeric,errors='coerce')
df_gourd['place'].info()
df_gourd.info()
162/7:
plt.figure(figsize = (8,5))
df_gourd['country'].value_counts().head(5).plot(kind='bar', 
                                                title = 'Top 5 countries in gourd dataset', 
                                                ylabel='Number of Appearance in data', 
                                                xlabel='Country')
plt.xticks(rotation = 0, fontsize = 14);
162/8:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)
df_gourd.head()
df_gourd.shape #(28011, 18)
df_gourd.columns
df_gourd.info()
df_gourd.describe()
162/9:
plt.figure(figsize = (8,5))
df_gourd['year'].value_counts().sort_index() #2013 to 2021
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line', 
                                                    title = 'Trend in pumpkin weight over the year', 
                                                    ylabel='Weight of the pumpkin (lbs)', 
                                                    xlabel='Year')
plt.xticks(rotation = 0, fontsize = 14);
162/10: df_gourd['type'].value_counts()
162/11:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Lon{g Gourd"}

df_gourd['type'] = df_gourd['type'].replace ({'F': "Field Pumpkin", 
                          'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 
                                                        'T': "Tomato", 'L': "Long Gourd"})
df_gourd['type'].value_counts()
162/12:
plt.figure(figsize = (8,5))
df_gourd.groupby(['type'])['weight_lbs'].agg('mean').plot(kind='bar', 
                                                    title = 'Average pumpkin weight among different types', 
                                                    ylabel='Average weight (lbs)', 
                                                    xlabel='TYPE')
plt.xticks(rotation = 45, fontsize = 14);
162/13:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data

plt.figure(figsize = (8,5))
sns.boxplot(data=df_gourd, x="type", y="weight_lbs") 
plt.xticks(rotation = 45, fontsize = 14);
162/14: df_gourd.plot(kind="scatter", x='est_weight', y='weight_lbs')
162/15:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
print(df_gourd['weight_error'].describe())
df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
162/16: df_gourd[['place', 'type']]
162/17:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd['type'] == 'Field Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
162/18: df_gourd[(df_gourd['type'] == 'Giant Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
162/19:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(542, 19)
df_top10.groupby(df_top10['type'])['grower_name'].value_counts().reset_index(name = 'count').head(50)
162/20: df_gourd.groupby(df_gourd['grower_name'] == "Jutras, Joe")['weight_lbs'].value_counts()
162/21: df_gourd.head()
162/22: df_gourd[['place', 'type']]['grower_name'].value_counts()
162/23: df_gourd[['place', 'type']]
162/24: df_gourd[(df_gourd['type']) & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
162/25: df_gourd.groupby[(df_gourd['type']) & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
162/26: df_gourd[(df_gourd['type'] == 'Giant Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
162/27:
df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['type','place'])['grower_name'].idxmax()].reset_index(drop=True)
162/28:
df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['type','place'])['grower_name'].counts()].reset_index(drop=True)
162/29:
df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['type','place'])['grower_name'].count()].reset_index(drop=True)
162/30:
df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['type','place'])['grower_name'].value_counts()].reset_index(drop=True)
162/31:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['type','place'])['grower_name'].value_counts()].reset_index(drop=True)
162/32:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True)
162/33:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)
162/34:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df2 = df_gourd.groupby(['type','place'])['grower_name'].agg('count').reset_index()
print(df2)
162/35:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df2 = df_gourd.groupby(['type','place'])['grower_name'].agg('count').reset_index()
df2.head()
162/36:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df2 = df_gourd.groupby(['type','grower_name'])['place'].agg('count').reset_index()
df2.head()
162/37:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df2 = df_gourd.groupby(['place'] <=10 & ['type'])['grower_name'].agg('count').reset_index()
df2.head()
162/38:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df2 = df_gourd.groupby((['place'] <=10) & ['type'])['grower_name'].agg('count').reset_index()
df2.head()
162/39:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['type'])['type'].count()
162/40:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['type'])['grower_name'].count()
162/41:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['type'])['grower_name'].nunquie()
162/42:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['type'])['grower_name'].nunique()
162/43:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['type', 'place'])['grower_name'].nunique()
162/44:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['type', 'place', 'grower_name'])['grower_name'].nunique()
162/45:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'place'])['type'].nunique()
162/46:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'place'])['type'].nunique().tail()
162/47:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'place', 'type'])['place'].count()
162/48:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'place', 'type'])['place'].count().max()
162/49:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])['place'].count().max()
162/50:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])['place'].count()
162/51:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])['place'].count().sort_value(ascending = False)
162/52:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])['place'].count().sort_values(ascending = False)
162/53:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])['place'].count().sort_values(ascending = False).head(20)
162/54:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])[['place']<=10].count().sort_values(ascending = False).head(20)
162/55:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])[['place']].count().sort_values(ascending = False).head(20)
162/56:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])['place'].count().sort_values(ascending = False).head(20)
162/57:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])['place'].count().sort_values(ascending = False).head(40)
162/58:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])['place'].count().sort_values(ascending = False).tail(40)
162/59:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])['place'].count().sort_values(ascending = False).tail(100)
162/60:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['grower_name', 'type'])['place'].count().sort_values(ascending = False).head(50)
162/61:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['type', 'grower_name'])['place'].count().sort_values(ascending = False).head(50)
162/62:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(542, 19)
df_top10.groupby(df_top10['type'])['grower_name'].value_counts().reset_index(name = 'count').head(10)
162/63: df_gourd['pollinator_father'].vaue_counts().reset_index(name="count_father")
162/64: df_gourd['pollinator_father'].value_counts().reset_index(name="count_father")
163/1: df_gourd['pollinator_mother'].value_counts().reset_index(name="count_mother")
164/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import statistics
164/2:
df_gourd = pd.read_csv('data/gourds.csv')
df_gourd.head(5)
164/3:
df_gourd.shape #(28011, 16)
df_gourd.info()
df_gourd.columns
164/4:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
#df_gourd["weight_lbs"].plot(kind='kde')
164/5:
df_gourd["weight_lbs"].plot(kind='kde',
                           ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')
164/6:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
#df_gourd.info() #object 28011

df_gourd['place'] = df_gourd['place'].apply(pd.to_numeric,errors='coerce')
df_gourd['place'].info()
df_gourd.info()
164/7:
plt.figure(figsize = (8,5))
df_gourd['country'].value_counts().head(5).plot(kind='bar', 
                                                title = 'Top 5 countries in gourd dataset', 
                                                ylabel='Number of Appearance in data', 
                                                xlabel='Country')
plt.xticks(rotation = 0, fontsize = 14);
164/8:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)
df_gourd.head()
df_gourd.shape #(28011, 18)
df_gourd.columns
df_gourd.info()
df_gourd.describe()
164/9:
plt.figure(figsize = (8,5))
df_gourd['year'].value_counts().sort_index() #2013 to 2021
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line', 
                                                    title = 'Trend in pumpkin weight over the year', 
                                                    ylabel='Weight of the pumpkin (lbs)', 
                                                    xlabel='Year')
plt.xticks(rotation = 0, fontsize = 14);
164/10: df_gourd['type'].value_counts()
164/11:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Lon{g Gourd"}

df_gourd['type'] = df_gourd['type'].replace ({'F': "Field Pumpkin", 
                          'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 
                                                        'T': "Tomato", 'L': "Long Gourd"})
df_gourd['type'].value_counts()
164/12:
plt.figure(figsize = (8,5))
df_gourd.groupby(['type'])['weight_lbs'].agg('mean').plot(kind='bar', 
                                                    title = 'Average pumpkin weight among different types', 
                                                    ylabel='Average weight (lbs)', 
                                                    xlabel='TYPE')
plt.xticks(rotation = 45, fontsize = 14);
164/13:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data

plt.figure(figsize = (8,5))
sns.boxplot(data=df_gourd, x="type", y="weight_lbs") 
plt.xticks(rotation = 45, fontsize = 14);
164/14: df_gourd.plot(kind="scatter", x='est_weight', y='weight_lbs')
164/15:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
print(df_gourd['weight_error'].describe())
df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
164/16:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['type', 'grower_name'])['place'].count().sort_values(ascending = False).head(50)
164/17:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd['type'] == 'Field Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
164/18: df_gourd[(df_gourd['type'] == 'Giant Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
164/19:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(542, 19)
df_top10.groupby(df_top10['type'])['grower_name'].value_counts().reset_index(name = 'count').head(10)
164/20: df_gourd.groupby(df_gourd['grower_name'] == "Jutras, Joe")['weight_lbs'].value_counts()
164/21: df_gourd['pollinator_father'].value_counts().reset_index(name="count_father")
164/22: df_gourd['pollinator_mother'].value_counts().reset_index(name="count_mother")
164/23: df_gourd['seed_mother'].value_counts().reset_index(name="count_mother")
164/24: off1 = df_gourd['pollinator_father'].value_counts().reset_index(name="count_father")
164/25: off2 = df_gourd['seed_mother'].value_counts().reset_index(name="count_mother")
164/26: pd.merge(off1, off2, how = 'inner')
164/27:
off1 = df_gourd['pollinator_father'].value_counts().reset_index(name="count_father")
off1.shape
164/28:
off2 = df_gourd['seed_mother'].value_counts().reset_index(name="count_mother")
off2.shape
164/29: pd.merge(off1, off2, how = 'inner').shape
164/30:
pd.merge(off1, off2, how = 'outer')

.shape #inner 2320, 3
164/31:
pd.merge(off1, off2, how = 'outer')

 #inner 2320, 3
164/32:
off2 = df_gourd['seed_mother'].value_counts().reset_index(name="count_mother")
off2.shape

off2.head(20)
164/33:
off1 = df_gourd['pollinator_father'].value_counts().reset_index(name="count_father")
off1.shape
off1.head(20)
164/34: pd.merge(off1, off2, how = 'inner')
164/35:
off_mer = pd.merge(off1, off2, how = 'outer')

 #inner 2320, 3; outer 12101, 3.
164/36:
off_mer_out = pd.merge(off1, off2, how = 'outer')


 #inner 2320, 3; outer 12101, 3.
164/37:
off_mer_out = pd.merge(off1, off2, how = 'outer')
off_mer_out.group_by([count_father] > 100).count()

 #inner 2320, 3; outer 12101, 3.
164/38:
off_mer_out = pd.merge(off1, off2, how = 'outer')
off_mer_out.group_by(off_mer_out[count_father] > 100).count()

 #inner 2320, 3; outer 12101, 3.
164/39:
off_mer_out = pd.merge(off1, off2, how = 'outer')
off_mer_out.groupby(off_mer_out[count_father] > 100).count()

 #inner 2320, 3; outer 12101, 3.
164/40:
off_mer_out = pd.merge(off1, off2, how = 'outer')
off_mer_out.groupby(off_mer_out[count_father] > 100).count()
off_mer_out.head()
 #inner 2320, 3; outer 12101, 3.
164/41:
off_mer_out = pd.merge(off1, off2, how = 'outer')
#off_mer_out.groupby(off_mer_out[count_father] > 100).count()
off_mer_out.head()
 #inner 2320, 3; outer 12101, 3.
164/42:
off_mer_out = pd.merge(off1, off2, how = 'outer')
off_mer_out.groupby(off_mer_out[count_father] > 100).count()
#off_mer_out.head()
 #inner 2320, 3; outer 12101, 3.
164/43:
off_mer_out = pd.merge(off1, off2, how = 'outer')
off_mer_out.groupby(off_mer_out[count_father] > 100)['count_father'].count()
#off_mer_out.head()
 #inner 2320, 3; outer 12101, 3.
164/44:
off_mer_out = pd.merge(off1, off2, how = 'outer')
off_mer_out.groupby(off_mer_out['count_father'] > 100)['count_father'].count()
#off_mer_out.head()
 #inner 2320, 3; outer 12101, 3.
164/45:
off_mer_out = pd.merge(off1, off2, how = 'outer')
off_mer_out.groupby(off_mer_out['count_father'] > 100)['count_father', 'count_mother'].count()
#off_mer_out.head()
 #inner 2320, 3; outer 12101, 3.
164/46: off_mer_out.groupby(off_mer_out['count_mother'] > 100)['count_father', 'count_mother'].count()
164/47:
off_mer_inn = pd.merge(off1, off2, how = 'inner')
off_mer_inn.groupby(off_mer_out['count_father'] > 100)['count_father', 'count_mother'].count()
164/48:
off1 = df_gourd['pollinator_father'].value_counts().reset_index(name="count_father")
off1.shape
off1.head(20)
off1.sort_index()
164/49:
off_mer_out = pd.merge(off1, off2, how = 'outer')
off_mer_out.groupby(off_mer_out['count_father'] > 100)['count_father', 'count_mother'].count()
off_mer_out.sort_index()
 #inner 2320, 3; outer 12101, 3.
164/50:
off_mer_inn = pd.merge(off1, off2, how = 'inner')
off_mer_inn.groupby(off_mer_out['count_father'] > 100)['count_father', 'count_mother'].count()
off_mer_inn.sort_index()
164/51:
off_mer_inn = pd.merge(off1, off2, how = 'inner')
off_mer_inn.groupby(off_mer_out['count_father'] > 100)['count_father', 'count_mother'].count()
off_mer_inn['count_mother'].sort_index()
164/52:
off_mer_inn = pd.merge(off1, off2, how = 'inner')
off_mer_inn.groupby(off_mer_out['count_father'] > 100)['count_father', 'count_mother'].count()
164/53:
off_mer_out.groupby(off_mer_out['count_mother'] > 100)['count_father', 'count_mother'].count()
off_mer_out.sort_index()
164/54:
off_mer_out = pd.merge(off1, off2)
off_mer_out.groupby(off_mer_out['count_father'] > 100)['count_father', 'count_mother'].count()
off_mer_out.sort_index()
 #inner 2320, 3; outer 12101, 3.
164/55: off_mer_out.loc(['count_mother']>100).count()
164/56: off_mer_out.loc[off_mer_out['count_mother'] > 100]['count_mother'].count()
164/57: off_mer_out.loc[off_mer_out['count_mother'] > 100]['count_father'].count()
164/58: off_mer_in.loc[off_mer_in['count_mother'] > 100]['count_father'].count()
164/59: off_mer_inn.loc[off_mer_inn['count_mother'] > 100]['count_father'].count()
164/60: off_mer_inn.loc[off_mer_inn['count_mother'] > 100]['count_father']
164/61: [off_mer_inn.loc[off_mer_inn['count_mother'] > 100]['count_father']]
164/62: off_mer_inn.loc[off_mer_inn['count_mother'] > 100]['count_father'].count()
164/63: off_mer_inn.loc[off_mer_inn['count_father'] > 100]['count_father'].count()
164/64: off_mer_inn.loc[off_mer_inn['count_mother'] > 100 & off_mer_inn['count_father'] > 100]['count_father'].count()
164/65: off_mer_inn.loc([off_mer_inn['count_mother'] > 100] & [off_mer_inn['count_father'] > 100])['count_father'].count()
164/66: off_mer_inn.loc[off_mer_inn['count_mother'] > 100]['count_father'].count()
166/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import statistics
166/2:
df_gourd = pd.read_csv('data/gourds.csv')
df_gourd.head(5)
166/3:
df_gourd.shape #(28011, 16)
df_gourd.info()
df_gourd.columns
166/4:
plt.figure(figsize = (8,5))
df_gourd['weight_lbs'].plot(kind='hist', edgecolor='black', 
                            ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')


plt.xticks(rotation = 0, fontsize = 14);
#df_gourd["weight_lbs"].plot(kind='kde')
166/5:
df_gourd["weight_lbs"].plot(kind='kde',
                           ylabel='Frequency',
                            xlabel='Weight of pumpkins in lbs',
                           title = ' Distribution of the weight of pumpkin in the gourd dataset')
166/6:
df_gourd['place'].value_counts() #EXH=1862, DMG=465
#df_gourd.info() #object 28011

df_gourd['place'] = df_gourd['place'].apply(pd.to_numeric,errors='coerce')
df_gourd['place'].info()
df_gourd.info()
166/7:
plt.figure(figsize = (8,5))
df_gourd['country'].value_counts().head(5).plot(kind='bar', 
                                                title = 'Top 5 countries in gourd dataset', 
                                                ylabel='Number of Appearance in data', 
                                                xlabel='Country')
plt.xticks(rotation = 0, fontsize = 14);
166/8:
df_gourd[['year','type']] = df_gourd['id'].str.split("-", expand=True)
df_gourd.head()
df_gourd.shape #(28011, 18)
df_gourd.columns
df_gourd.info()
df_gourd.describe()
166/9:
plt.figure(figsize = (8,5))
df_gourd['year'].value_counts().sort_index() #2013 to 2021
df_gourd.groupby(['year'])['weight_lbs'].max().plot(kind='line', 
                                                    title = 'Trend in pumpkin weight over the year', 
                                                    ylabel='Weight of the pumpkin (lbs)', 
                                                    xlabel='Year')
plt.xticks(rotation = 0, fontsize = 14);
166/10: df_gourd['type'].value_counts()
166/11:
#dict = { 'F': "Field Pumpkin", 'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 'T': "Tomato", 'L': "Lon{g Gourd"}

df_gourd['type'] = df_gourd['type'].replace ({'F': "Field Pumpkin", 
                          'P': "Giant Pumpkin", 'S': "Giant Squash", 'W': "Giant Watermelon", 
                                                        'T': "Tomato", 'L': "Long Gourd"})
df_gourd['type'].value_counts()
166/12:
plt.figure(figsize = (8,5))
df_gourd.groupby(['type'])['weight_lbs'].agg('mean').plot(kind='bar', 
                                                    title = 'Average pumpkin weight among different types', 
                                                    ylabel='Average weight (lbs)', 
                                                    xlabel='TYPE')
plt.xticks(rotation = 45, fontsize = 14);
166/13:
data = df_gourd.groupby(['type'])['weight_lbs'].agg('mean').reset_index(name= 'Average_weight_lbs')
data

plt.figure(figsize = (8,5))
sns.boxplot(data=df_gourd, x="type", y="weight_lbs") 
plt.xticks(rotation = 45, fontsize = 14);
166/14: df_gourd.plot(kind="scatter", x='est_weight', y='weight_lbs')
166/15:
df_gourd['weight_error'] = df_gourd['est_weight']-df_gourd['weight_lbs']
print(df_gourd['weight_error'].describe())
df_gourd[df_gourd['weight_error'] > 0].count()/len(df_gourd.axes[0])*100
166/16:
#df_gourd[['place', 'type']]
df_gourd.loc[df_gourd.groupby(['grower_name','place'])['type'].value_counts()].reset_index(drop=True).tail(20)

df_gourd.groupby(['type', 'grower_name'])['place'].count().sort_values(ascending = False).head(50)
166/17:
df_gourd['grower_name'].value_counts().sort_values(ascending = False).head(10)
df_gourd[(df_gourd['type'] == 'Field Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
166/18: df_gourd[(df_gourd['type'] == 'Giant Pumpkin') & (df_gourd['place'] < 10)]['grower_name'].value_counts().reset_index(name="counts").head(15)
166/19:
df_top10 = df_gourd[(df_gourd['place'] <= 10)]
df_top10.head()
df_top10.shape #(542, 19)
df_top10.groupby(df_top10['type'])['grower_name'].value_counts().reset_index(name = 'count').head(10)
166/20: df_gourd.groupby(df_gourd['grower_name'] == "Jutras, Joe")['weight_lbs'].value_counts()
166/21:
off1 = df_gourd['pollinator_father'].value_counts().reset_index(name="count_father")
off1.shape
off1.head(20)
off1.sort_index()
166/22:
off2 = df_gourd['seed_mother'].value_counts().reset_index(name="count_mother")
off2.shape

off2.head(20)
166/23:
off_mer_out = pd.merge(off1, off2)
off_mer_out.groupby(off_mer_out['count_father'] > 100)['count_father', 'count_mother'].count()
off_mer_out.sort_index()
 #inner 2320, 3; outer 12101, 3.
166/24:
off_mer_out.groupby(off_mer_out['count_mother'] > 100)['count_father', 'count_mother'].count()
off_mer_out.sort_index()
166/25:
off_mer_inn = pd.merge(off1, off2, how = 'inner')
off_mer_inn.groupby(off_mer_out['count_father'] > 100)['count_father', 'count_mother'].count()
166/26: off_mer_inn.loc[off_mer_inn['count_mother'] > 100]['count_father'].count()
166/27: off_mer_inn.loc[off_mer_inn['count_father'] > 100]['count_father'].count()
166/28: off_mer_inn.loc[off_mer_inn['count_mother'] > 100]['count_father'].count()
166/29: df_top10.loc[df_top10['grower_name'] == "Treece, Jef"]['type'].count()
166/30: df_top10.loc[df_top10['grower_name'] == "Treece, Jef"]['type'].value_count()
166/31: df_top10.loc[df_top10['grower_name'] == "Treece, Jef"]['type'].value_counts()
166/32: df_top10.loc[df_top10['grower_name'] == "Treece, Jef"]['type'].nunique()
166/33: df_top10.loc[df_top10['grower_name']]['type'].nunique()
166/34: df_top10.groupby[df_top10['grower_name']]['type'].nunique()
166/35: df_top10.groupby('grower_name')['type'].nunique()
166/36: df_top10.groupby('grower_name')['type'].nunique().sort_values()
166/37: df_top10.groupby('grower_name')['type'].nunique().sort_values(ascendng = False)
166/38: df_top10.groupby('grower_name')['type'].nunique().sort_values(ascending = False)
166/39:
off_mer_out.groupby(off_mer_out['count_mother'] > 100)['count_father', 'count_mother'].count()
off_mer_out.sort_index()
off_mer_out['agg'] = off_mer_out['count_father'] + off_mer_out['count_mother']
166/40:
off_mer_out.groupby(off_mer_out['count_mother'] > 100)['count_father', 'count_mother'].count()
off_mer_out.sort_index()
off_mer_out['agg'] = off_mer_out['count_father'] + off_mer_out['count_mother']
off_mer_out.sort_index()
166/41: off_mer_out.groupby(['agg'] >= 100).count()
166/42: off_mer_out.groupby(['agg'] >= 100)['agg'].count()
166/43: off_mer_out.groupby(off_mer_out['agg'] >= 100)['agg'].count()
166/44: df_mmg = df_gourd.loc(df_gourd['grower_name'] == "Martin, Margaret & Glen")
166/45: df_mmg = df_gourd.groupby(df_gourd['grower_name'] == "Martin, Margaret & Glen")
166/46:
df_mmg = df_gourd.groupby(df_gourd['grower_name'] == "Martin, Margaret & Glen")
df_mmg
166/47:
df_mmg = df_gourd.groupby(df_gourd['grower_name'] == "Martin, Margaret & Glen")
df_mmg.shape
166/48:
df_mmg = df_gourd.groupby[(df_gourd['grower_name'] == "Martin, Margaret & Glen")]
df_mmg.shape
166/49:
df_mmg = df_gourd.groupby(df_gourd['grower_name'] == "Martin, Margaret & Glen")
df_mmg.head()
168/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
169/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
169/2:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN07&spec=N'  #URL of the webpage

response = requests.get(URL)  #creating a response object pulling the URL
type(response)       # it is a requests.status.models.Response
response.status_code  #200 not broke read properly
response.text         # helps open the object to see it looks
soup = BS(response.text) #converts the response object in Beautiful object
#print(soup.prettify()) #to visualize how the conversion looks like and be able to read the class, tab etc where are the containers
soup.find('title')
169/3:
#Finding all the members in the list item and 
list_item1 = soup.find('div', attrs={'class' : 'Members--list-item'}).text
list_item1
169/4:
list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
list_item1
169/5:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_item1 in soup.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_item1 = list_item1.text

    list_item1 = list_item1.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_item1)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_item1)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_item1)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_item1)[1])
    
    x = re.search(r"\bWinner", list_item1)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_item1)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)
169/6:
df_TN7 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
pd.DataFrame(df_TN7)
169/7:
state = 'TN'
d_num = 1

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
169/8:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN1 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_01 = pd.DataFrame(df_TN1)
TN_01.head()
169/9:

d = soupX.find('ol',attrs = {'class':'SubNav-items'}) 
a_tags = d.find_all('a')
a_tags

d_count = 0                               #Start counting districts in state
for i in a_tags:
    if str.isdigit(i.text.strip()[-1]):   #if the last substring is a digit
        d_count+=1                        #increase district count by 1
    #print(i.text.strip()[-1])
    
d_count
169/10:
state = 'TN'

count = 1

Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []



while count <= d_count:     #for each district number
    url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(count)+ '&spec=N'   #here's the url
    r = requests.get(url)    
    #soup = BS(r.text,'html.parser')
    soupX = BS(r.text)
    
    print(url)
    
    Name = []
    Party = []
    pct_votes = []
    Raised = []
    Spent = []
    Winner = []
    Incumbent = []
    
    for list_itemY in soupX.find('div', attrs={'class' : 'Members--list-item'}):
    
        list_itemY = list_itemY.text

        list_itemY = list_itemY.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")    

        type(list_itemY)
        
    count+=1 # increment count
169/11:
xyz = soupX.find('div', attrs={'class' : 'Members--list-item'})
xyz = (xyz.text).replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
Name = re.search(r"(.+)\(([A-Z])\)", xyz)[1]
type(xyz)
xyz
Name
169/12:
URL = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+str(d_num)+'&spec=N'
print(URLOX)
169/13:
state = 'TN'
d_num = 2

#this URL is different for races/summary we need the races/candidates instead for picking the candidates
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id='+str(state)+'0'+str(d_num)+'&spec=N'  #URL of the webpage
#URLX = 'https://www.opensecrets.org/races/summary?cycle=2022&id=TN02&spec=N'

#URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN01&spec=N'
URLOX1 = 'https://www.opensecrets.org/races/candidates?cycle=2020&id=TN06&spec=N'

URLOX = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+str(state)+'0'+str(d_num)+'&spec=N'
print(URLOX)

responseX = requests.get(URLOX)  #creating a response object pulling the URL
type(responseX)       # it is a requests.status.models.Response

responseX.status_code  #200 not broke read properly
responseX.text         # helps open the object to see it looks
soupX = BS(responseX.text, 'html.parser') #converts the response object in Beautiful object
#print(soup.prettify())
soupX.find('title')

list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
#list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

list_itemX
169/14:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []


for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
    list_itemX = list_itemX.text

    list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
    
    
    Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
    Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])
    
    pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])
    
    Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])
    
    Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])
    
    x = re.search(r"\bWinner", list_itemX)  
    #the search makes the word match so it stops if no match so that is why weused the append YEs or No
    if x:
        Winner.append("Yes")
    else:
        Winner.append("No")
        
    y = re.search(r"\bIncumbent", list_itemX)

    if y:
      Incumbent.append("Yes")
    else:
       Incumbent.append("No") 
print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent)

df_TN2 = {'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_02 = pd.DataFrame(df_TN1)
TN_02.head()
169/15:
Name = []
Party = []
pct_votes = []
Raised = []
Spent = []
Winner = []
Incumbent = []
ST = []
ST_DIT = []


for key, value in state.items():
    for i in range(1, value+1):
        #print(str(key) + " " + str(i))
        url = 'https://www.opensecrets.org/races/candidates?cycle=2020&id='+ key+ f"{i:02}"+'&spec=N'
        #print(url)
        
        res1 = requests.get(url)  #creating a response object pulling the URL
        type(res1)       # it is a requests.status.models.Response

        res1.status_code  #200 not broke read properly
        res1.text         # helps open the object to see it looks
        soupX = BS(res1.text, 'html.parser') #converts the response object in Beautiful object
        #print(soup.prettify())
        soupX.find('title')

        #list_itemX = soupX.find('div', attrs={'class' : 'Members--list-item'}).text
        #list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")

        #print(list_itemX)
        for list_itemX in soupX.findAll('div', attrs={'class' : 'Members--list-item'}):
    
            list_itemX = list_itemX.text

            list_itemX = list_itemX.replace("\n", " ").replace(",", "").replace("\t", " ").replace("•", "")
            
            ST.append(key)
            ST_DIT.append(f"{i:02}")
    
    
            Name.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[1])
   
            Party.append(re.search(r"(.+)\(([A-Z])\)", list_itemX)[2])

            pct_votes.append(re.search("\d+\.\d+\%", list_itemX)[0])

            Raised.append(re.search(r"\bRaised\b\:\W\$(\d*)", list_itemX)[1])

            Spent.append(re.search(r"\bSpent\b\:\W\$(\d*)", list_itemX)[1])

            x = re.search(r"\bWinner", list_itemX)  
            #the search makes the word match so it stops if no match so that is why weused the append YEs or No
            if x:
                Winner.append("Yes")
            else:
                Winner.append("No")

            y = re.search(r"\bIncumbent", list_itemX)

            if y:
              Incumbent.append("Yes")
            else:
               Incumbent.append("No") 
#print(Name, Party, pct_votes, Raised, Spent, Winner, Incumbent, ST, ST_DIT)

dic_list = {'State':ST, 'District':ST_DIT, 'CandidateName': Name,'Party': Party,  'PercentVotes' : pct_votes, 'FundsRaised': Raised, 'MoneySpent':Spent, 
          'Winner': Winner, 'Incumbent': Incumbent}
TN_CA = pd.DataFrame(dic_list)
TN_CA.tail()
170/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
170/2: url = 'https://redplanetscience.com/'
170/3: response = requests.get(url)
170/4: response
170/5: type(response0
170/6: type(response)
170/7: response.status_code
170/8: response.status_code
170/9: response.text
170/10: soup = BS(response.text)
170/11: print(soup.prettify())
171/1: soup.find('title')
171/2: soup = BS(response.text)
171/3: soup = BS(response.text)
173/1:
#Import the packages and library
import requests
from bs4 import BeautifulSoup as BS
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re

%matplotlib inline

import seaborn as sns
import datetime
173/2: url = 'https://redplanetscience.com/'
173/3: response = requests.get(url)
173/4: type(response)
173/5: response.status_code
173/6: response.text
173/7: soup = BS(response.text)
173/8: print(soup.prettify())
173/9: soup.find('title')
173/10: soup.find('div', attrs={'class' : 'list_date'}).text
173/11: soup.find-all('div', attrs={'class' : 'list_date'}).text
173/12: soup.find_all('div', attrs={'class' : 'list_date'}).text
173/13: soup.find_all('div', attrs={'class' : 'list_date'}).text
173/14: soup.find_all('div', attrs={'class' : 'list_date'}).text()
173/15: soup.find_all('div', attrs={'class' : 'list_date'}).get_text()
173/16: soup.find('div', attrs={'class' : 'list_date'}).get_text()
175/1:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
175/2:
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'county',
    'in' : 'state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)

response_pop
175/3:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
#TNDA_pop.shape
175/4:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(3)
TNDA_pop.shape
175/5:
response_pop.text
res_pop = response_pop.json()
TNDA_pop = pd.DataFrame(res_pop)
TNDA_pop.head(10)
#TNDA_pop.shape
175/6:
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract county',
    'in' : 'state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)

response_pop
175/7:
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract:*&in=county',
    'in' : 'state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)

response_pop
175/8:
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract & county',
    'in' : 'state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)

response_pop
175/9:
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'county',
    'in' : 'state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)

response_pop
175/10:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=county:*&in=state:*&key=YOUR_KEY_GOES_HERE
            
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'county',
    'in' : 'state'
}


response_pop_us = requests.get(endpoint_pop, params = params1)

response_pop
175/11:
response_pop_us.text
res_pop_us = response_pop_us.json()
US_pop = pd.DataFrame(res_pop_us)
US_pop.head(10)
#TNDA_pop.shape
175/12:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=county:*&in=state:*&key=YOUR_KEY_GOES_HERE
            
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'county',
    'in' : 'state'
}


response_pop_us = requests.get(endpoint_pop, params = params2)

response_pop_us
175/13:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=county:*&in=state:*&key=YOUR_KEY_GOES_HERE
            
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'county',
    'in' : 'state:*'
}


response_pop_us = requests.get(endpoint_pop, params = params2)

response_pop_us
175/14:
response_pop_us.text
res_pop_us = response_pop_us.json()
US_pop = pd.DataFrame(res_pop_us)
US_pop.head(10)
#TNDA_pop.shape
175/15:
response_pop_us.text
res_pop_us = response_pop_us.json()
US_pop = pd.DataFrame(res_pop_us)
US_pop.head(10)
#US_pop.shape
175/16:
response_pop_us.text
res_pop_us = response_pop_us.json()
US_pop = pd.DataFrame(res_pop_us)
US_pop.head(10)
US_pop.shape
175/17:
response_pop_us.text
res_pop_us = response_pop_us.json()
US_pop = pd.DataFrame(res_pop_us)
US_pop.head(10)
#US_pop.shape (3222, 4)
175/18: **Reading the data for all counties/states in US**
175/19:
#Renaming the column B01001_00IE as population

TNDA_pop = TNDA_pop.rename(columns={'B01001_001E':'Population'})
TNDA_pop.head()
175/20:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TNDA_pop.iloc[0] #pull the row 1 in a list called header
TNDA_pop = TNDA_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TNDA_pop.columns = header
TNDA_pop.head()#Renaming the column B01001_00IE as population

TNDA_pop = TNDA_pop.rename(columns={'B01001_001E':'Population'})
TNDA_pop.head()
175/21:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = US_pop.iloc[0] #pull the row 1 in a list called header
US_pop = US_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
US_pop.columns = header
US_pop.head()
#Renaming the column B01001_00IE as population

US_pop = US_pop.rename(columns={'B01001_001E':'Population'})
US_pop.head()
175/22:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = US_pop.iloc[0] #pull the row 1 in a list called header
US_pop = US_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
US_pop.columns = header
US_pop.head()
#Renaming the column B01001_00IE as population

US_pop = US_pop.rename(columns={'B01001_001E':'Population', 'state':'State', 'county':'County'})
US_pop.head()
175/23:
response_pop_us.text
res_pop_us = response_pop_us.json()
US_pop = pd.DataFrame(res_pop_us)
US_pop.head(10)
#US_pop.shape (3222, 4)
175/24:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = US_pop.iloc[0] #pull the row 1 in a list called header
US_pop = US_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
US_pop.columns = header
US_pop.head()
#Renaming the column B01001_00IE as population

US_pop = US_pop.rename(columns={'B01001_001E':'Population', 'state':'State', 'county':'County'})
US_pop.head()
175/25:
response_pop.text
res_pop = response_pop.json()
TN_pop = pd.DataFrame(res_pop)
TN_pop.head(10)
#TN_pop.shape
175/26:
response_pop.text
res_pop = response_pop.json()
TN_pop = pd.DataFrame(res_pop)
TN_pop.head(10)
TN_pop.shape
175/27:
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'county',
    'in' : 'state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)

response_pop
175/28:
response_pop.text
res_pop = response_pop.json()
TN_pop = pd.DataFrame(res_pop)
TN_pop.head(10)
TN_pop.shape (96, 4)
175/29:
response_pop.text
res_pop = response_pop.json()
TN_pop = pd.DataFrame(res_pop)
TN_pop.head(10)
#TN_pop.shape (96, 4)
175/30:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TN_pop.iloc[0] #pull the row 1 in a list called header
TN_pop = TN_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TN_pop.columns = header
TN_pop.head()
175/31:
#Reading Income data for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TN_inc = pd.DataFrame(res_inc)
TN_inc.head()
#TNDA_inc.shape (175, 5)
175/32:
#Reading Income data for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TN_inc = pd.DataFrame(res_inc)
TN_inc.head()
TN_inc.shape (175, 5)
175/33:
#Reading Income data for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TN_inc = pd.DataFrame(res_inc)
TN_inc.head()
TN_inc.shape
175/34:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TN_inc.iloc[0] #pull the row 1 in a list called header
TN_inc = TN_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TN_inc.columns = header
TN_inc.head()
175/35:
#Renaming the column B01001_00IE as population

TN_pop = TNDA_pop.rename(columns={'B01001_001E':'Population', 'state':'State', 'county':'County'})
TN_pop.head()
175/36:
#Renaming the column S1901_C01_012E as population

TNDA_inc = TNDA_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'County'})
TNDA_inc.head()
175/37:
#Renaming the column S1901_C01_012E as population

TN_inc = TNDA_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'County'})
TN_inc.head()
175/38:
#Renaming the column S1901_C01_012E as population

TN_inc = TN_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'County'})
TN_inc.head()
175/39:
#Reading data profile for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/profile'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME',
    'for' : 'county',
    'in' : 'state:47'
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
175/40:
response_data.text

res_data = response_data.json()
res_data
TN_data = pd.DataFrame(res_data)
TN_data.head()
#TN_data.shape (96, 4)
175/41:
US_pop_inc = pd.merge(US_pop, US_inc, on=['NAME', 'State', 'County'])
US_pop_inc.shape #(174, 6)
US_pop_inc.head(3)
175/42:
TN_pop_inc = pd.merge(TN_pop, TN_inc, on=['NAME', 'State', 'County'])
TN_pop_inc.shape #(174, 6)
TN_pop_inc.head(3)
174/1: import geopandas as gpd
175/43: import geopandas as gpd
176/1:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
176/2:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
176/3:
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'county',
    'in' : 'state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)

response_pop
176/4:
response_pop.text
res_pop = response_pop.json()
TN_pop = pd.DataFrame(res_pop)
TN_pop.head(10)
#TN_pop.shape (96, 4)
176/5:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TN_pop.iloc[0] #pull the row 1 in a list called header
TN_pop = TN_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TN_pop.columns = header
TN_pop.head()
176/6:
#Renaming the column B01001_00IE as population

TN_pop = TNDA_pop.rename(columns={'B01001_001E':'Population', 'state':'State', 'county':'County'})
TN_pop.head()
176/7:
#Renaming the column B01001_00IE as population

TN_pop = TN_pop.rename(columns={'B01001_001E':'Population', 'state':'State', 'county':'County'})
TN_pop.head()
176/8:
#Reading Income data for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TN_inc = pd.DataFrame(res_inc)
TN_inc.head()
#TN_inc.shape (96, 4)
176/9:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TN_inc.iloc[0] #pull the row 1 in a list called header
TN_inc = TN_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TN_inc.columns = header
TN_inc.head()
176/10:
#Renaming the column S1901_C01_012E as population

TN_inc = TN_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'County'})
TN_inc.head()
176/11:
#Reading data profile for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/profile'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME',
    'for' : 'county',
    'in' : 'state:47'
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
176/12:
response_data.text

res_data = response_data.json()
res_data
TN_data = pd.DataFrame(res_data)
TN_data.head()
#TN_data.shape (96, 4)
176/13:
TN_pop_inc = pd.merge(TN_pop, TN_inc, on=['NAME', 'State', 'County'])
TN_pop_inc.shape #(174, 6)
TN_pop_inc.head(3)
176/14: import geopandas as gpd
176/15:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=county:*&in=state:*&key=YOUR_KEY_GOES_HERE
            
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'county',
    'in' : 'state:*'
}


response_pop_us = requests.get(endpoint_pop, params = params2)

response_pop_us
176/16:
response_pop_us.text
res_pop_us = response_pop_us.json()
US_pop = pd.DataFrame(res_pop_us)
US_pop.head(10)
#US_pop.shape (3222, 4)
176/17:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = US_pop.iloc[0] #pull the row 1 in a list called header
US_pop = US_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
US_pop.columns = header
US_pop.head()
#Renaming the column B01001_00IE as population

US_pop = US_pop.rename(columns={'B01001_001E':'Population', 'state':'State', 'county':'County'})
US_pop.head()
176/18:
#activate the geospatial environment in anaconda powershell using "conda activate geospatial_ds6" the open the notebook
import geopandas as gpd
177/1: import geopandas as gpd
176/19: TN_pop_inc.to_csv('data/TN_pop_inc.csv')
176/20:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
#TN_shape.shape #(1701, 13)
176/21:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
TN_shape.shape #(1701, 13)
176/22:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
TN_shape.shape #(1701, 13)
TN_shape.columns
176/23: TN_shape.head(10)
176/24: TN_pop_inc.head()
176/25: TN_shape.head(5)
176/26:
TN_info = TN_pop_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'COUNTYFP'})
TN_info.head()
176/27:
TN_info = TN_pop_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'COUNTYFP'})
TN_info.head()
176/28:
TN_pop_inc = TN_pop_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'COUNTYFP'})
TN_pop_inc.head()
176/29:
TN_pop_inc = TN_pop_inc.rename(columns={'county':'COUNTYFP'})
TN_pop_inc.head()
176/30:
TN_pop_inc = TN_pop_inc.rename(columns={'County':'COUNTYFP'})
TN_pop_inc.head()
176/31:
TN_info = TN_pop_inc.rename(columns={'County':'COUNTYFP'})
TN_info.head()
176/32: pd.merge(TN_info, TN_shape, on = 'COUNTYFP', how = 'left')
176/33: pd.merge(TN_info, TN_shape, on = 'COUNTYFP', how = 'right')
176/34:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
TN_shape.shape #(1701, 13)
#TN_shape.columns
176/35:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
TN_shape.shape #(1701, 13)
TN_shape.columns
176/36:
TN_info = TN_pop_inc.rename(columns={'County':'COUNTYFP'})
TN_info.head()
TN_info['COUNTYFP'].nunique()
176/37: pd.merge(TN_info, TN_shape, on = 'COUNTYFP', how = 'left')
176/38: TN_info_shape = pd.merge(TN_info, TN_shape, on = 'COUNTYFP', how = 'left')
176/39:
TN_info_shape = pd.merge(TN_info, TN_shape, on = 'COUNTYFP', how = 'left')
TN_info_shape['COUNTYFP'].nunique()
176/40:
TN_info_shape = pd.merge(TN_info, TN_shape, on = 'COUNTYFP', how = 'left')
TN_info_shape['COUNTYFP'].nunique()
TN_info_shape.columns
176/41: TN_info_shape.head()
176/42: type(TN_info_shape)
176/43:
#Renaming the county to COUNTYFP so that it can be merged with the TN_shape files to get the lat long
TN_info = TN_pop_inc.rename(columns={'County':'COUNTYFP'})
TN_info.head()
TN_info['COUNTYFP'].nunique()
176/44: TN_info_shape.to_csv('data/TN_pop_inc_geo.csv')
176/45:
##Reading the medianincome data for all the US counties

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:*'
}
response_inc_US = requests.get(endpoint_inc, params = params2)

response_inc_US.text

res_inc_US = response_inc.json()
res_inc_US
US_inc = pd.DataFrame(res_inc_US)
US_inc.head()
#US_inc.shape (96, 4)
176/46:
##Reading the medianincome data for all the US counties

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:*'
}
response_inc_US = requests.get(endpoint_inc, params = params2)

response_inc_US.text

res_inc_US = response_inc.json()
res_inc_US
US_inc = pd.DataFrame(res_inc_US)
US_inc.head()
US_inc.shape (96, 4)
176/47:
##Reading the medianincome data for all the US counties

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:*'
}
response_inc_US = requests.get(endpoint_inc, params = params2)

response_inc_US.text

res_inc_US = response_inc.json()
res_inc_US
US_inc = pd.DataFrame(res_inc_US)
US_inc.head()
US_inc.shape #(96, 4)
176/48:
##Reading the medianincome data for all the US counties

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:*'
}
response_inc_US = requests.get(endpoint_inc, params = params2)

response_inc_US.text

res_inc_US = response_inc_US.json()
res_inc_US
US_inc = pd.DataFrame(res_inc_US)
US_inc.head()
US_inc.shape #(96, 4)
176/49:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = US_inc.iloc[0] #pull the row 1 in a list called header
US_inc = US_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
US_inc.columns = header
US_inc.head()
#Renaming the column 'S1901_C01_012E':'MedianIncome', as population

US_inc = US_pop.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'County'})
US_inc.head()
176/50:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = US_inc.iloc[0] #pull the row 1 in a list called header
US_inc = US_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
US_inc.columns = header
US_inc.head()
#Renaming the column 'S1901_C01_012E':'MedianIncome', as population

US_inc = US_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'County'})
US_inc.head()
176/51:
##Reading the medianincome data for all the US counties

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:*'
}
response_inc_US = requests.get(endpoint_inc, params = params2)

response_inc_US.text

res_inc_US = response_inc_US.json()
res_inc_US
US_inc = pd.DataFrame(res_inc_US)
US_inc.head()
#US_inc.shape #(3222, 4)
176/52:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = US_inc.iloc[0] #pull the row 1 in a list called header
US_inc = US_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
US_inc.columns = header
US_inc.head()
#Renaming the column 'S1901_C01_012E':'MedianIncome', as population

US_inc = US_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'County'})
US_inc.head()
176/53:
US_pop_inc = pd.merge(US_pop, US_inc, on=['NAME', 'State', 'County'])
US_pop_inc.shape #(174, 6)
US_pop_inc.head(3)
176/54:
US_pop_inc = pd.merge(US_pop, US_inc, on=['NAME', 'State', 'County'])
US_pop_inc.shape #(174, 6)
#US_pop_inc.head(3)
176/55:
US_pop_inc = pd.merge(US_pop, US_inc, on=['NAME', 'State', 'County'])
US_pop_inc.shape #(3221, 5)
US_pop_inc.head(3)
176/56: US_pop_inc.to_csv('data/US_pop_inc.csv')
176/57:
#Reading data profile for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/profile'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,DP05_0037E',
    'for' : 'county',
    'in' : 'state:47'
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
176/58:
response_data.text

res_data = response_data.json()
res_data
TN_data = pd.DataFrame(res_data)
TN_data.head()
#TN_data.shape (96, 4)
176/59:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:06&in=county:*

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract:*',
    'in' : 'state:*'
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
176/60:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:06&in=county:*

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract:*',
    'in' : ['state:*', 'county:*']
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
176/61: response_data.text
176/62:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:06&in=county:*

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract:*',
    'in' : ['state:47', 'county:*']
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
176/63:
response_data.text

res_data = response_data.json()
res_data
TN_data = pd.DataFrame(res_data)
TN_data.head()
#TN_data.shape (96, 4)
176/64:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:06&in=county:*

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract:*',
    'in' : 'state:*
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
176/65:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:06&in=county:*

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract:*',
    'in' : 'state:*'
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
176/66:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:06&in=county:*

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract',
    'in' : 'state:*'
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
176/67:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:06&in=county:*

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract:*',
    'in' : ['state:47, 'county:*']
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
176/68:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:06&in=county:*

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract:*',
    'in' : ['state:47', 'county:*']
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
179/1:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
179/2:
US_tract = pd.read_csv("./data/ACSDP5Y2021_DP03_censusTractdata_US.csv")
US_tract.head(10)
US_tract.tail(5)
179/3: US_tract.head(10)
179/4: US_tract.columns
179/5:
US_tract.columns
US_tract.shape
179/6:
header = US_tract.iloc[0] #pull the row 1 in a list called header
US_tract = US_tract[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
US_tract.columns = header
US_tract.head()
179/7: US_tract.columns
179/8:
US_tract = pd.read_csv("./data/ACSDP5Y2021_DP03_censusTractdata_US.csv")

#US_tract.tail(5)
179/9:
US_tract.columns
US_tract.shape
179/10: US_tract.head(2)
179/11: US_tract.columns
179/12: US_tract.columns
179/13:
DP03 = pd.read_csv("./data/ACSDP5Y2021_DP03_censusTractdata_US.csv")

#US_tract.tail(5)
179/14: DP05 = pd.read_csv("./data/ACSDP5Y2021_DP05_censusTractdata_US_ageandsex.csv")
179/15: DP04 = pd.read_csv("./data/ACSDP5Y2021_censusTractdata_US_vehicle.csv")
179/16:
US_tract.columns
DP03.shape
179/17:
US_tract.columns
DP03.shape #(85397, 1098)
DP04.shape
179/18:
US_tract.columns
DP03.shape #(85397, 1098)
DP04.shape #(85397, 1146)
DP05.shape
179/19:
US_tract.columns
DP03.shape #(85397, 1098)
DP04.shape #(85397, 1146)
DP05.shape #(85397, 714)
DP03.colums
179/20:
US_tract.columns
DP03.shape #(85397, 1098)
DP04.shape #(85397, 1146)
DP05.shape #(85397, 714)
DP03.columns
179/21:
US_tract.columns
DP03.shape #(85397, 1098)
DP04.shape #(85397, 1146)
DP05.shape #(85397, 714)#GEO_ID', 'NAME'
DP04.columns
179/22:
US_tract.columns
DP03.shape #(85397, 1098)
DP04.shape #(85397, 1146)
DP05.shape #(85397, 714)#GEO_ID', 'NAME'
DP05.columns
179/23: DP03.head(2)
179/24: DP05.head(2)
179/25: DP05.head(5)
179/26:
DP05_selcted = DP05[['GEO_ID', 'NAME', 'DP05_0001E', 'DP05_0002E', 'DP05_0003E', 
      'DP05_0018E', 'DP05_0033E', 'DP05_0037E', 'DP05_0038E', 'DP05_0039E',
      'DP05_0044E', 'DP05_0052E']]
179/27: DP05_selcted.shape
179/28:
DP05_selcted.shape #(85397, 12)
DP05_selcted.head(10)
179/29:
DP05_selcted.shape #(85397, 12)
DP05_selcted.head(5)
179/30: DP03_selected = DP03[['GEO_ID', 'NAME', 'DP03_0062E', 'DP03_0086E','DP03_0095E', 'DP03_0096E', 'DP03_0099E']]
179/31: DP03_selected.head(10)
179/32: DP03_selected.shape
179/33:
DP05_selected = DP05[['GEO_ID', 'NAME', 'DP05_0001E', 'DP05_0002E', 'DP05_0003E', 
      'DP05_0018E', 'DP05_0033E', 'DP05_0037E', 'DP05_0038E', 'DP05_0039E',
      'DP05_0044E', 'DP05_0052E']]
179/34:
DP05_selected.shape #(85397, 12)
DP05_selected.head(5)
179/35:
DP04_selected = DP04[['GEO_ID', 'NAME', 'DP04_0058E', 'DP04_0059E']]
DP04_selected.shape
179/36: DP04.head(5)
179/37: DP04_selected.head(5)
179/38:
DP04_selected.head(5)
DP04_selected.columns
179/39: pd.merge(pd.merge(DP05_selected, DP03_selected,on= ['GEO_ID', 'NAME']), DP04_selected, on= ['GEO_ID', 'NAME'])
179/40: DP_merged = pd.merge(pd.merge(DP05_selected, DP03_selected,on= ['GEO_ID', 'NAME']), DP04_selected, on= ['GEO_ID', 'NAME'])
179/41:
DP_merged = pd.merge(pd.merge(DP05_selected, DP03_selected,on= ['GEO_ID', 'NAME']), DP04_selected, on= ['GEO_ID', 'NAME'])
DP_merged.shape
179/42:
DP_merged = pd.merge(pd.merge(DP05_selected, DP03_selected,on= ['GEO_ID', 'NAME']), DP04_selected, on= ['GEO_ID', 'NAME'])
DP_merged.shape #(85397, 19)
DP_merged.columns
179/43:
DP05_selected = DP05[['GEO_ID', 'NAME', 'DP05_0001E', 'DP05_0002E', 'DP05_0003E', 
      'DP05_0018E', 'DP05_0037E', 'DP05_0038E', 'DP05_0039E',
      'DP05_0044E', 'DP05_0052E']]
179/44:
DP05_selected.shape #(85397, 12)
DP05_selected.head(5)
179/45:
DP_merged = pd.merge(pd.merge(DP05_selected, DP03_selected,on= ['GEO_ID', 'NAME']), DP04_selected, on= ['GEO_ID', 'NAME'])
DP_merged.shape #(85397, 19)
DP_merged.columns
179/46:
#rankings_pd.rename(columns = {'test':'TEST'}, inplace = True)

DP_census = DP_merged.rename(columns = {'GEO_ID':'Geo_ID', 
 'NAME':'Tract', 
 'DP05_0001E':'Total_Population', 
 'DP05_0002E': 'Male_Population', 
 'DP05_0003E': 'Female_Population',
 'DP05_0018E': 'Median_Age', 
 'DP05_0037E': 'Race_White', 
 'DP05_0038E': 'Race_Black',
 'DP05_0039E': 'Race_AI_AN',
 'DP05_0044E': 'Race_Asian',
 'DP05_0052E': 'Race_PI_NH',
 'DP03_0062E': 'Median_Household_Income', 
 'DP03_0086E': 'Median_Family_Income', 
 'DP03_0095E': 'Total_HealthInsurance',
 'DP03_0096E': 'With_HealthInsurance', 
 'DP03_0099E': 'Without_HelathInsurance', 
 'DP04_0058E': 'No_Vehicle',
 'DP04_0059E': 'With_Vehicle'}, inplace = True)
179/47: DP_census.columns
179/48: DP_census.shape()
179/49: DP_census.shape
179/50: DP_census.head(5)
179/51:
#rankings_pd.rename(columns = {'test':'TEST'}, inplace = True)

DP_merged = DP_merged.rename(columns = {'GEO_ID':'Geo_ID', 
 'NAME':'Tract', 
 'DP05_0001E':'Total_Population', 
 'DP05_0002E': 'Male_Population', 
 'DP05_0003E': 'Female_Population',
 'DP05_0018E': 'Median_Age', 
 'DP05_0037E': 'Race_White', 
 'DP05_0038E': 'Race_Black',
 'DP05_0039E': 'Race_AI_AN',
 'DP05_0044E': 'Race_Asian',
 'DP05_0052E': 'Race_PI_NH',
 'DP03_0062E': 'Median_Household_Income', 
 'DP03_0086E': 'Median_Family_Income', 
 'DP03_0095E': 'Total_HealthInsurance',
 'DP03_0096E': 'With_HealthInsurance', 
 'DP03_0099E': 'Without_HelathInsurance', 
 'DP04_0058E': 'No_Vehicle',
 'DP04_0059E': 'With_Vehicle'})
179/52: DP_merged.head(5)
179/53: US_census = US_merged[1:]
179/54: US_census = DP_merged[1:]
179/55:
US_census = DP_merged[1:]
US_census
179/56: US_census.info()
179/57: US_census.to_csv('data/US_census.csv')
184/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
184/2:
#Reading the gdp_per_capita.csv which is downloaaded fro the URL("") as .csv file and nested in the data folder
gdp_df = pd.read_csv("../data/gdp_per_capita.csv")
gdp_df.head(10)
gdp_df.tail(5)
184/3:
gdp_df.shape #(6871, 4)
gdp_df.info()
gdp_df.columns
184/4:
# droping the NaN values in the df columns country and value
gdp_df_dropnan = gdp_df.dropna(subset=['Country or Area', 'Value']) 
gdp_df_dropnan.shape
gdp_df_dropnan.tail(5)
gdp_df_dropnan.info()
184/5:
#Drop the 'Value Footnotes' column, 

gdp_df_dropnan_dropfootnotes = gdp_df_dropnan.drop(columns = ["Value Footnotes"])
print(gdp_df_dropnan_dropfootnotes.tail(5))
gdp_df_dropnan_dropfootnotes.columns
184/6:
##After droping Value Footnote column rename the remaining columns to 'Country', 'Year', and 'GDP_Per_Capita'
gdp_df_column_rename = gdp_df_dropnan_dropfootnotes.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'GDP_Per_Capita'})
print(gdp_df_column_rename.head(5))
gdp_df_column_rename.info() # we can also use the df.dtype() to find the type.
184/7:
#convert the type using the .astype() function for the column, and selecting it to be assinged back sort of saving it in the same dataframe to the same df
gdp_df_column_rename["Year"] = gdp_df_column_rename["Year"].astype(int)
gdp_df_column_rename.info()
184/8:
# Reread the .csv file into the dataframe without changing the datatype and droping the Footnotes row.
gdp_df_nrows = pd.read_csv("../data/gdp_per_capita.csv", nrows=6868)
gdp_df_nrows.info()
gdp_df_nrows.shape
gdp_df_nrows.columns
184/9:
gdp_df_for_analysis = gdp_df_nrows.drop(columns = ["Value Footnotes"])
gdp_df_for_analysis = gdp_df_for_analysis.rename(columns = {'Country or Area': 'Country', 'Year': 'Year', 'Value': 'GDP_Per_Capita'})
gdp_df_for_analysis.head(5)
184/10: gdp_df_for_analysis.info()
184/11:
gdp_df_for_analysis['Year'].value_counts().sort_index(ascending=False)
#.sort_index() helps put the output in lowest to highest value using the index.
#.sort_index(ascending=False) puts from highest to lowest
#.plot(kind = 'bar')
184/12:
gdp_df_for_analysis['Country'].value_counts()
#gdp_df_for_analysis['Country'].value_counts().head(20)
gdp_df_for_analysis['Country'].value_counts().tail(20)
#gdp_df_for_analysis.head()
184/13: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
184/14: gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 1990].value_counts() #207 countries start the count since 1990
184/15: gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Djibouti"]
184/16:
gdp_groupby_country = gdp_df_for_analysis.groupby('Country').value_counts().to_frame()
gdp_groupby_country.head(5)
#gdp_df_for_analysis.groupby(['Country'])['Country'].count()
184/17:
gdp_2014 = gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] == 2014]
gdp_2014 = gdp_2014.set_index('Year')
gdp_2014.head(20)
#gdp_2014.shape #(238, 2)
184/18: gdp_2014['GDP_Per_Capita'].describe()
184/19:
#plt.hist(gdp_2014['GDP_Per_Capita'])
plt.hist(gdp_2014['GDP_Per_Capita'], color="green", edgecolor="red", bins=50)
plt.title("GDP_Per_Capita for 2014 for all Countries")
plt.xlabel("GDP_Per_Capita")
plt.ylabel("Number of Countries")
184/20: gdp_2014.sort_values('GDP_Per_Capita').head(5)
184/21: gdp_2014.sort_values('GDP_Per_Capita', ascending=False).head(5)
184/22:
gdp_1990_2017= gdp_df_for_analysis.loc[gdp_df_for_analysis['Year'] .isin ([1990, 2017])]
gdp_1990_2017.shape
gdp_1990_2017.head()
184/23:
gdp_pivoted = gdp_1990_2017.pivot_table(values=['GDP_Per_Capita'], index=['Country'],
                    columns=['Year']).dropna()

#Create the gdp_pivoted df from the .loc df with 1990 and 2017 year and using the .dropna() method to delete the NaN row.
184/24: gdp_pivoted.head()
184/25: gdp_pivoted.describe()
184/26:
gdp_pivoted.shape
gdp_pivoted.columns
184/27: gdp_pivoted.info()
184/28:
gdp_pivoted['Percent_Change'] = 100 *(gdp_pivoted[('GDP_Per_Capita', 2017)] - gdp_pivoted[('GDP_Per_Capita', 1990)])/ gdp_pivoted[('GDP_Per_Capita', 1990)]
gdp_pivoted.head()
184/29: gdp_pivoted.sort_values("Percent_Change").head(10)
184/30: gdp_pivoted.sort_values("Percent_Change").tail(10)
184/31:
#Count number of negative value in the Percent change column to find the countries with negative growth
#.value looks for the value in the column
#.fatten converts the column in a array to count the values which are lower than 0 and spots the total count

sum(n < 0 for n in gdp_pivoted['Percent_Change'].values.flatten())
184/32: gdp_pivoted.sort_values("Percent_Change", ascending=False).head(10)
184/33:
#to plot the line plot for Equatorial Guinea GDP_Per_Capita for all years.


gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'] == "Equatorial Guinea"].plot(y='GDP_Per_Capita')

#I see the graph but I want x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
184/34:
#using plt.plot function
#finding row number for the Equatorial Guinea
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"].tolist()
#gdp_df_for_analysis.iloc[1754:1783].plot(x='Year', y='GDP_Per_Capita')

gdp_df_for_analysis.iloc[gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="Equatorial Guinea"]].plot(x='Year', y='GDP_Per_Capita', title= "Equatorial Guinea", xlabel="Year", ylabel="GDP_Per_Capita")


#Here we got the x-axis to be 'Year' and y-axis to be 'GDP_Per_Capita' not sure what is the x-axis hear.
184/35:
gdp_df_for_analysis.index[gdp_df_for_analysis['Country']=="China"].tolist()

gdp_df_for_analysis.iloc[1162:1191].plot(x='Year', y='GDP_Per_Capita', title= "China", xlabel="Year", ylabel="GDP_Per_Capita")
184/36:
#To get both on the same graph subseting the values and making one df and than changing the index to country and than transposing the rows.
gdp_EG_China = gdp_df_for_analysis.loc[gdp_df_for_analysis['Country'].isin(["Equatorial Guinea", "China"])]
gdp_EG_China.head()
gdp_EG_China.columns
#gdp_EG_China.set_index("Country")
184/37:
EG_China = gdp_EG_China.pivot(index='Year', columns='Country', values='GDP_Per_Capita')
EG_China.head()
#EG_China.plot(title = "Countries with hhighest GDP_Growth", xlabel: "Year", ylabel: "GDP_Per_Capita")
EG_China.plot(title= "Country", xlabel="Year", ylabel="GDP_Per_Capita")
184/38:

continents = pd.read_csv("../data/continents.csv")
continents.head()
continents.shape
continents.columns
#continents.head()
184/39:
gdp_df_for_analysis.shape
gdp_df_for_analysis.head()
184/40:
gdp_df_merged = pd.merge(left = gdp_df_for_analysis, 
         right = continents, how = 'inner', on= 'Country') 
         
gdp_df_merged.info()
gdp_df_merged.shape
gdp_df_merged.head()
184/41: #gdp_df_merged['Continent'].nunique()
184/42: gdp_df_merged.groupby('Continent')['Country'].nunique().plot(kind="bar", title = 'Countries_Per_Continent', xlabel='', ylabel='Number of Countries')
184/43:
#scratch work
gdp_df_merged.groupby('Continent')['GDP_Per_Capita'].plot(kind='bar')
#, x="Continent", y="GDP_Per_Capita", hue="smoker")
184/44:
#Scratch work
import seaborn as sns
#gdp_df_merged(gdp_df_merged.loc[gdp_df_merged['Year']== 2014]).groupby('Continent')['GDP_Per_Capita'].plot()
df_merged_2014 = gdp_df_merged.loc[gdp_df_merged['Year']== 2014]
df_merged_2014.shape

sns.boxplot(data = df_merged_2014, x='Continent', y='GDP_Per_Capita');

sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], x='Continent', y='GDP_Per_Capita');

#df_merged_2014.groupby('Continent')['GDP_Per_Capita'].nunique().plot(kind="bar", title = '2014_GDP_Per_Continent', xlabel='', ylabel='GDP_Per_capita'')
#sns.boxplot(data=df[["age", "fare"]], orient="h")
184/45:
sns.boxplot(data = gdp_df_merged.loc[gdp_df_merged['Year']== 2014], 
            x='Continent', y='GDP_Per_Capita');
184/46:
#df of the life expectancy data, looking at the file in CSV it show that there are few top rows which are note, that is why I skiped first 4 rows at the time of reading the file.As it was showing error upon reading the csv file**

life_expectancy_a = pd.read_csv('../data/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4558314.csv', 
                              skiprows = 4)
life_expectancy_a.shape #(266, 67)

life_expectancy_a.columns
184/47:
#droping the columns and making the table transpose. Column to rows.
lemelt = pd.melt(life_expectancy_a.drop(columns = ['Country Code', 'Indicator Name', 'Indicator Code']), id_vars=['Country Name'], value_vars=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',
       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',
       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',
       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',
       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',
       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',
       '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021'])
lemelt.head(2)
184/48:
#renaming the column names and saving the data as life_expectancy
life_expectancy = lemelt.rename(columns={"Country Name": "Country", "variable": "Year", "value": "Life_Expectancy"}, errors="raise")

#inspection
life_expectancy.head(2)
life_expectancy.tail(2)
life_expectancy['Country'].value_counts() #266 countries
life_expectancy['Year'].value_counts() #62 counts
life_expectancy.describe()
life_expectancy.shape #(16492, 3)
life_expectancy.head(2)
184/49:
#df[df['cloname']>80]

life_expectancy[life_expectancy['Life_Expectancy']>80].sort_values('Year').head(10)
184/50:
#gdp_df_merged.info()
life_expectancy.info()
life_expectancy['Year'] = life_expectancy['Year'].astype(int)
life_expectancy.info()
184/51:
gdp_le = pd.merge(left = gdp_df_merged, 
         right = life_expectancy, how = 'outer', on= ['Country', 'Year']) 
gdp_le.head(5)
gdp_le.tail()
gdp_le.shape #16821, 5
gdp_le.info()
184/52:
gdp_le_2019 = gdp_le.loc[gdp_le['Year'] == 2019]
gdp_le_2019.shape #276,5
gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
184/53: gdp_le_2019.sort_values('GDP_Per_Capita', ascending=False)
184/54:
#Scratch work
#Subsetting the df for top three countries
gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))]
184/55:
# Form a facetgrid using columns with a hue
graph = sns.FacetGrid(gdp_le[(gdp_le['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Life_Expectancy", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
184/56:
fig = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig)
184/57:
fig2 = sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of GDP_Per_Capita growth on Life expectancy")
plt.show(fig2)
184/58: gdp_le_2019['GDP_Per_Capita'].corr(gdp_le_2019['Life_Expectancy'])
184/59: gdp_le_2019['log_GDP'] = np.log(gdp_le_2019['GDP_Per_Capita'])
184/60: gdp_le_2019.head(5)
184/61: gdp_le_2019['log_GDP'].corr(gdp_le_2019['Life_Expectancy'])
184/62:
fig3 = sns.scatterplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
               
plt.title("Effect of log_GDP growth on Life expectancy")
plt.show(fig3)
sns.lmplot(data=gdp_le_2019, x="Life_Expectancy", y="log_GDP", hue='Continent')
plt.title("Regression plot of the log_GDP growth on Life expectancy")
184/63:
tc = pd.read_csv('../data/telecommunication_percentage usage.csv', nrows=4494)
#tc.head()
#tc.tal(i)
#tc.info()
tc2=tc.drop(columns=['Value Footnotes', 'Unnamed: 4',
       'Unnamed: 5'])
tc2.info()
tc3 = tc2.rename(columns = {'Country or Area' : 'Country', 'Year' : 'Year', 'Value' : 'Percentage_Internet_User'})
tc3.head()
tc3.info()
tc3.describe()
184/64:
gdp_df_merged_tele = pd.merge(left = gdp_df_merged, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_df_merged_tele.info()
gdp_df_merged_tele.shape
gdp_df_merged_tele.head()
184/65:
gdp_df_merged_tele.sort_values("Percentage_Internet_User", ascending=False).head(10)

#Ireland and Bermida has the highest interntusers
184/66:
#INTERUSER IN 2014
sns.boxplot(data = gdp_df_merged_tele.loc[gdp_df_merged_tele['Year']== 2014], 
            y='Continent', x='Percentage_Internet_User',
           );
184/67:
gdp_le_tele = pd.merge(left = gdp_le, 
         right = tc3, how = 'inner', on= ['Country', 'Year']) 
         
gdp_le_tele.info()
gdp_le_tele.shape
gdp_le_tele.head()
184/68:
# Form a facetgrid using columns with a hue percent use for the country with high gdp
graph = sns.FacetGrid(gdp_le_tele[(gdp_le_tele['Country'].isin(['Macao SAR, China', 'Luxembourg', 'Singapore', 'Iceland']))],
                      col ="Country")
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "Year", "Percentage_Internet_User", edgecolor ="w").add_legend()
# show the object
#plt.title('Top three Countries with largest growth in GDP for year 2019')
plt.show()
184/69:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.legend(bbox_to_anchor =(1.0, 1.0)); 
plt.show(figx)
184/70:
figy = sns.lmplot(data=gdp_le_tele, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth")
plt.show(figy)
184/71:
gdp_le_tele['log_gdp2'] = np.log(gdp_le_tele['GDP_Per_Capita'])
gdp_le_tele.head()
184/72:
figx = sns.scatterplot(data=gdp_le_tele, x="Percentage_Internet_User", y="log_gdp2", hue='Continent')
               
plt.title("Effect of Internet usage on GDP_Per_Capita growth")
plt.legend(bbox_to_anchor =(1.0, 1.0)); 
plt.show(figx)
184/73:
gdp_le_tele_2014 = gdp_le_tele.loc[gdp_le_tele['Year'] == 2014]
gdp_le_tele_2014.shape #179, 6
gdp_le_tele_2014.head(10)
#gdp_le_2019[gdp_le_2019['Life_Expectancy']>=80].info()
184/74:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="GDP_Per_Capita", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on GDP_Per_Capita growth in 2014")
plt.show(figz)
184/75:
figz = sns.lmplot(data=gdp_le_tele_2014, x="Percentage_Internet_User", y="Life_Expectancy", hue='Continent', ci=None)
               
plt.title("Correlation Plot Effect of Internet Usage on Life_Expectancy in 2014")
plt.show(figz)
184/76: a= gdp_le_tele_2014['GDP_Per_Capita'].corr(gdp_le_tele_2014['Life_Expectancy'])
184/77: b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
184/78: c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])
184/79:
b=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['Life_Expectancy'])
c=gdp_le_tele_2014['Percentage_Internet_User'].corr(gdp_le_tele_2014['GDP_Per_Capita'])

print("The increased internet usage is directly correlate to GDP growth by", c, "and life expectancy by", b, "for the year 2014 across globe. Asia and Europe had the highest Inter users in year 2014.")
184/80: gdp_le_tele_2014.corr()
184/81: gdp_le_tele.head(5)
184/82: import bar_chart_race as bcr
182/1:
age = DP05[['GEO_ID', 'NAME', 'DP05_0005E', 'DP05_0006E', 'DP05_0007E', 'DP05_0008E', 'DP05_0009E', 'DP05_0010E',
            'DP05_0011E', 'DP05_0012E', 'DP05_0013E', 'DP05_0014E', 'DP05_0015E', 'DP05_0016E', 'DP05_0017E']]
185/1:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
185/2:
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'county',
    'in' : 'state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)

response_pop
185/3:
response_pop.text
res_pop = response_pop.json()
TN_pop = pd.DataFrame(res_pop)
TN_pop.head(10)
#TN_pop.shape (96, 4)
185/4:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TN_pop.iloc[0] #pull the row 1 in a list called header
TN_pop = TN_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TN_pop.columns = header
TN_pop.head()
185/5:
#Renaming the column B01001_00IE as population

TN_pop = TN_pop.rename(columns={'B01001_001E':'Population', 'state':'State', 'county':'County'})
TN_pop.head()
185/6:
#Reading Income data for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TN_inc = pd.DataFrame(res_inc)
TN_inc.head()
#TN_inc.shape (96, 4)
185/7:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TN_inc.iloc[0] #pull the row 1 in a list called header
TN_inc = TN_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TN_inc.columns = header
TN_inc.head()
185/8:
#Renaming the column S1901_C01_012E as population

TN_inc = TN_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'County'})
TN_inc.head()
185/9:
#Reading data profile for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/profile'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,DP05_0037E',
    'for' : 'county',
    'in' : 'state:47'
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
185/10:
response_data.text

res_data = response_data.json()
res_data
TN_data = pd.DataFrame(res_data)
TN_data.head()
#TN_data.shape (96, 4)
185/11:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:06&in=county:*

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract:*',
    'in' : ['state:47', 'county:*']
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
185/12:
response_data.text

res_data = response_data.json()
res_data
TN_data = pd.DataFrame(res_data)
TN_data.head()
#TN_data.shape (96, 4)
185/13:
TN_pop_inc = pd.merge(TN_pop, TN_inc, on=['NAME', 'State', 'County'])
TN_pop_inc.shape #(174, 6)
TN_pop_inc.head(3)
185/14: TN_pop_inc.to_csv('data/TN_pop_inc.csv')
185/15:
#activate the geospatial environment in anaconda powershell using "conda activate geospatial_ds6" the open the notebook
import geopandas as gpd
186/1:
import requests
import matplotlib.pyplot as plt
import json
import pandas as pd
186/2:
endpoint_pop = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,B01001_001E&for=tract:*&in=county:037&in=state:47'

params1 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'county',
    'in' : 'state:47'
}


response_pop = requests.get(endpoint_pop, params = params1)

response_pop
186/3:
response_pop.text
res_pop = response_pop.json()
TN_pop = pd.DataFrame(res_pop)
TN_pop.head(10)
#TN_pop.shape (96, 4)
186/4:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TN_pop.iloc[0] #pull the row 1 in a list called header
TN_pop = TN_pop[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TN_pop.columns = header
TN_pop.head()
186/5:
#Renaming the column B01001_00IE as population

TN_pop = TN_pop.rename(columns={'B01001_001E':'Population', 'state':'State', 'county':'County'})
TN_pop.head()
186/6:
#Reading Income data for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/subject'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,S1901_C01_012E',
    'for' : 'county',
    'in' : 'state:47'
}
response_inc = requests.get(endpoint_inc, params = params2)

response_inc.text

res_inc = response_inc.json()
res_inc
TN_inc = pd.DataFrame(res_inc)
TN_inc.head()
#TN_inc.shape (96, 4)
186/7:
# as the header row has the numbers 1, 2,not the names we are going to change it with the first row which is a header title
header = TN_inc.iloc[0] #pull the row 1 in a list called header
TN_inc = TN_inc[1:] #delete the row 0 which is now becoming the header and start the df from row 1 pands count from 0,1
TN_inc.columns = header
TN_inc.head()
186/8:
#Renaming the column S1901_C01_012E as population

TN_inc = TN_inc.rename(columns={'S1901_C01_012E':'MedianIncome', 'state':'State', 'county':'County'})
TN_inc.head()
186/9:
#Reading data profile for all TN counties
endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5/profile'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,DP05_0037E',
    'for' : 'county',
    'in' : 'state:47'
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
186/10:
response_data.text

res_data = response_data.json()
res_data
TN_data = pd.DataFrame(res_data)
TN_data.head()
#TN_data.shape (96, 4)
186/11:
#https://api.census.gov/data/2021/acs/acs5?get=NAME,B01001_001E&for=tract:*&in=state:06&in=county:*

endpoint_inc = 'https://api.census.gov/data/2021/acs/acs5'

#?get=NAME,S1901_C01_012E&for=tract:*&in=county:037&in=state:47'

params2 = {
    'get' : 'NAME,B01001_001E',
    'for' : 'tract:*',
    'in' : ['state:47', 'county:*']
}
response_data = requests.get(endpoint_inc, params = params2)

response_data
186/12:
response_data.text

res_data = response_data.json()
res_data
TN_data = pd.DataFrame(res_data)
TN_data.head()
#TN_data.shape (96, 4)
186/13:
TN_pop_inc = pd.merge(TN_pop, TN_inc, on=['NAME', 'State', 'County'])
TN_pop_inc.shape #(174, 6)
TN_pop_inc.head(3)
186/14: TN_pop_inc.to_csv('data/TN_pop_inc.csv')
186/15:
#activate the geospatial environment in anaconda powershell using "conda activate geospatial_ds6" the open the notebook
import geopandas as gpd
186/16:
#activate the geospatial environment in anaconda powershell using "conda activate geospatial_ds6" the open the notebook
#import geopandas as gpd
186/17:
TN_shape = gpd.read_file('./TN2020_census_shapefiles/')
TN_shape.head()
TN_shape.shape #(1701, 13)
TN_shape.columns
186/18: TN_shape.head(5)
186/19: DP05 = pd.read_csv("./data/ACSDP5Y2021_DP05_censusTractdata_US_ageandsex.csv")
186/20: DP04 = pd.read_csv("./data/ACSDP5Y2021_censusTractdata_US_vehicle.csv")
186/21:
US_tract.columns
DP03.shape #(85397, 1098)
DP04.shape #(85397, 1146)
DP05.shape #(85397, 714)#GEO_ID', 'NAME'
DP05.columns
186/22:
#US_tract.columns
DP03.shape #(85397, 1098)
DP04.shape #(85397, 1146)
DP05.shape #(85397, 714)#GEO_ID', 'NAME'
DP05.columns
186/23:
DP03 = pd.read_csv("./data/ACSDP5Y2021_DP03_censusTractdata_US.csv")

#US_tract.tail(5)
186/24: DP05.head(5)
186/25:
#US_tract.columns
DP03.shape #(85397, 1098)
DP04.shape #(85397, 1146)
DP05.shape #(85397, 714)#GEO_ID', 'NAME'
DP05.columns
186/26:
DP05_selected = DP05[['GEO_ID', 'NAME', 'DP05_0001E', 'DP05_0002E', 'DP05_0003E', 
      'DP05_0018E', 'DP05_0037E', 'DP05_0038E', 'DP05_0039E',
      'DP05_0044E', 'DP05_0052E']]
186/27:
DP05_selected.shape #(85397, 12)
DP05_selected.head(5)
186/28: DP03_selected = DP03[['GEO_ID', 'NAME', 'DP03_0062E', 'DP03_0086E','DP03_0095E', 'DP03_0096E', 'DP03_0099E']]
186/29: DP03_selected.shape #(85397, 7)
186/30: DP03_selected.head(10)
186/31:
DP04_selected = DP04[['GEO_ID', 'NAME', 'DP04_0058E', 'DP04_0059E']]
DP04_selected.shape #(85397, 4)
186/32:
DP04_selected.head(5)
DP04_selected.columns
186/33:
DP_merged = pd.merge(pd.merge(DP05_selected, DP03_selected,on= ['GEO_ID', 'NAME']), DP04_selected, on= ['GEO_ID', 'NAME'])
DP_merged.shape #(85397, 19)
DP_merged.columns
186/34:
#rankings_pd.rename(columns = {'test':'TEST'}, inplace = True)

DP_merged = DP_merged.rename(columns = {'GEO_ID':'Geo_ID', 
 'NAME':'Tract', 
 'DP05_0001E':'Total_Population', 
 'DP05_0002E': 'Male_Population', 
 'DP05_0003E': 'Female_Population',
 'DP05_0018E': 'Median_Age', 
 'DP05_0037E': 'Race_White', 
 'DP05_0038E': 'Race_Black',
 'DP05_0039E': 'Race_AI_AN',
 'DP05_0044E': 'Race_Asian',
 'DP05_0052E': 'Race_PI_NH',
 'DP03_0062E': 'Median_Household_Income', 
 'DP03_0086E': 'Median_Family_Income', 
 'DP03_0095E': 'Total_HealthInsurance',
 'DP03_0096E': 'With_HealthInsurance', 
 'DP03_0099E': 'Without_HelathInsurance', 
 'DP04_0058E': 'No_Vehicle',
 'DP04_0059E': 'With_Vehicle'})
186/35: DP_merged.head(5)
186/36:
US_census = DP_merged[1:]
US_census
186/37: US_census.info()
186/38: DP05.shape
186/39:
age = DP05[['GEO_ID', 'NAME', 'DP05_0005E', 'DP05_0006E', 'DP05_0007E', 'DP05_0008E', 'DP05_0009E', 'DP05_0010E',
            'DP05_0011E', 'DP05_0012E', 'DP05_0013E', 'DP05_0014E', 'DP05_0015E', 'DP05_0016E', 'DP05_0017E']]
186/40:
age = DP05[['GEO_ID', 'NAME', 'DP05_0005E', 'DP05_0006E', 'DP05_0007E', 'DP05_0008E', 'DP05_0009E', 'DP05_0010E',
            'DP05_0011E', 'DP05_0012E', 'DP05_0013E', 'DP05_0014E', 'DP05_0015E', 'DP05_0016E', 'DP05_0017E']]
age.head(10)
186/41:
age = age.rename(columns = {'GEO_ID':'Geo_ID', 
                      'NAME':'Tract',
                      'DP05_0005E': 'Age<5yrs', 
                      'DP05_0006E':'Age5-9yrs', 
                      'DP05_0007E':'Age10-14yrs',
                      'DP05_0008E':'Age15-19yrs', 
                      'DP05_0009E':'Age20-24yrs', 
                      'DP05_0010E':'Age25-34yrs',
                    'DP05_0011E':'Age35-44yrs',
                      'DP05_0012E':'Age45-54yrs', 
                      'DP05_0013E':'Age55-59yrs', 
                      'DP05_0014E':'Age60-64yrs', 
                      'DP05_0015E':'Age65-74yrs', 
                      'DP05_0016E':'Age75-14yrs', 
                      'DP05_0017E':'Age>=85yrs'})
186/42: age.head(10)
186/43:
age_mod = age[1:]
age_mod
186/44: pd.merge(US_census, age_mod, on= ['GEO_ID', 'NAME'])
186/45: pd.merge(US_census, age_mod, on= ['Geo_ID', 'NAME'])
186/46: pd.merge(US_census, age_mod, on= ['Geo_ID', 'Tract'])
186/47: UScensus_age = pd.merge(US_census, age_mod, on= ['Geo_ID', 'Tract'])
186/48:
UScensus_age = pd.merge(US_census, age_mod, on= ['Geo_ID', 'Tract'])
UScensus_age.shape
186/49: UScensus_age.to_csv('data/UScensus_age.csv')
187/1: from sqlalchemy import create_engine
187/2:
database_name = ''    # Fill this in with your database name

# Update this if you have a different username/password/port
connection_string = f"postgresql://postgres:postgres@localhost:5432/{database_name}"
187/3:
database_name = ''    # Fill this in with your database name

# Update this if you have a different username/password/port
connection_string = f"postgresql://postgres:postgres@localhost:5432/{database_name}"
187/4: engine = create_engine(connection_string)
188/1: from sqlalchemy import create_engine
188/2:
database_name = ''    # Fill this in with your database name

# Update this if you have a different username/password/port
connection_string = f"postgresql://postgres:postgres@localhost:5432/{database_name}"
188/3: engine = create_engine(connection_string)
188/4:
query = '''
SELECT *
FROM names
LIMIT 100;
'''

result = engine.execute(query)
188/5:
database_name = 'usa_names'    # Fill this in with your database name

# Update this if you have a different username/password/port
connection_string = f"postgresql://postgres:postgres@localhost:5432/{database_name}"
188/6: engine = create_engine(connection_string)
188/7:
query = '''
SELECT *
FROM names
LIMIT 100;
'''

result = engine.execute(query)
188/8: result.fetchone()
189/1: import pandas as pd
189/2:
metro_calls = pd.read_csv('data/Metro_Nashville_Police_Department_Calls_for_Service.csv', 
                          nrows = 100)
189/3: metro_calls.head()
189/4: chunks = pd.read_csv('data/Metro_Nashville_Police_Department_Calls_for_Service.csv', chunksize = 10000)
189/5: chunks
189/6: next(chunks)
189/7: chunk = next(chunks)
189/8: chunk[chunk['Tencode Description']]=='SHOTS FIRED'
189/9: chunk[chunk['Tencode Description']=='SHOTS FIRED']
189/10:
#chunks = pd.read_csv('data/Metro_Nashville_Police_Department_Calls_for_Service.csv', chunksize = 10000)

chunks = pd.read_csv('data/Metro_Nashville_Police_Department_Calls_for_Service.csv', chunksize = 10000)
shots_fired = []
for chunk in chunks:
    chunk = chunk[chunk['Tencode Description'] == 'SHOTS FIRED']
    shots_fired.append(chunk)
189/11: shots_fired = pd.concat(shots_fired, ignore_index = 'True')
189/12: shots_fired
189/13:
#chunks = pd.read_csv('data/Metro_Nashville_Police_Department_Calls_for_Service.csv', chunksize = 10000)

chunks = pd.read_csv('data/Metro_Nashville_Police_Department_Calls_for_Service.csv', chunksize = 10000)
shots_fired = []
for chunk in chunks:
    chunk = chunk[chunk['Tencode Description'] == 'SHOTS FIRED']
    shots_fired.append(chunk)
189/14: shots_fired = pd.concat(shots_fired, ignore_index = 'True')
189/15: shots_fired
189/16: import sqlite3
189/17: from tqdm.notebook import tqdm
189/18:
db = sqlite3.connect('data/police_calls.sqlite')

for chunk in tqdm(pd.read_csv('data/Metro_Nashville_Police_Department_Calls_for_Service.csv', chunksize = 10000)):
    chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names
    chunk.to_sql('calls', db, if_exists = 'append', index = False)            # Append the chunk to a calls table
189/19: db.execute('CREATE INDEX tencode_description ON calls(tencode_description)')
189/20: db.close()
189/21:
db = sqlite3.connect('data/police_calls.sqlite') #connect to database for executing the query

query = "SELECT * FROM calls WHERE tencode_description = 'SHOTS FIRED'"

shots_sqlite = pd.read_sql(query, db)

db.close()
189/22: shorts_sqlite
189/23: shots_sqlite
189/24:
db = sqlite3.connect('data/police_calls.sqlite')
for chunk in tqdm(pd.read_csv('data/Metro_Nashville_Police_Department_Incidents.csv', 
                              chunksize = 10000)):   
    chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names   
    chunk.to_sql('incidents', db, if_exists = 'append', index = False)
189/25:
query = """

SELECT *
FROM calls
INNER JOIN incidents
ON calls.complaint_number = incidents.incident_number
WHERE tencode_description = 'SHOTS FIRED'
"""
189/26:
with sqlite3.connect('data/police_calls.sqlite') as db: 
    shots_sqlite = pd.read_sql(query, db)
190/1: import pandas as pd
190/2: import sqlite3
190/3: from tqdm.notebook import tqdm
190/4:
db = sqlite3.connect('data/police_calls.sqlite') #establish connection with database

db.execute('CREATE INDEX complaint_number ON calls(complaint_number)')
db.execute('CREATE INDEX incident_number ON incidents(incident_number)')

db.close()
190/5:
query = """

SELECT *
FROM calls
INNER JOIN incidents
ON calls.complaint_number = incidents.incident_number
WHERE tencode_description = 'SHOTS FIRED'
"""
190/6:
with sqlite3.connect('data/police_calls.sqlite') as db: 
    shots_sqlite = pd.read_sql(query, db)
190/7: shots_sqlite
191/1: import pandas as pd
191/2:
pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv',
           chunksize = 10000)
191/3:
chunks = pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv',
           chunksize = 10000)
191/4: chunks
191/5: chunks
191/6: chunk = next(chunks)
191/7: chunk
191/8: chunk[chunk['transaction_count'] >= 50]
191/9: chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)]
191/10: chunk = next(chunks)
191/11: chunk = next(chunks)
191/12: chunk
191/13: chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)]
191/14:
#Reading the NEPSS_NPI for glance
chunks_npi = pd.read_csv('data/npidata_pfile_20050523-20230212.csv',
           chunksize = 10000)
191/15: chunk_npi = next(chunk_npi)
191/16: chunk_npi = next(chunks_npi)
191/17:
chunk_npi = next(chunks_npi)
chunk_npi
191/18: chunk_npi.columns
191/19: chunk_npi.columns()
191/20: colname = chunk_npi.columns
191/21:
colname = chunk_npi.columns
colname
191/22: list(chunk_npi.column)
191/23: list(chunk_npi.columns)
191/24:
list(chunk_npi.columns)
chunk_npi.shape
191/25:
list(chunk_npi.columns)
#chunk_npi.shape
191/26:
list(chunk_npi.columns)
#chunk_npi.shape
chunk_npi.head(5)
191/27:
list(chunk_npi.columns)
#chunk_npi.shape
chunk_npi.describe()
191/28:
list(chunk_npi.columns)
#chunk_npi.shape
chunk_npi.describe
191/29:
list(chunk_npi.columns)
#chunk_npi.shape
chunk_npi.info()
191/30:
list(chunk_npi.columns)
#chunk_npi.shape
chunk_npi.info
191/31:
list(chunk_npi.columns)
#chunk_npi.shape
#chunk_npi[.info]
191/32: chunk_npi['Provider Other Organization Name'].value_counts()
191/33: chunk_npi['Provider Other Organization Name'].value_counts
191/34: chunk_npi['Provider Other Organization Name'].to_list()
191/35: chunk_npi['Provider Other Organization Name'].unique()
191/36:
chunk_npi = next(chunks_npi)
chunk_npi
191/37:
chunk_npi = next(chunks_npi)
chunk_npi
191/38:
chunk_npi = next(chunks_npi)
chunk_npi
191/39: chunk_npi['Provider Organization Name (Legal Business Name)'].unique()
191/40: chunk_npi['Provider Organization Name (Legal Business Name)'].unique()
191/41:
chunk_npi['Provider Organization Name (Legal Business Name)'].unique()
chunk_npi['Healthcare Provider Taxonomy Group_1'].unique()
191/42:
chunk_npi['Provider Organization Name (Legal Business Name)'].unique()
chunk_npi['Healthcare Provider Taxonomy Group_2'].unique()
191/43:
chunk_npi['Provider Organization Name (Legal Business Name)'].unique()
chunk_npi['Healthcare Provider Taxonomy Group_3'].unique()
191/44:
chunk_npi['Provider Organization Name (Legal Business Name)'].unique()
chunk_npi['Healthcare Provider Primary Taxonomy Switch_1'].unique()
191/45:
chunk_npi['Provider Organization Name (Legal Business Name)'].unique()
chunk_npi['Healthcare Provider Primary Taxonomy Switch_1'].value_counts()
191/46:
chunk_npi['Provider Organization Name (Legal Business Name)'].unique()
chunk_npi['Healthcare Provider Primary Taxonomy Switch_2'].value_counts()
191/47:
chunk_npi['Provider Organization Name (Legal Business Name)'].unique()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
191/48:
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
#chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
191/49:
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
191/50:
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
chunk_npi[ 'Healthcare Provider Primary Taxonomy Switch_1'].valur_counts()
191/51:
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
chunk_npi[ 'Healthcare Provider Primary Taxonomy Switch_1'].value_counts()
191/52:
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
chunk_npi[ 'Healthcare Provider Primary Taxonomy Switch_2'].value_counts()
191/53:
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
chunk_npi[ 'Healthcare Provider Primary Taxonomy Switch_1'].value_counts()
191/54:
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
chunk_npi[ 'Healthcare Provider Primary Taxonomy Switch_15'].value_counts()
191/55:
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
chunk_npi[ 'Healthcare Provider Primary Taxonomy Switch_10'].value_counts()
191/56:
#subsetting the df
npi = chunk_npi[['NPI',
 'Entity Type Code',
 'Provider Organization Name (Legal Business Name)',
 'Provider Last Name (Legal Name)',
 'Provider First Name',
 'Provider Middle Name',
 'Provider Name Prefix Text',
 'Provider Name Suffix Text',
 'Provider Credential Text',
 'Provider First Line Business Practice Location Address',
 'Provider Second Line Business Practice Location Address',
 'Provider Business Practice Location Address City Name',
 'Provider Business Practice Location Address State Name',
 'Provider Business Practice Location Address Postal Code',
           'Healthcare Provider Taxonomy Code_1',
           'Healthcare Provider Primary Taxonomy Switch_1',
           'Healthcare Provider Taxonomy Code_2',
           'Healthcare Provider Primary Taxonomy Switch_2', 
           'Healthcare Provider Taxonomy Code_3',
           'Healthcare Provider Primary Taxonomy Switch_3',     
           'Healthcare Provider Taxonomy Code_4',
           'Healthcare Provider Primary Taxonomy Switch_4',
           'Healthcare Provider Taxonomy Code_5',
           'Healthcare Provider Primary Taxonomy Switch_5',
            'Healthcare Provider Taxonomy Code_6',
           'Healthcare Provider Primary Taxonomy Switch_6',
        'Healthcare Provider Taxonomy Code_7',
           'Healthcare Provider Primary Taxonomy Switch_7',
                 'Healthcare Provider Taxonomy Code_8',
           'Healthcare Provider Primary Taxonomy Switch_8',
                 'Healthcare Provider Taxonomy Code_9',
           'Healthcare Provider Primary Taxonomy Switch_9',
                 'Healthcare Provider Taxonomy Code_10',
           'Healthcare Provider Primary Taxonomy Switch_10',
                 'Healthcare Provider Taxonomy Code_11',
           'Healthcare Provider Primary Taxonomy Switch_11',
                 'Healthcare Provider Taxonomy Code_12',
           'Healthcare Provider Primary Taxonomy Switch_12',
                 'Healthcare Provider Taxonomy Code_13',
           'Healthcare Provider Primary Taxonomy Switch_13',
                 'Healthcare Provider Taxonomy Code_14',
           'Healthcare Provider Primary Taxonomy Switch_14',
                 'Healthcare Provider Taxonomy Code_15',
           'Healthcare Provider Primary Taxonomy Switch_15'
            ]]
191/57: npi
191/58:
for column in (chunk_npi.columns):
    
    # Select column contents by column
    # name using [] operator
    columnSeriesObj = cunk_npi[column]
    print('Column Name : ', column)
    print('Column Contents : ', columnSeriesObj.values)
191/59:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
191/60:
#reading HOP.csv for overveiw and glance
chunks = pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv',
           chunksize = 10000)
191/61: chunks
191/62: chunk = next(chunks)
191/63: chunk
191/64: chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)]
191/65:
#Reading the NEPSS_NPI for glance
chunks_npi = pd.read_csv('data/npidata_pfile_20050523-20230212.csv',
           chunksize = 10000)
191/66:
chunk_npi = next(chunks_npi)
chunk_npi
191/67:
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
chunk_npi[ 'Healthcare Provider Primary Taxonomy Switch_10'].value_counts()
191/68:
#subsetting the df
npi = chunk_npi[['NPI',
 'Entity Type Code',
 'Provider Organization Name (Legal Business Name)',
 'Provider Last Name (Legal Name)',
 'Provider First Name',
 'Provider Middle Name',
 'Provider Name Prefix Text',
 'Provider Name Suffix Text',
 'Provider Credential Text',
 'Provider First Line Business Practice Location Address',
 'Provider Second Line Business Practice Location Address',
 'Provider Business Practice Location Address City Name',
 'Provider Business Practice Location Address State Name',
 'Provider Business Practice Location Address Postal Code',
           'Healthcare Provider Taxonomy Code_1',
           'Healthcare Provider Primary Taxonomy Switch_1',
           'Healthcare Provider Taxonomy Code_2',
           'Healthcare Provider Primary Taxonomy Switch_2', 
           'Healthcare Provider Taxonomy Code_3',
           'Healthcare Provider Primary Taxonomy Switch_3',     
           'Healthcare Provider Taxonomy Code_4',
           'Healthcare Provider Primary Taxonomy Switch_4',
           'Healthcare Provider Taxonomy Code_5',
           'Healthcare Provider Primary Taxonomy Switch_5',
            'Healthcare Provider Taxonomy Code_6',
           'Healthcare Provider Primary Taxonomy Switch_6',
        'Healthcare Provider Taxonomy Code_7',
           'Healthcare Provider Primary Taxonomy Switch_7',
                 'Healthcare Provider Taxonomy Code_8',
           'Healthcare Provider Primary Taxonomy Switch_8',
                 'Healthcare Provider Taxonomy Code_9',
           'Healthcare Provider Primary Taxonomy Switch_9',
                 'Healthcare Provider Taxonomy Code_10',
           'Healthcare Provider Primary Taxonomy Switch_10',
                 'Healthcare Provider Taxonomy Code_11',
           'Healthcare Provider Primary Taxonomy Switch_11',
                 'Healthcare Provider Taxonomy Code_12',
           'Healthcare Provider Primary Taxonomy Switch_12',
                 'Healthcare Provider Taxonomy Code_13',
           'Healthcare Provider Primary Taxonomy Switch_13',
                 'Healthcare Provider Taxonomy Code_14',
           'Healthcare Provider Primary Taxonomy Switch_14',
                 'Healthcare Provider Taxonomy Code_15',
           'Healthcare Provider Primary Taxonomy Switch_15'
            ]]
191/69:
for column in (npi.columns):
    
    # Select column contents by column
    # name using [] operator
    columnSeriesObj = cunk_npi[column]
    print('Column Name : ', column)
    print('Column Contents : ', columnSeriesObj.values)
191/70:
for column in (npi.columns):
    
    # Select column contents by column
    # name using [] operator
    columnSeriesObj = npi[column]
    print('Column Name : ', column)
    print('Column Contents : ', columnSeriesObj.values)
191/71:
x = []

for column in (npi.columns):
    
    # Select column contents by column
    # name using [] operator
    columnSeriesObj = npi[column]
    print('Column Name : ', column)
    print('Column Contents : ', columnSeriesObj.values)
for i in range (1:16):
    columnSeriesObj = npi[column]
    if f'Healthcare Provider Primary Taxonomy Switch_1' == 'Y'
    x.append[;1:14]
 #   else 
 
print(x)
#use the loop for selecting the rows with switch as 'Y' and collect all codes in code column to create tiny tables and then concat them
191/72:
x = []

for column in (npi.columns):
    
    # Select column contents by column
    # name using [] operator
    columnSeriesObj = npi[column]
    print('Column Name : ', column)
    print('Column Contents : ', columnSeriesObj.values)
for i in range (1:16):
    columnSeriesObj = npi[column]
    if f'Healthcare Provider Primary Taxonomy Switch_1' == 'Y'
    x.append[;1:14]
 #   else 
 
print(x)
#use the loop for selecting the rows with switch as 'Y' and collect all codes in code column to create tiny tables and then concat them
191/73:
x = []


for i in range (1:16):
    columnSeriesObj = npi[column]
    if f'Healthcare Provider Primary Taxonomy Switch_1' == 'Y'
    x.append[;1:14]
 #   else 
 
print(x)
#use the loop for selecting the rows with switch as 'Y' and collect all codes in code column to create tiny tables and then concat them
191/74:
i=1
npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'Y']
191/75:
i=1
# df2 = df1[['NPI',
#  'Entity Type Code',
#  'Provider Organization Name (Legal Business Name)',
#  'Provider Last Name (Legal Name)',
#  'Provider First Name',
#  'Provider Middle Name',
#  'Provider Name Prefix Text',
#  'Provider Name Suffix Text',
#  'Provider Credential Text',
#  'Provider First Line Business Practice Location Address',
#  'Provider Second Line Business Practice Location Address',
#  'Provider Business Practice Location Address City Name',
#  'Provider Business Practice Location Address State Name',
#  'Provider Business Practice Location Address Postal Code']]

npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'Y'][['NPI',
 'Entity Type Code',
 'Provider Organization Name (Legal Business Name)',
 'Provider Last Name (Legal Name)',
 'Provider First Name',
 'Provider Middle Name',
 'Provider Name Prefix Text',
 'Provider Name Suffix Text',
 'Provider Credential Text',
 'Provider First Line Business Practice Location Address',
 'Provider Second Line Business Practice Location Address',
 'Provider Business Practice Location Address City Name',
 'Provider Business Practice Location Address State Name',
 'Provider Business Practice Location Address Postal Code']]
191/76:
i=1

npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'Y'][['NPI',
 'Entity Type Code',
 'Provider Organization Name (Legal Business Name)',
 'Provider Last Name (Legal Name)',
 'Provider First Name',
 'Provider Middle Name',
 'Provider Name Prefix Text',
 'Provider Name Suffix Text',
 'Provider Credential Text',
 'Provider First Line Business Practice Location Address',
 'Provider Second Line Business Practice Location Address',
 'Provider Business Practice Location Address City Name',
 'Provider Business Practice Location Address State Name',
 'Provider Business Practice Location Address Postal Code', f'Healthcare Provider Taxonomy Code_{i}']]
191/77:
i=1

npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'Y'][['NPI',
 'Entity Type Code',
 'Provider Organization Name (Legal Business Name)',
 'Provider Last Name (Legal Name)',
 'Provider First Name',
 'Provider Middle Name',
 'Provider Name Prefix Text',
 'Provider Name Suffix Text',
 'Provider Credential Text',
 'Provider First Line Business Practice Location Address',
 'Provider Second Line Business Practice Location Address',
 'Provider Business Practice Location Address City Name',
 'Provider Business Practice Location Address State Name',
 'Provider Business Practice Location Address Postal Code', f'Healthcare Provider Taxonomy Code_{i}']].rename({f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'})
191/78:
i=1

npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'Y'][['NPI',
 'Entity Type Code',
 'Provider Organization Name (Legal Business Name)',
 'Provider Last Name (Legal Name)',
 'Provider First Name',
 'Provider Middle Name',
 'Provider Name Prefix Text',
 'Provider Name Suffix Text',
 'Provider Credential Text',
 'Provider First Line Business Practice Location Address',
 'Provider Second Line Business Practice Location Address',
 'Provider Business Practice Location Address City Name',
 'Provider Business Practice Location Address State Name',
 'Provider Business Practice Location Address Postal Code', f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'})
191/79:
#i=1
x = []
for i in range (1:16):
    x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/80:
#i=1
x = []
for i in range (1,16):
    x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/81: print(x)
191/82: pd.concat(x)
191/83: npi['Healthcare Provider Primary Taxonomy Switch_1'].value_counts()
191/84: npi['Healthcare Provider Primary Taxonomy Switch_2'].value_counts()
191/85: npi['Healthcare Provider Primary Taxonomy Switch_3'].value_counts()
191/86: npi['Healthcare Provider Primary Taxonomy Switch_4'].value_counts()
191/87: npi['Healthcare Provider Primary Taxonomy Switch_5'].value_counts()
191/88: npi['Healthcare Provider Primary Taxonomy Switch_10'].value_counts()
191/89: npi['Healthcare Provider Primary Taxonomy Switch_6'].value_counts()
191/90: npi['Healthcare Provider Primary Taxonomy Switch_7'].value_counts()
191/91: npi['Healthcare Provider Primary Taxonomy Switch_8'].value_counts()
191/92: npi['Healthcare Provider Primary Taxonomy Switch_1'].value_counts()
191/93: npi['Healthcare Provider Primary Taxonomy Switch_1'].value_counts(dropna = False)
191/94: npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'X']
191/95:
i=1
npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'X']
191/96:
#finding the switch with X
with_x = []
for i in range (1,16):
    x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'X'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/97: pd.concat(with_x)
191/98:
#finding the switch with X
with_x = []
for i in range (1,16):
    x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'X'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/99: pd.concat(with_x)
191/100:
#finding the switch with X
with_x = []
for i in range (1,16):
    x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'X'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/101: pd.concat(with_x)
191/102: with_x
191/103:
#finding the switch with X
with_x = []
for i in range (1,16):
    with_x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'X'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/104: with_x
191/105: pd.concat(with_x)
191/106:
#finding the switch with X
with_x = []
for i in range (1,16):
    with_x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'N'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/107: pd.concat(with_x)
191/108: yes_code = pd.concat(x)
191/109:
pd.merge(left = npi[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left")
191/110: npi[npi['NPI']== 1740284231]
191/111:
pd.merge(left = npi[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
191/112:
pd.merge(left = npi[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()['NPI'].value_counts()
191/113: npi[npi['NPI']== 1477551547]
191/114:
npi[npi['NPI']== 1477551547]
npi.loc[8422]
191/115:
#i=1
x = []
for i in range (1,16):
    x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/116: yes_code = pd.concat(x)
191/117:
yes_code = pd.concat(x)
yes_code.shape
191/118:
pd.merge(left = npi[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()['NPI'].value_counts()
191/119:
npi[npi['NPI']== 1477551547]
npi.loc[8422]
191/120:
pd.merge(left = npi[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
191/121:
npi[npi['NPI']== 1477551547]
npi.loc[8422]
npi['NPI'].unique()
191/122:
npi[npi['NPI']== 1477551547]
npi.loc[8422]
npi['NPI'].nunique()
191/123:
#getting the feel of the read in chunk 
#list(chunk_npi.columns)
#chunk_npi.shape
chunk_npi.info()
191/124:
#getting the feel of the read in chunk 

chunk_npi.shape 
#chunk_npi.info()
#list(chunk_npi.columns) getting the name of the columns as a list to see
191/125:
#getting the feel of the read in chunk 
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
chunk_npi[ 'Healthcare Provider Primary Taxonomy Switch_10'].value_counts()
191/126:
#getting the feel of the read in chunk 
chunk_npi['Provider Organization Name (Legal Business Name)'].value_counts()
chunk_npi['Healthcare Provider Taxonomy Code_1'].value_counts()
chunk_npi[ 'Healthcare Provider Primary Taxonomy Switch_1'].value_counts()
191/127:
x = []

for column in (npi.columns):
for i in range (1,16):
    columnSeriesObj = npi[column]
    if f'Healthcare Provider Primary Taxonomy Switch_1' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_1'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_2' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_2'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_4' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_4'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    else
    

print(x)
#use the loop for selecting the rows with switch as 'Y' and collect all codes in code column to create tiny tables and then concat them
191/128:
x = []

for column in (npi.columns):
#for i in range (1,16):
    columnSeriesObj = npi[column]
    if f'Healthcare Provider Primary Taxonomy Switch_1' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_1'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_2' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_2'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_4' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_4'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y'
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    else
    

print(x)
#use the loop for selecting the rows with switch as 'Y' and collect all codes in code column to create tiny tables and then concat them
191/129:
x = []

for column in (npi.columns):
#for i in range (1,16):
    columnSeriesObj = npi[column]
    if f'Healthcare Provider Primary Taxonomy Switch_1' == 'Y',
    x.append['Healthcare Provider Taxonomy Code_1'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_2' == 'Y',
    x.append['Healthcare Provider Taxonomy Code_2'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y',
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_4' == 'Y',
    x.append['Healthcare Provider Taxonomy Code_4'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y',
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y',
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y',
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_3' == 'Y',
    x.append['Healthcare Provider Taxonomy Code_3'.values]
    else
    

print(x)
#use the loop for selecting the rows with switch as 'Y' and collect all codes in code column to create tiny tables and then concat them
191/130:
#Steps for my loop
x=[]
fot i in range [1:16]:
if f'Healthcare Provider Primary Taxonomy Switch_{i}' == 'Y':
    x.append[f'Healthcare Provider Taxonomy Code_{i}'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_{i+1}' == 'Y':
    x.append[f'Healthcare Provider Taxonomy Code_{i+1}'.values]
    else:
        x.append['none']
191/131:
#Steps for my loop
x=[]
fot i in range [1,16]:
if f'Healthcare Provider Primary Taxonomy Switch_{i}' == 'Y':
    x.append[f'Healthcare Provider Taxonomy Code_{i}'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_{i+1}' == 'Y':
    x.append[f'Healthcare Provider Taxonomy Code_{i+1}'.values]
    else:
        x.append['none']
191/132:
#Steps for my loop
x=[]
for i in range [1,16]:
    if f'Healthcare Provider Primary Taxonomy Switch_{i}' == 'Y':
    x.append[f'Healthcare Provider Taxonomy Code_{i}'.values]
    elif
    f'Healthcare Provider Primary Taxonomy Switch_{i+1}' == 'Y':
    x.append[f'Healthcare Provider Taxonomy Code_{i+1}'.values]
    else:
        x.append['none']
191/133:
#Steps for my loop
x=[]
for i in range [1,16]:
    if
        f'Healthcare Provider Primary Taxonomy Switch_{i}' == 'Y':
        x.append[f'Healthcare Provider Taxonomy Code_{i}'.values]
    elif
        f'Healthcare Provider Primary Taxonomy Switch_{i+1}' == 'Y':
        x.append[f'Healthcare Provider Taxonomy Code_{i+1}'.values]
    else:
        x.append['none']
191/134:
#Steps for my loop
x=[]
for i in range [1,16]:
    if f'Healthcare Provider Primary Taxonomy Switch_{i}' == 'Y':
        x.append[f'Healthcare Provider Taxonomy Code_{i}'.values]
    elif
        f'Healthcare Provider Primary Taxonomy Switch_{i+1}' == 'Y':
        x.append[f'Healthcare Provider Taxonomy Code_{i+1}'.values]
    else:
        x.append['none']
191/135:
#Steps for my loop
x=[]
for i in range [1,16]:
    if f'Healthcare Provider Primary Taxonomy Switch_{i}' == 'Y':
        x.append[f'Healthcare Provider Taxonomy Code_{i}'.values]
    elif f'Healthcare Provider Primary Taxonomy Switch_{i+1}' == 'Y':
        x.append[f'Healthcare Provider Taxonomy Code_{i+1}'.values]
    else:
        x.append['none']
191/136:
#Steps for my loop
x = []
for i in range [1,16]:
    if f'Healthcare Provider Primary Taxonomy Switch_{i}' == 'Y':
        x.append[f'Healthcare Provider Taxonomy Code_{i}'.values]
    elif f'Healthcare Provider Primary Taxonomy Switch_{i+1}' == 'Y':
        x.append[f'Healthcare Provider Taxonomy Code_{i+1}'.values]
    else:
        x.append['none']
191/137:
#i=1
x = []
for i in range (1,16):
    x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/138:
pd.merge(left = npi[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = pd.concat(x), how = "left").drop_duplicates()
191/139:
# to get all the "Y" taxonomy codes
x = []
for i in range (1,16):
    x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ]== 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/140:
#x is coming as multiple df so to stich them together use the pd.concat 
yes_code = pd.concat(x)
yes_code.shape
191/141:
not_yes = []
for i in range (1,16):
    not-yes.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] != 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/142:
not_yes = []
for i in range (1,16):
    not_yes.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] != 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/143:
not_yes = []
for i in range (1,16):
    not_yes.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] != 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'})).drop_duplicates()
    pd.concat(not_yes)
191/144:
not_yes = []
for i in range (1,16):
    not_yes.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] != 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'})).drop_duplicates()
pd.concat(not_yes)
191/145:
not_yes = []
for i in range (1,16):
    not_yes.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] != 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'})).drop_duplicates()
191/146:
not_yes = []
for i in range (1,16):
    not_yes.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] != 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'})).drop_duplicates()
191/147:
not_yes = []
for i in range (1,16):
    not_yes.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] != 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
191/148:
not_yes = []
for i in range (1,16):
    not_yes.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] != 'Y'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
    
pd.concat(not_yes)
195/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
195/2:
db = sqlite3.connect('data/hop_referal.sqlite')

for chunk in tqdm(pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv', chunksize = 10000)):
    chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names
    chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)]
    chunk.to_sql('hop', db, if_exists = 'append', index = False)            # Append the chunk to a calls table
195/3:
db = sqlite3.connect('data/hop_referal.sqlite')

for chunk in tqdm(pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv', chunksize = 10000):
    chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names
    chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)]
    chunk.to_sql('hop', db, if_exists = 'append', index = False)            # Append the chunk to a calls table
195/4:
db = sqlite3.connect('data/hop_referal.sqlite')

for chunk in tqdm(pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv', chunksize = 10000)):
    chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names
    chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)] #filter the desired row
    chunk.to_sql('hop', db, if_exists = 'append', index = False)            # Append the chunk to a calls table
195/5:
db = sqlite3.connect('data/hop_referal.sqlite')

for chunk in tqdm(pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv', chunksize = 10000)):
    chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names
    chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)] #filter the desired row
    chunk.to_sql('hop', db, if_exists = 'append', index = False)            # Append the chunk to a calls table
191/149:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
191/150:
#reading HOP.csv for overveiw and glance
chunks = pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv',
           chunksize = 10000)
195/6:
db = sqlite3.connect('data/hop_referal.sqlite')

for chunk in tqdm(pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv',
           chunksize = 10000)):
    chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names
    chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)] #filter the desired row
    chunk.to_sql('hop', db, if_exists = 'append', index = False)            # Append the chunk to a calls table
195/7:
db = sqlite3.connect('data/hop_referal.sqlite')

for chunk in tqdm(pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv',
           chunksize = 10000)):
    #chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names
    chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)] #filter the desired row
    chunk.to_sql('hop', db, if_exists = 'append', index = False)            # Append the chunk to a calls table
197/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
197/2:
db = sqlite3.connect('data/hop_referal.sqlite')

for chunk in tqdm(pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv',
           chunksize = 10000)):
    #chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names
    chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)] #filter the desired row
    chunk.to_sql('hop', db, if_exists = 'append', index = False)            # Append the chunk to a calls table
197/3: db.close
197/4: db.close()
197/5:
db = sqlite3.connect('data/hop_referal.sqlite')

for chunk in tqdm(pd.read_csv('data/DocGraph_Hop_Teaming_2018.csv',
           chunksize = 10000)):
    #chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names
    chunk = chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait']<50)] #filter the desired row
    chunk.to_sql('hop', db, if_exists = 'append', index = False)            # Append the chunk to a calls table
197/6: db.close()
199/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
199/2: db.close()
199/3: db = sqlite3.connect('data/hop_referal.sqlite')
199/4:
db = sqlite3.connect('data/hop_referal.sqlite') #connect to database for executing the query

query = "SELECT * FROM calls LIMIT 10"

shots_sqlite = pd.read_sql(query, db)

db.close()
199/5:
db = sqlite3.connect('data/hop_referal.sqlite') #connect to database for executing the query

query = "SELECT * FROM hop LIMIT 10"

shots_sqlite = pd.read_sql(query, db)

db.close()
199/6:
query = """
SELECT *
FROM hop
LIMIT 10
""
199/7:
query = """
SELECT *
FROM hop
LIMIT 10
"""
199/8:
db = sqlite3.connect('data/hop_referal.sqlite') #connect to database for executing the query

query = "SELECT * FROM hop LIMIT 10"

shots_sqlite = pd.read_sql(query, db)

db.close()
shots_sqlite
199/9:
query = """
SELECT *
FROM hop
LIMIT 10
"""
199/10:
db = sqlite3.connect('data/hop_referal.sqlite') #connect to database for executing the query

query = "SELECT * FROM hop LIMIT 10"

hop_sqlite = pd.read_sql(query, db)

db.close()
hop_sqlite
199/11:
query = "SELECT * FROM hop LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    hop_sqlite = pd.read_sql(query, db)
199/12: hop_sqlite
200/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
200/2:
#reading CBSA.csv
cbsa_chunks = pd.read_csv('data/ZIP_TRACT_122021.xlsx',
           chunksize = 10000)
200/3:
#reading CBSA.csv
cbsa_chunks = pd.read_excel('data/ZIP_TRACT_122021.xlsx',
           chunksize = 10000)
200/4:
#reading CBSA.csv
cbsa_chunks = pd.read_excel('data/ZIP_TRACT_122021.xlsx')
200/5:
#reading CBSA.csv
cbsa_chunks = pd.read_excel('data/ZIP_TRACT_122021.xlsx')
cbsa_chunk
200/6:
#reading CBSA.csv
cbsa_chunks = pd.read_excel('data/ZIP_TRACT_122021.xlsx')
199/13:
##reading the CBSA tabe in the 
cbsa = pd.read_excel('data/ZIP_TRACT_122021.xlsx')
200/7: cbsa_chunks.head(10)
199/14:
##reading the CBSA tabe in the 
cbsa = pd.read_excel('data/ZIP_TRACT_122021.xlsx')
db = sqlite3.connect('data/hop_referal.sqlite')
cbsa.to_sql('hop', db, if_exists = 'append', index = False)
199/15:
##reading the CBSA tabe in the 
cbsa = pd.read_excel('data/ZIP_TRACT_122021.xlsx')
db = sqlite3.connect('data/hop_referal.sqlite')
cbsa.to_sql('cbsa', db, if_exists = 'append', index = False)
199/16:
query = "SELECT * FROM cbsa LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
199/17: cbsa_sqlite3
199/18: cbsa_sqlite
199/19: cbsa.shape
200/8:
cbsa_chunks.head(10)
cbsa_chunks.shape
199/20: cbsa2 = pd.read_excel('data/ZIP_CBSA_122021.xlsx')
199/21: cbsa2.shape
199/22:
db = sqlite3.connect('data/hop_referal.sqlite')
#DROP TABLE [IF EXISTS] [schema_name.]cbsa;
DROP TABLE cbsa
199/23:
db = sqlite3.connect('data/hop_referal.sqlite')
DROP TABLE [IF EXISTS] [hop_referal.]cbsa;
#DROP TABLE cbsa
200/9: chunks = pd.read_csv('data/nucc_taxonomy_230.csv', chunksize = 10000)
200/10:
chunks = pd.read_csv('data/nucc_taxonomy_230.csv', chunksize = 10000)
chunks
200/11:
chunks = pd.read_csv('data/nucc_taxonomy_230.csv', chunksize = 10000)
chunks 
chunk = next(chunk)
200/12:
chunks = pd.read_csv('data/nucc_taxonomy_230.csv', chunksize = 10000)
chunks 
chunk = next(chunks)
200/13:
chunks = pd.read_csv('data/nucc_taxonomy_230.csv', chunksize = 10000)
#chunks 
chunk = next(chunks)
200/14: pd.read_csv('data/nucc_taxonomy_230.csv', chunksize = 10000)
200/15: chunk = pd.read_csv('data/nucc_taxonomy_230.csv', chunksize = 10000)
200/16: trans = next(chunk)
200/17: chunk = pd.read_csv('data/nucc_taxonomy_230.csv')
200/18: pd.read_csv('data/nucc_taxonomy_230.csv')
200/19:
cbsa_chunks.head(10)
cbsa_chunks.shape
199/24:
db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("DROP TABLE cbsa")
#DROP TABLE [IF EXISTS] [hop_referal.]cbsa;
#DROP TABLE cbsa
199/25:
query = "SELECT * FROM cbsa LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
199/26:
##reading the CBSA tabe in the 
cbsa = pd.read_excel('data/ZIP_CBSA_122021.xlsx')
db = sqlite3.connect('data/hop_referal.sqlite')
cbsa.to_sql('cbsa', db, if_exists = 'append', index = False)
199/27: cbsa.shape
199/28:
query = "SELECT * FROM cbsa LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
199/29: cbsa_sqlite
199/30:
#taxonomy code file
pd.read_csv('data/nucc_taxonomy_230 (1).csv')
200/20: pd.read_csv('data/nucc_taxonomy_230.csv', encoding = "utf-16")
200/21: pd.read_csv('data/nucc_taxonomy_230.csv', encoding = "latin")
199/31:
#taxonomy code file
pd.read_csv('data/nucc_taxonomy_230 (1).csv', nrows = 320)
199/32:
#taxonomy code file
pd.read_csv('data/nucc_taxonomy_230 (1).csv', nrows = 321)
199/33:
#taxonomy code file
pd.read_csv('data/nucc_taxonomy_230 (1).csv', nrows = 322)
199/34:
#taxonomy code file
pd.read_csv('data/nucc_taxonomy_230 (1).csv', encodeing = "latin")
199/35:
#taxonomy code file
pd.read_csv('data/nucc_taxonomy_230 (1).csv', encoding = "latin")
199/36:
#taxonomy code file
tax = pd.read_csv('data/nucc_taxonomy_230 (1).csv', encoding = "latin")
199/37:
#taxonomy code file
tax = pd.read_csv('data/nucc_taxonomy_230 (1).csv', encoding = "latin")
tax[322, ]
199/38:
#taxonomy code file
tax = pd.read_csv('data/nucc_taxonomy_230 (1).csv', encoding = "latin")
199/39:
#taxonomy code file
tax = pd.read_csv('data/nucc_taxonomy_230 (1).csv', encoding = "latin")
tax
199/40:
#taxonomy code file
tax = pd.read_csv('data/nucc_taxonomy_230 (1).csv', encoding = "latin")
tax.loc[320:325, ]
199/41:
db = sqlite3.connect('Data/hop_referal.sqlite')

'''
SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';
    
    '''
199/42:
db = sqlite3.connect('data/hop_referal.sqlite')

'''
SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';
    
'''
199/43:
db = sqlite3.connect('data/hop_referal.sqlite')

"""
SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';
    
"""
199/44: tax.shape
199/45:
#reading the taxonomy code file
tax = pd.read_csv('data/nucc_taxonomy_230 (1).csv', encoding = "latin") #around row 321 (using nrows 321) read_csv does not work so the error is fixed using the encoding ='latin'
tax.loc[320:325, ] #looking between the lines 320 to 325
tax.shape
199/46:
db = sqlite3.connect('data/hop_referal.sqlite')
tax.columns = [x.lower().replace(' ', '_') for x in chunk.columns]      # Clean up the column names  for db
tax.to_sql('taxonomy', db, if_exists = 'append', index = False)
199/47:
db = sqlite3.connect('data/hop_referal.sqlite')
tax.columns = [x.lower().replace(' ', '_') for x in tax.columns]      # Clean up the column names  for db
tax.to_sql('taxonomy', db, if_exists = 'append', index = False)
199/48:
#checking the taxonomy table 
query = "SELECT * FROM taxonomy LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_sqlite = pd.read_sql(query, db)
199/49:
#checking the taxonomy table 
query = "SELECT * FROM taxonomy LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_sqlite = pd.read_sql(query, db)
tax_sqlite
199/50:
db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetch_all()
199/51:
db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
199/52: db.close()
199/53: db.close()
200/22:
columns_to_use = ['NPI',
 'Entity Type Code',
 'Provider Organization Name (Legal Business Name)',
 'Provider Last Name (Legal Name)',
 'Provider First Name',
 'Provider Middle Name',
 'Provider Name Prefix Text',
 'Provider Name Suffix Text',
 'Provider Credential Text',
 'Provider First Line Business Practice Location Address',
 'Provider Second Line Business Practice Location Address',
 'Provider Business Practice Location Address City Name',
 'Provider Business Practice Location Address State Name',
 'Provider Business Practice Location Address Postal Code',
           'Healthcare Provider Taxonomy Code_1',
           'Healthcare Provider Primary Taxonomy Switch_1',
           'Healthcare Provider Taxonomy Code_2',
           'Healthcare Provider Primary Taxonomy Switch_2', 
           'Healthcare Provider Taxonomy Code_3',
           'Healthcare Provider Primary Taxonomy Switch_3',     
           'Healthcare Provider Taxonomy Code_4',
           'Healthcare Provider Primary Taxonomy Switch_4',
           'Healthcare Provider Taxonomy Code_5',
           'Healthcare Provider Primary Taxonomy Switch_5',
            'Healthcare Provider Taxonomy Code_6',
           'Healthcare Provider Primary Taxonomy Switch_6',
        'Healthcare Provider Taxonomy Code_7',
           'Healthcare Provider Primary Taxonomy Switch_7',
                 'Healthcare Provider Taxonomy Code_8',
           'Healthcare Provider Primary Taxonomy Switch_8',
                 'Healthcare Provider Taxonomy Code_9',
           'Healthcare Provider Primary Taxonomy Switch_9',
                 'Healthcare Provider Taxonomy Code_10',
           'Healthcare Provider Primary Taxonomy Switch_10',
                 'Healthcare Provider Taxonomy Code_11',
           'Healthcare Provider Primary Taxonomy Switch_11',
                 'Healthcare Provider Taxonomy Code_12',
           'Healthcare Provider Primary Taxonomy Switch_12',
                 'Healthcare Provider Taxonomy Code_13',
           'Healthcare Provider Primary Taxonomy Switch_13',
                 'Healthcare Provider Taxonomy Code_14',
           'Healthcare Provider Primary Taxonomy Switch_14',
                 'Healthcare Provider Taxonomy Code_15',
           'Healthcare Provider Primary Taxonomy Switch_15'
            ]

columns_kept = ['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code',
         'taxonomy_code']
200/23:
for chunks in tqdm(pd.read_csv('data/npidata_pfile_20050523-20230212.csv',
           chunksize = 10000)):
    x = []
    for i in range (1,16):
    with_x.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] != 'N'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
200/24:
for chunks in tqdm(pd.read_csv('data/npidata_pfile_20050523-20230212.csv',
           chunksize = 10000)):
    x_select = []
    for i in range (1,16):
    x_select.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] != 'N'][['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
200/25:
for chunks in tqdm(pd.read_csv('data/npidata_pfile_20050523-20230212.csv',
           chunksize = 10000)):
x_select = []
    for i in range (1,16):
    x_select.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] == 'Y']
                    [['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
200/26:
for chunks in tqdm(pd.read_csv('data/npidata_pfile_20050523-20230212.csv',
           chunksize = 10000)):
    x_select = []
    for i in range (1,16):
        x_select.append(npi[npi[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] == 'Y']
                    [['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
200/27:
for chunk in tqdm(pd.read_csv('data/npidata_pfile_20050523-20230212.csv', usecols = columns_to_use,
           chunksize = 10000)):
    x_select = []
    for i in range (1,16):
        x_select.append(chunk[chunk[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] == 'Y']
                    [['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
200/28:
yes_code = pd.concat(x)
npi_data = pd.merge(left = chunk[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
200/29:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
200/30: npi_data.shape
200/31:
npi_data.shape
npi_data.head(5)
200/32:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
final.columns = [x.lower().replace(' ', '_') for x in final.columns]
200/33:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
npi_data.columns = [x.lower().replace(' ', '_') for x in final.columns]
200/34:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
npi_data.columns = [x.lower().replace(' ', '_') for x in npi_data.columns]
200/35:
npi_data.shape
npi_data.head(5)
200/36:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk, right = yes_code, how = "left")[columns_kept].drop_duplicates()
npi_data.columns = [x.lower().replace(' ', '_') for x in npi_data.columns]
200/37:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk[columns_kept], right = yes_code, how = "left").drop_duplicates()
npi_data.columns = [x.lower().replace(' ', '_') for x in npi_data.columns]
200/38:
columns_to_use = ['NPI',
 'Entity Type Code',
 'Provider Organization Name (Legal Business Name)',
 'Provider Last Name (Legal Name)',
 'Provider First Name',
 'Provider Middle Name',
 'Provider Name Prefix Text',
 'Provider Name Suffix Text',
 'Provider Credential Text',
 'Provider First Line Business Practice Location Address',
 'Provider Second Line Business Practice Location Address',
 'Provider Business Practice Location Address City Name',
 'Provider Business Practice Location Address State Name',
 'Provider Business Practice Location Address Postal Code',
           'Healthcare Provider Taxonomy Code_1',
           'Healthcare Provider Primary Taxonomy Switch_1',
           'Healthcare Provider Taxonomy Code_2',
           'Healthcare Provider Primary Taxonomy Switch_2', 
           'Healthcare Provider Taxonomy Code_3',
           'Healthcare Provider Primary Taxonomy Switch_3',     
           'Healthcare Provider Taxonomy Code_4',
           'Healthcare Provider Primary Taxonomy Switch_4',
           'Healthcare Provider Taxonomy Code_5',
           'Healthcare Provider Primary Taxonomy Switch_5',
            'Healthcare Provider Taxonomy Code_6',
           'Healthcare Provider Primary Taxonomy Switch_6',
        'Healthcare Provider Taxonomy Code_7',
           'Healthcare Provider Primary Taxonomy Switch_7',
                 'Healthcare Provider Taxonomy Code_8',
           'Healthcare Provider Primary Taxonomy Switch_8',
                 'Healthcare Provider Taxonomy Code_9',
           'Healthcare Provider Primary Taxonomy Switch_9',
                 'Healthcare Provider Taxonomy Code_10',
           'Healthcare Provider Primary Taxonomy Switch_10',
                 'Healthcare Provider Taxonomy Code_11',
           'Healthcare Provider Primary Taxonomy Switch_11',
                 'Healthcare Provider Taxonomy Code_12',
           'Healthcare Provider Primary Taxonomy Switch_12',
                 'Healthcare Provider Taxonomy Code_13',
           'Healthcare Provider Primary Taxonomy Switch_13',
                 'Healthcare Provider Taxonomy Code_14',
           'Healthcare Provider Primary Taxonomy Switch_14',
                 'Healthcare Provider Taxonomy Code_15',
           'Healthcare Provider Primary Taxonomy Switch_15'
            ]

columns_kept = ['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']
#          'taxonomy_code']
200/39:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk[columns_kept], right = yes_code, how = "left").drop_duplicates()
npi_data.columns = [x.lower().replace(' ', '_') for x in npi_data.columns]
200/40:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
npi_data.columns = [x.lower().replace(' ', '_') for x in npi_data.columns]
200/41:
npi_data.shape
npi_data.head(5)
200/42:
yes_code = pd.concat(x_select)
npi_data2 = pd.merge(left = chunk[columns_kept], right = yes_code, how = "left").drop_duplicates()
npi_data2.columns = [x.lower().replace(' ', '_') for x in npi_data.columns] 
npi_data2
200/43:
npi_data.shape
#npi_data.head(5)
200/44:
yes_code = pd.concat(x_select)
npi_data2 = pd.merge(left = chunk[columns_kept], right = yes_code, how = "left").drop_duplicates()
npi_data2.columns = [x.lower().replace(' ', '_') for x in npi_data.columns] 
npi_data2
npi_data2.shape
200/45:
for chunk in tqdm(pd.read_csv('data/npidata_pfile_20050523-20230212.csv', usecols = columns_to_use,
           chunksize = 10000)):
    x_select = []
    for i in range (1,16):
        x_select.append(chunk[chunk[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] == 'Y']
                    [[columns_kept, 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
199/54:
columns_to_use = ['NPI',
 'Entity Type Code',
 'Provider Organization Name (Legal Business Name)',
 'Provider Last Name (Legal Name)',
 'Provider First Name',
 'Provider Middle Name',
 'Provider Name Prefix Text',
 'Provider Name Suffix Text',
 'Provider Credential Text',
 'Provider First Line Business Practice Location Address',
 'Provider Second Line Business Practice Location Address',
 'Provider Business Practice Location Address City Name',
 'Provider Business Practice Location Address State Name',
 'Provider Business Practice Location Address Postal Code',
           'Healthcare Provider Taxonomy Code_1',
           'Healthcare Provider Primary Taxonomy Switch_1',
           'Healthcare Provider Taxonomy Code_2',
           'Healthcare Provider Primary Taxonomy Switch_2', 
           'Healthcare Provider Taxonomy Code_3',
           'Healthcare Provider Primary Taxonomy Switch_3',     
           'Healthcare Provider Taxonomy Code_4',
           'Healthcare Provider Primary Taxonomy Switch_4',
           'Healthcare Provider Taxonomy Code_5',
           'Healthcare Provider Primary Taxonomy Switch_5',
            'Healthcare Provider Taxonomy Code_6',
           'Healthcare Provider Primary Taxonomy Switch_6',
        'Healthcare Provider Taxonomy Code_7',
           'Healthcare Provider Primary Taxonomy Switch_7',
                 'Healthcare Provider Taxonomy Code_8',
           'Healthcare Provider Primary Taxonomy Switch_8',
                 'Healthcare Provider Taxonomy Code_9',
           'Healthcare Provider Primary Taxonomy Switch_9',
                 'Healthcare Provider Taxonomy Code_10',
           'Healthcare Provider Primary Taxonomy Switch_10',
                 'Healthcare Provider Taxonomy Code_11',
           'Healthcare Provider Primary Taxonomy Switch_11',
                 'Healthcare Provider Taxonomy Code_12',
           'Healthcare Provider Primary Taxonomy Switch_12',
                 'Healthcare Provider Taxonomy Code_13',
           'Healthcare Provider Primary Taxonomy Switch_13',
                 'Healthcare Provider Taxonomy Code_14',
           'Healthcare Provider Primary Taxonomy Switch_14',
                 'Healthcare Provider Taxonomy Code_15',
           'Healthcare Provider Primary Taxonomy Switch_15'
            ]

columns_kept = ['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']
#          'taxonomy_code']

for chunk in tqdm(pd.read_csv('data/npidata_pfile_20050523-20230212.csv', usecols = columns_to_use,
           chunksize = 10000)):
    x_select = []
    for i in range (1,16):
        x_select.append(chunk[chunk[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] == 'Y']
                    [['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
yes_code = pd.concat(x_select)
npi_data2 = pd.merge(left = chunk[columns_kept], right = yes_code, how = "left").drop_duplicates()
npi_data2.columns = [x.lower().replace(' ', '_') for x in npi_data.columns]
199/55: npi_data2.shape
199/56: npi_data2.head(5)
199/57:
columns_to_use = ['NPI',
 'Entity Type Code',
 'Provider Organization Name (Legal Business Name)',
 'Provider Last Name (Legal Name)',
 'Provider First Name',
 'Provider Middle Name',
 'Provider Name Prefix Text',
 'Provider Name Suffix Text',
 'Provider Credential Text',
 'Provider First Line Business Practice Location Address',
 'Provider Second Line Business Practice Location Address',
 'Provider Business Practice Location Address City Name',
 'Provider Business Practice Location Address State Name',
 'Provider Business Practice Location Address Postal Code',
           'Healthcare Provider Taxonomy Code_1',
           'Healthcare Provider Primary Taxonomy Switch_1',
           'Healthcare Provider Taxonomy Code_2',
           'Healthcare Provider Primary Taxonomy Switch_2', 
           'Healthcare Provider Taxonomy Code_3',
           'Healthcare Provider Primary Taxonomy Switch_3',     
           'Healthcare Provider Taxonomy Code_4',
           'Healthcare Provider Primary Taxonomy Switch_4',
           'Healthcare Provider Taxonomy Code_5',
           'Healthcare Provider Primary Taxonomy Switch_5',
            'Healthcare Provider Taxonomy Code_6',
           'Healthcare Provider Primary Taxonomy Switch_6',
        'Healthcare Provider Taxonomy Code_7',
           'Healthcare Provider Primary Taxonomy Switch_7',
                 'Healthcare Provider Taxonomy Code_8',
           'Healthcare Provider Primary Taxonomy Switch_8',
                 'Healthcare Provider Taxonomy Code_9',
           'Healthcare Provider Primary Taxonomy Switch_9',
                 'Healthcare Provider Taxonomy Code_10',
           'Healthcare Provider Primary Taxonomy Switch_10',
                 'Healthcare Provider Taxonomy Code_11',
           'Healthcare Provider Primary Taxonomy Switch_11',
                 'Healthcare Provider Taxonomy Code_12',
           'Healthcare Provider Primary Taxonomy Switch_12',
                 'Healthcare Provider Taxonomy Code_13',
           'Healthcare Provider Primary Taxonomy Switch_13',
                 'Healthcare Provider Taxonomy Code_14',
           'Healthcare Provider Primary Taxonomy Switch_14',
                 'Healthcare Provider Taxonomy Code_15',
           'Healthcare Provider Primary Taxonomy Switch_15'
            ]

columns_kept = ['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']
#          'taxonomy_code']
db = sqlite3.connect('data/hop_referal.sqlite')
for chunk in tqdm(pd.read_csv('data/npidata_pfile_20050523-20230212.csv', usecols = columns_to_use,
           chunksize = 10000)):
    x_select = []
    for i in range (1,16):
        x_select.append(chunk[chunk[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] == 'Y']
                    [['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
    yes_code = pd.concat(x_select)
    npi_data = pd.merge(left = chunk[columns_kept], right = yes_code, how = "left").drop_duplicates()
    npi_data.columns = [x.lower().replace(' ', '_') for x in npi_data.columns] 
    npi_data.to_sql('npicode', db, if_exists = 'append', index = False)
200/46:
yes_code = pd.concat(x_select)
npi_data2 = pd.merge(left = chunk[columns_kept], right = yes_code, how = "left").drop_duplicates()
npi_data2.columns = [x.lower().replace(' ', '_') for x in npi_data.columns] 
npi_data2
npi_data2.shape
npi_data2.head(5)
200/47:
yes_code = pd.concat(x_select)
npi_data2 = pd.merge(left = chunk[columns_kept], right = yes_code, how = "left").drop_duplicates()
npi_data2.columns = [x.lower().replace(' ', '_') for x in npi_data2.columns] 
npi_data2
npi_data2.shape
npi_data2.head(5)
200/48:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
npi_data.columns = [x.lower().replace(' ', '_') for x in npi_data.columns]
200/49:
yes_code = pd.concat(x_select)
npi_data2 = pd.merge(left = chunk[columns_kept], right = yes_code, how = "left").drop_duplicates()
npi_data2.columns = [x.lower().replace(' ', '_') for x in npi_data2.columns] 
npi_data2
npi_data2.shape
npi_data2.head(5)
200/50:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
npi_data.columns = [x.lower().replace(' ', '_') for x in npi_data.columns]
200/51:
for chunk in tqdm(pd.read_csv('data/npidata_pfile_20050523-20230212.csv', usecols = columns_to_use,
           chunksize = 10000)):
    x_select = []
    for i in range (1,16):
        x_select.append(chunk[chunk[f'Healthcare Provider Primary Taxonomy Switch_{i}' ] == 'Y']
                    [['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code', 
        f'Healthcare Provider Taxonomy Code_{i}']].rename(columns={f'Healthcare Provider Taxonomy Code_{i}':'Taxonomy_code'}))
199/58:
query = "SELECT * FROM npicode LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
199/59:
query = "SELECT * FROM npicode LIMIT 50"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
200/52:
yes_code = pd.concat(x_select)
npi_data = pd.merge(left = chunk[['NPI',
         'Entity Type Code',
         'Provider Organization Name (Legal Business Name)',
         'Provider Last Name (Legal Name)',
         'Provider First Name',
         'Provider Middle Name',
         'Provider Name Prefix Text',
         'Provider Name Suffix Text',
         'Provider Credential Text',
         'Provider First Line Business Practice Location Address',
         'Provider Second Line Business Practice Location Address',
         'Provider Business Practice Location Address City Name',
         'Provider Business Practice Location Address State Name',
         'Provider Business Practice Location Address Postal Code']], right = yes_code, how = "left").drop_duplicates()
npi_data.columns = [x.lower().replace(' ', '_') for x in npi_data.columns]
200/53:
npi_data.shape
npi_data.head(5)
199/60:
query = 
"""
SELECT * 
FROM npicode 
WHERE taxonomy_code = '225100000X'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
199/61:
query = 
"""
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
199/62:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
199/63:
#checking the tables in the sqlite_db

db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
199/64: close.db()
199/65: db.close()
199/66:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND provider_organization_name_(legal_business_name) LIKE '%VANDERBILT'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
199/67:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%VANDERBILT%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
201/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
201/2:
#checking the tables in the sqlite_db

db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
201/3:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
201/4:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%VANDERBILT%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
201/5:
#checking the cbsa table 
query = "SELECT * FROM hop LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    hop_sqlite = pd.read_sql(query, db)
201/6:
#checking the cbsa table 
query = "SELECT * FROM hop LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    hop_sqlite = pd.read_sql(query, db)
hop_sqlite
201/7:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%TRISTAR%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
201/8:
query = """
SELECT * 
FROM npicode 
WHERE `provider_organization_name_(legal_business_name)` LIKE '%TRISTAR%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
201/9:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%TRISTAR%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
201/10:
#REFERALS from vandy to tristar

query = """

SELECT *
FROM npicode
INNER JOIN hop
ON npicode.npi = hop.from_npi
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%VANDERBILT%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    vandy_tristar = pd.read_sql(query, db)
vandy_tristar
203/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
203/2:
#checking the tables in the sqlite_db

db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
203/3:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
203/4:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%VANDERBILT%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
203/5:
#REFERALS from vandy to tristar

query = """
SELECT *
FROM npicode
INNER JOIN hop
ON npicode.npi = hop.from_npi
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%VANDERBILT%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    vandy_tristar = pd.read_sql(query, db)
vandy_tristar
204/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
204/2:
#checking the tables in the sqlite_db

db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
204/3:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%VANDERBILT%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
     vandy= pd.read_sql(query, db)
vandy
204/4:
#checking the cbsa table 
query = "SELECT * FROM hop"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    hop_sqlite = pd.read_sql(query, db)
hop_sqlite
205/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
205/2:
#checking the cbsa table 
query = "SELECT * FROM hop LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    hop_sqlite = pd.read_sql(query, db)
hop_sqlite
205/3:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%VANDERBILT%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
     vandy= pd.read_sql(query, db)
vandy
205/4:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
205/5:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
 """

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tn_npi = pd.read_sql(query, db)
tn_npi
205/6:
query = """
WITH tn_npi AS (
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN')
SELECT *
FROM tn_npi
INNER JOIN hop
ON tn_npi.npi = hop.from_npi

 """

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tn_referal = pd.read_sql(query, db)
tn_referal
206/1:
create the index using the npi
db = sqlite3.connect('data/hop_referal.sqlite')
db.execute('CREATE INDEX npi ON npicode(npi)')
db.execute('CREATE INDEX from_npi ON hops(from_npi)')
db.execute('CREATE INDEX to_npi ON hops(to_npi)')
206/2:
# create the index using the npi
db = sqlite3.connect('data/hop_referal.sqlite')
db.execute('CREATE INDEX npi ON npicode(npi)')
db.execute('CREATE INDEX from_npi ON hops(from_npi)')
db.execute('CREATE INDEX to_npi ON hops(to_npi)')
206/3:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
206/4:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
206/5:
#checking the tables in the sqlite_db

db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
206/6:
# create the index using the npi
db = sqlite3.connect('data/hop_referal.sqlite')
db.execute('CREATE INDEX npi ON npicode(npi)')
db.execute('CREATE INDEX from_npi ON hops(from_npi)')
db.execute('CREATE INDEX to_npi ON hops(to_npi)')
206/7:
# create the index using the npi
db = sqlite3.connect('data/hop_referal.sqlite')
db.execute('CREATE INDEX npi ON npicode(npi)')
db.execute('CREATE INDEX from_npi ON hop(from_npi)')
db.execute('CREATE INDEX to_npi ON hop(to_npi)')
206/8:
# create the index using the npi
db = sqlite3.connect('data/hop_referal.sqlite')
# db.execute('CREATE INDEX npi ON npicode(npi)')
db.execute('CREATE INDEX from_npi ON hop(from_npi)')
db.execute('CREATE INDEX to_npi ON hop(to_npi)')
206/9:
query = """
WITH tn_npi AS (
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN')
SELECT *
FROM tn_npi
INNER JOIN hop
ON tn_npi.npi = hop.from_npi

 """
##employ the group_by
with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tn_referal = pd.read_sql(query, db)
tn_referal
206/10:
query = """
WITH tn_npi AS (
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN')
SELECT to_npi,
SUM (transaction_count) AS count
FROM tn_npi
INNER JOIN hop
ON tn_npi.npi = hop.from_npi
GROUP BY to_npi
ORDER BY count DESC

 """
##employ the group_by

# SUM transaction_count AS count
# GROUP BY to_npi
# ORDER BY count DESC
with sqlite3.connect('data/hop_referal.sqlite') as db: 
    top_refered_to = pd.read_sql(query, db)
top_refered_to
207/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
207/2:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%VANDERBILT%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
     vandy= pd.read_sql(query, db)
vandy
207/3:
query = """
WITH tn_npi AS (
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN')
SELECT to_npi,
SUM (transaction_count) AS npi_total
FROM tn_npi
INNER JOIN hop
ON tn_npi.npi = hop.from_npi
GROUP BY to_npi
ORDER BY npi_total DESC

 """
##employ the group_by

# SUM transaction_count AS count
# GROUP BY to_npi
# ORDER BY count DESC
with sqlite3.connect('data/hop_referal.sqlite') as db: 
    top_refered_to = pd.read_sql(query, db)
top_refered_to
208/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
208/2:
#checking the tables in the sqlite_db

db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
208/3:
# create the index using the npi
db = sqlite3.connect('data/hop_referal.sqlite')
db.execute('CREATE INDEX npi ON npicode(npi)')
db.execute('CREATE INDEX from_npi ON hop(from_npi)')
db.execute('CREATE INDEX to_npi ON hop(to_npi)')
208/4:
# create the index using the npi
db = sqlite3.connect('data/hop_referal.sqlite')
# db.execute('CREATE INDEX npi ON npicode(npi)')
db.execute('CREATE INDEX from_npi ON hop(from_npi)')
db.execute('CREATE INDEX to_npi ON hop(to_npi)')
208/5: db.close()
208/6:
query = """
WITH tn_npi AS (
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'),

npi_hop AS (SELECT *
FROM tn_npi
INNER JOIN hop
ON tn_npi.npi = hop.from_npi)

SELECT to_npi,
SUM (transaction_count) AS npi_total
FROM npi_hop
GROUP BY to_npi
ORDER BY npi_total DESC

 """
##employ the group_by

# SUM transaction_count AS count
# GROUP BY to_npi
# ORDER BY count DESC
with sqlite3.connect('data/hop_referal.sqlite') as db: 
    top_refered_to = pd.read_sql(query, db)
top_refered_to
208/7: top_refered_to.to_csv('data/topreferredintn.csv')
208/8:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
208/9:
#checking the tables in the sqlite_db

db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
208/10:
# create the index using the npi
# db = sqlite3.connect('data/hop_referal.sqlite')
# db.execute('CREATE INDEX npi ON npicode(npi)')
# db.execute('CREATE INDEX from_npi ON hop(from_npi)')
# db.execute('CREATE INDEX to_npi ON hop(to_npi)')
208/11:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%TRISTAR%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
208/12:
#checking the cbsa table 
query = "SELECT * FROM taxonomy LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_sqlite = pd.read_sql(query, db)
tax_sqlite
208/13:
#checking the cbsa table 
query = "SELECT count(*) FROM taxonomy"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_sqlite = pd.read_sql(query, db)
tax_sqlite
208/14:
query = """
SELECT *
FROM taxonomy

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_tristar = pd.read_sql(query, db)
tax_tristar
208/15:
query = """
SELECT code, grouping, classification, specialization
FROM taxonomy

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_tristar = pd.read_sql(query, db)
tax_tristar
208/16:
query = """
SELECT code, grouping, classification, specialization
FROM taxonomy as tax
INNER JOIN npicode as nc
USING nc.taxonomy_code = tax.code
INNER JOIN hop as hop
USING nc.npi = hop.to_npi
GROUP BY tax.code


"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_npi_hop = pd.read_sql(query, db)
tax_npi_hop
208/17:
query = """
SELECT code, grouping, classification, specialization
FROM taxonomy as tax
INNER JOIN npicode as nc
    USING nc.taxonomy_code = tax.code


"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_npi_hop = pd.read_sql(query, db)
tax_npi_hop
208/18:
query = """
SELECT code, grouping, classification, specialization
FROM taxonomy AS tax
INNER JOIN npicode AS nc
    USING nc.taxonomy_code = tax.code
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_npi_hop = pd.read_sql(query, db)
tax_npi_hop
208/19:
query = """
SELECT code, grouping, classification, specialization
FROM taxonomy AS tax
INNER JOIN npicode AS nc
    USING tax.code = nc.taxonomy_code
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_npi_hop = pd.read_sql(query, db)
tax_npi_hop
208/20:
query = """
SELECT code, grouping, classification, specialization
FROM taxonomy AS tax

    
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_npi_hop = pd.read_sql(query, db)
tax_npi_hop
208/21:
query = """
SELECT code, grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_code AS nc
    USING nc.taxonomy_code = tax.code

    
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_npi_hop = pd.read_sql(query, db)
tax_npi_hop
208/22:
query = """
SELECT code, grouping, classification, specialization
FROM taxonomy AS tax
    
    
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_npi_hop = pd.read_sql(query, db)
tax_npi_hop
208/23:
query = """
SELECT code, grouping, classification, specialization, nc.npi
FROM taxonomy AS tax
INNER JOIN npicode AS nc
ON nc.taxonomy_code = tax.code
    
    
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_npi_hop = pd.read_sql(query, db)
tax_npi_hop
208/24:
#checking the cbsa table 
query = "SELECT * FROM cbsa LIMIT 10"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    hop_sqlite = pd.read_sql(query, db)
hop_sqlite
208/25:
#checking the cbsa table 
query = "SELECT * FROM cbsa WHERE usps_zip_pref_state = 'TN' "

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
cbsa_sqlite
208/26:
#checking the cbsa table 
query = "SELECT DISTICT usps_zip_pref_city FROM cbsa WHERE usps_zip_pref_state = 'TN' "

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
cbsa_sqlite
208/27:
#checking the cbsa table 
query = "SELECT DISTINCT usps_zip_pref_city FROM cbsa WHERE usps_zip_pref_state = 'TN' "

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
cbsa_sqlite
208/28:
#checking the cbsa table 
query = "SELECT DISTINCT usps_zip_pref_city AS city FROM cbsa WHERE usps_zip_pref_state = 'TN' ORDER BY city ASC "

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
cbsa_sqlite
208/29:
#checking the cbsa table 
query = "SELECT DISTINCT usps_zip_pref_city AS city FROM cbsa WHERE usps_zip_pref_state = 'TN' ORDER BY city ASC LIMIT 20 "

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
cbsa_sqlite
208/30:
#checking the cbsa table 
query = "SELECT DISTINCT usps_zip_pref_city AS city FROM cbsa WHERE usps_zip_pref_state = 'TN' ORDER BY city ASC LIMIT 50 "

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
cbsa_sqlite
208/31:
#checking the cbsa table 
# query = "SELECT DISTINCT usps_zip_pref_city AS city FROM cbsa WHERE usps_zip_pref_state = 'TN' ORDER BY city ASC LIMIT 50 "
query = "SELECT * FROM cbsa"
with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
cbsa_sqlite
208/32:
#checking the cbsa table 
# query = "SELECT DISTINCT usps_zip_pref_city AS city FROM cbsa WHERE usps_zip_pref_state = 'TN' ORDER BY city ASC LIMIT 50 "
query = "SELECT * FROM cbsa WHERE cbsa = 34980"
with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
cbsa_sqlite
208/33:
#checking the cbsa table 
# query = "SELECT DISTINCT usps_zip_pref_city AS city FROM cbsa WHERE usps_zip_pref_state = 'TN' ORDER BY city ASC LIMIT 50 "
query = "SELECT * FROM cbsa WHERE cbsa = 34980 ORDER BY zip"
with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_sqlite = pd.read_sql(query, db)
cbsa_sqlite
208/34:
query = """
WITH cbsa_nasnear AS (
SELECT zip 
FROM cbsa
WHERE cbsa = 34980

),

npi_hop AS (SELECT *
FROM tn_npi
INNER JOIN hop
ON tn_npi.npi = hop.from_npi)

SELECT to_npi,
SUM (transaction_count) AS npi_total
FROM npi_hop
GROUP BY to_npi
ORDER BY npi_total DESC

 """
##employ the group_by

# SUM transaction_count AS count
# GROUP BY to_npi
# ORDER BY count DESC
with sqlite3.connect('data/hop_referal.sqlite') as db: 
    top_refered_to = pd.read_sql(query, db)
top_refered_to
208/35:
SELECT zip 
FROM cbsa
WHERE cbsa = 34980
208/36:
query= """
SELECT zip 
FROM cbsa
WHERE cbsa = 34980
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz
208/37:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT *
    SUBSTRING(provider_business_practice_location_address_postal_code, 1, 5)
FROM npicode

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz
208/38:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT *
    SUBSTRING(nc.provider_business_practice_location_address_postal_code, 1, 5)
FROM npicode as 

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz
208/39:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT *
    SUBSTRING(nc.provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode as 

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz
208/40:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT *
    SUBSTRING(nc.provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz
208/41:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT *
    SUBSTR(nc.provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz
208/42:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT *
    SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz
208/43:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT npi
    SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz
208/44:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT npi,
    SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz
208/45:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
provider_organization_name_(legal_business_name) AS facility_name,
provider_last_name_(legal_name) AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode

FROM npicode AS nc

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


npi,
entity_type_code AS entity_code,
provider_organization_name_(legal_business_name) AS facility_name,
provider_last_name_(legal_name) AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
provider_business_practice_location_address_postal_code AS zipcode,
taxonomy_code
208/46:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
provider_organization_name_(legal_business_name) AS facility_name,
provider_last_name_(legal_name) AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode

FROM npicode AS nc

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
208/47:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
provider_organization_name_(legal_business_name) AS facility_name,
provider_last_name_(legal_name) AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
208/48:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
208/49:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc)
LEFT JOIN cbsa AS cbsa
ON cbsa.zipcode = cbsa.zip
WHERE cbsa.cbsa = 34980
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
208/50:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc)
INNER JOIN cbsa AS cbsa
ON cbsa.zipcode = cbsa.zip
WHERE cbsa.cbsa = 34980
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
208/51:
query = """
WITH cbsa_nasnear AS (
SELECT zip 
FROM cbsa
WHERE cbsa = 34980
),
npi_hop AS (SELECT *
FROM tn_npi
INNER JOIN hop
ON tn_npi.npi = hop.from_npi)

SELECT to_npi,
SUM (transaction_count) AS npi_total
FROM npi_hop
GROUP BY to_npi
ORDER BY npi_total DESC

 """
##employ the group_by

# SUM transaction_count AS count
# GROUP BY to_npi
# ORDER BY count DESC
with sqlite3.connect('data/hop_referal.sqlite') as db: 
    top_refered_to = pd.read_sql(query, db)
top_refered_to
208/52:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc)
INNER JOIN cbsa AS cbsa
ON cbsa.zipcode = cbsa.zip
WHERE cbsa.cbsa = 34980
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
208/53:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode AS nc)
INNER JOIN cbsa AS cbsa
ON cbsa_npi.zipcode = cbsa.zip
WHERE cbsa.cbsa = 34980
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
208/54:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode)
INNER JOIN cbsa AS cbsa
ON cbsa_npi.zipcode = cbsa.zip
WHERE cbsa.cbsa = 34980
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
208/55:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode)
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
208/56:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),
npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980)
SELECT cbsa_npi_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added
    ON tax.code = npi_cbsa_added.taxonomy_code
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
208/57:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980)

SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added
    ON tax.code = npi_cbsa_added.taxonomy_code
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
209/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
209/2:
#checking the tables in the sqlite_db

db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
209/3:
# create the index using the npi
# db = sqlite3.connect('data/hop_referal.sqlite')
# db.execute('CREATE INDEX npi ON npicode(npi)')
db.execute('CREATE INDEX taxonomy_code ON npicode(taxonomy_code)')
# db.execute('CREATE INDEX from_npi ON hop(from_npi)')
# db.execute('CREATE INDEX to_npi ON hop(to_npi)')
db.execute('CREATE INDEX code ON taxonomy(code)')
209/4: db.close()
209/5:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
209/6:
# query= """
# SELECT zip 
# FROM cbsa
# WHERE cbsa = 34980
# """

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980)

SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added
    ON tax.code = npi_cbsa_added.taxonomy_code
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz


# npi,
# entity_type_code AS entity_code,
# provider_organization_name_(legal_business_name) AS facility_name,
# provider_last_name_(legal_name) AS doc_lastname,
# provider_first_name AS doc_firstname,
# provider_middle_name AS doc_middlename,
# provider_first_line_business_practice_location_address AS primary_address,
# provider_second_line_business_practice_location_address AS secondary_address,
# provider_business_practice_location_address_city_name AS city,
# provider_business_practice_location_address_state_name AS state,
# provider_business_practice_location_address_postal_code AS zipcode,
# taxonomy_code
209/7: xyz.to_cv('data/npi_cbsa_taxdiscription.csv', index=False)
209/8: xyz.to_csv('data/npi_cbsa_taxdiscription.csv', index=False)
209/9:
query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980)

SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    RIGHT JOIN npi_cbsa_added --use the right join instead
    ON tax.code = npi_cbsa_added.taxonomy_code
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    xyz = pd.read_sql(query, db)
xyz
209/10:
query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980)

SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    RIGHT JOIN npi_cbsa_added --use the right join instead
    ON tax.code = npi_cbsa_added.taxonomy_code
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    nas_cbsa_tax = pd.read_sql(query, db)
xyz
209/11:
query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980)

SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    RIGHT JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    nas_cbsa_tax = pd.read_sql(query, db)
xyz
209/12:
query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980)

SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    LEFT JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    nas_cbsa_tax = pd.read_sql(query, db)
xyz
209/13:
query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980)

SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    LEFT JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    nas_cbsa_tax = pd.read_sql(query, db)
nas_cbsa_tax
209/14:
query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    LEFT JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT npi_tax.*, hop.*
FROM npi_tax
    LEFT JOIN hop as hop
    ON npi_tax.npi = hop.to_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    nas_cbsa_tax_hop = pd.read_sql(query, db)
nas_cbsa_tax_hop
209/15:
query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT npi_tax.*, hop.*
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.to_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    nas_cbsa_tax_hop_tonpi = pd.read_sql(query, db)
nas_cbsa_tax_hop_tonpi
209/16: nas_cbsa_tax_hop_tonpi.to_csv('data/nas_cbsa_tax_hop_tonpi.csv', index=False)
209/17:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
209/18:
#using the left join

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
`provider_last_name_(legal_name)` AS doc_lastname,
provider_first_name AS doc_firstname,
provider_middle_name AS doc_middlename,
provider_first_line_business_practice_location_address AS primary_address,
provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM npi_cbsa_added
    LEFT JOIN taxonomy AS tax
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT npi_tax.*, hop.*
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.to_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    nas_cbsa_tax_hop_tonpi_left = pd.read_sql(query, db)
nas_cbsa_tax_hop_tonpi_left
209/19: nas_cbsa_tax_hop_tonpi_left.to_csv('data/nas_cbsa_tonpi_leftjoin.csv', index=False)
209/20:
query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
WHERE entity_type_code = 2),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT npi_tax.*, hop.*
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.to_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tonpi_hos_nascbsa = pd.read_sql(query, db)
tonpi_hos_nascbsa
209/21: tonpi_hos_nascbsa.to_csv('data/refered_to_nashvillehospitals.csv', index=False)
209/22:
#looking for from_npi tax_code in nashville hospitals

query= """
with cbsa_npi AS (
SELECT npi,
taxonomy_code,
entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
WHERE entity_type_code = 2),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT npi_tax.*, hop.*
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.from_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   fromnpi_hos_nascbsa = pd.read_sql(query, db)
fromnpi_hos_nascbsa
209/23:
query= """
with cbsa_npi AS (
SELECT 
--npi,
taxonomy_code,
--entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
--provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
WHERE entity_type_code = 2),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT npi_tax.*, hop.*
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.from_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   fromnpi_hos_nascbsa = pd.read_sql(query, db)
fromnpi_hos_nascbsa
209/24:
query= """
with cbsa_npi AS (
SELECT 
--npi,
taxonomy_code,
--entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
--provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
WHERE entity_type_code = 2),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT npi_tax.*, hop.*
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.from_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   from_npi = pd.read_sql(query, db)
from_npi
209/25:
query= """
with cbsa_npi AS (
SELECT 
npi,
taxonomy_code,
--entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
--provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
WHERE entity_type_code = 2),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.*, grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT npi_tax.*, hop.*
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.from_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   from_npi = pd.read_sql(query, db)
from_npi
209/26:
query= """
with cbsa_npi AS (
SELECT 
npi,
taxonomy_code,
--entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
--provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
--WHERE entity_type_code = 2
),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.npi, npi_cbsa_added.taxonomy_code, npi_cbsa_added.facility_name,
    grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT hop.to_npi AS ref_to_npi, 
    npi_tax.facility_name AS ref_to_facility,
    npi_tax.gouping AS ref_to_group,
    npi_tax.classfication AS ref_to_classification,
    npi_tax.specialization AS ref_to_speciality, 
    hop.from_npi AS refered_from_npi, 
    hop.patient_count, 
    hop.transaction_count
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.to_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   ref_to = pd.read_sql(query, db)
ref_to
209/27:
query= """
with cbsa_npi AS (
SELECT 
npi,
taxonomy_code,
--entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
--provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
--WHERE entity_type_code = 2
),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.npi, npi_cbsa_added.taxonomy_code, npi_cbsa_added.facility_name,
    grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT hop.to_npi AS ref_to_npi, 
    npi_tax.facility_name AS ref_to_facility,
    npi_tax.grouping AS ref_to_group,
    npi_tax.classfication AS ref_to_classification,
    npi_tax.specialization AS ref_to_speciality, 
    hop.from_npi AS refered_from_npi, 
    hop.patient_count, 
    hop.transaction_count
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.to_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   ref_to = pd.read_sql(query, db)
ref_to
209/28:
query= """
with cbsa_npi AS (
SELECT 
npi,
taxonomy_code,
--entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
--provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
--WHERE entity_type_code = 2
),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.npi, npi_cbsa_added.taxonomy_code, npi_cbsa_added.facility_name,
    grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code)

SELECT hop.to_npi AS ref_to_npi, 
    npi_tax.facility_name AS ref_to_facility,
    npi_tax.grouping AS ref_to_group,
    npi_tax.classification AS ref_to_classification,
    npi_tax.specialization AS ref_to_speciality, 
    hop.from_npi AS refered_from_npi, 
    hop.patient_count, 
    hop.transaction_count
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.to_npi
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   ref_to = pd.read_sql(query, db)
ref_to
209/29: ref_to.to_csv('data/ref_to.csv', index=False)
209/30:
query= """
with cbsa_npi AS (
SELECT 
npi,
taxonomy_code,
--entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
--provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
--WHERE entity_type_code = 2
),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.npi, npi_cbsa_added.taxonomy_code, npi_cbsa_added.facility_name,
    grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code),

tonpi AS(
SELECT hop.to_npi AS ref_to_npi, 
    npi_tax.facility_name AS ref_to_facility,
    npi_tax.grouping AS ref_to_group,
    npi_tax.classification AS ref_to_classification,
    npi_tax.specialization AS ref_to_speciality, 
    hop.from_npi AS refered_from_npi, 
    hop.patient_count, 
    hop.transaction_count
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.to_npi)

SELECT tonpi.*, npi_tax.*
FROM tonpi
INNER JOIN npi_tax
ON tonpi_refered_from_npi = npi_tax.npi
    
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   ref_to_fromtax = pd.read_sql(query, db)
ref_to_fromtax
209/31:
query= """
with cbsa_npi AS (
SELECT 
npi,
taxonomy_code,
--entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
--provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
--WHERE entity_type_code = 2
),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.npi, npi_cbsa_added.taxonomy_code, npi_cbsa_added.facility_name,
    grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code),

tonpi AS(
SELECT hop.to_npi AS ref_to_npi, 
    npi_tax.facility_name AS ref_to_facility,
    npi_tax.grouping AS ref_to_group,
    npi_tax.classification AS ref_to_classification,
    npi_tax.specialization AS ref_to_speciality, 
    hop.from_npi AS refered_from_npi, 
    hop.patient_count, 
    hop.transaction_count
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.to_npi)

SELECT tonpi.*, npi_tax.*
FROM tonpi
INNER JOIN npi_tax
ON tonpi.refered_from_npi = npi_tax.npi
    
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   ref_to_fromtax = pd.read_sql(query, db)
ref_to_fromtax
209/32:
query = "SELECT count(*) FROM taxonomy"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    tax_count = pd.read_sql(query, db)
tax_count
209/33:
query = "SELECT count(*) FROM hop"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    hop_count = pd.read_sql(query, db)
hop_count
209/34:
query = "SELECT count(*) FROM cbsa"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    cbsa_count = pd.read_sql(query, db)
cbsa_count
209/35:
query = "SELECT count(*) FROM npicode"

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_count = pd.read_sql(query, db)
npicode_count
209/36:
query= """
with cbsa_npi AS (
SELECT 
npi,
taxonomy_code,
--entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
--provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
--WHERE entity_type_code = 2
),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.npi, npi_cbsa_added.taxonomy_code, npi_cbsa_added.facility_name,
    grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code),

tonpi AS(
SELECT hop.to_npi AS ref_to_npi, 
    npi_tax.facility_name AS ref_to_facility,
    npi_tax.grouping AS ref_to_group,
    npi_tax.classification AS ref_to_classification,
    npi_tax.specialization AS ref_to_speciality, 
    hop.from_npi AS refered_from_npi, 
    hop.patient_count, 
    hop.transaction_count
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.to_npi)

SELECT tonpi.*, npi_tax.*
FROM npi_tax
LEFT JOIN tonpi
ON tonpi.refered_from_npi = npi_tax.npi
    
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   ref_to_fromtax_left = pd.read_sql(query, db)
ref_to_fromtax_left
209/37:
query= """
with cbsa_npi AS (
SELECT 
npi,
taxonomy_code,
--entity_type_code AS entity_code,
`provider_organization_name_(legal_business_name)` AS facility_name,
--`provider_last_name_(legal_name)` AS doc_lastname,
--provider_first_name AS doc_firstname,
--provider_middle_name AS doc_middlename,
--provider_first_line_business_practice_location_address AS primary_address,
--provider_second_line_business_practice_location_address AS secondary_address,
--provider_business_practice_location_address_city_name AS city,
--provider_business_practice_location_address_state_name AS state,
SUBSTR(provider_business_practice_location_address_postal_code, 1, 5) AS zipcode
FROM npicode
--WHERE entity_type_code = 2
),

npi_cbsa_added AS (
SELECT cbsa_npi.*
FROM cbsa_npi
    INNER JOIN cbsa 
    ON cbsa_npi.zipcode = zip
    WHERE cbsa.cbsa = 34980),

npi_tax AS (
SELECT npi_cbsa_added.npi, npi_cbsa_added.taxonomy_code, npi_cbsa_added.facility_name,
    grouping, classification, specialization
FROM taxonomy AS tax
    INNER JOIN npi_cbsa_added 
    ON tax.code = npi_cbsa_added.taxonomy_code),

tonpi AS(
SELECT hop.to_npi AS ref_to_npi, 
    npi_tax.facility_name AS ref_to_facility,
    npi_tax.grouping AS ref_to_group,
    npi_tax.classification AS ref_to_classification,
    npi_tax.specialization AS ref_to_speciality, 
    hop.from_npi AS refered_from_npi, 
    hop.patient_count, 
    hop.transaction_count
FROM npi_tax
    INNER JOIN hop as hop
    ON npi_tax.npi = hop.to_npi)

SELECT tonpi.*, npi_tax.*
FROM tonpi
LEFT JOIN npi_tax
ON tonpi.refered_from_npi = npi_tax.npi
    
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
   ref_to_fromtax_left = pd.read_sql(query, db)
ref_to_fromtax_left
209/38: ref_to_fromtax_left.to_csv('data/refto_from.csv')
209/39: refto_from = pd.read_csv('data/refto_from.csv')
209/40:
refto_from = pd.read_csv('data/refto_from.csv')
refto_from.shape
209/41: refto_from.columns
209/42: refto_from.head(5)
209/43: ref_to_fromtax_left.to_csv('data/refto_from.csv', index=False)
209/44:
refto_from = pd.read_csv('data/refto_from.csv')
refto_from.shape
209/45: refto_from.columns
209/46: refto_from.head(5)
209/47:
refto_from = refto_from.rename(columns={'taxonomy_code': 'ref_from_taxcode',
                           'facility_name': 'ref_from_facility', 
                           'grouping': 'ref_from_group',
                           'classification': 'ref_from_classification',
                           'specialization': 'ref_from_specialization'})
refto_from.columns
209/48:
#df.drop(columns=['B', 'C'])

refto_from.drop(columns=['npi'])
209/49: refto_from.groupby('ref_to_facility').value_counts().to_frame()
209/50: refto_from.groupby('ref_to_facility').value_counts()
209/51:
#df.drop(columns=['B', 'C'])

refto_from = refto_from.drop(columns=['npi'])
209/52:
refto_from = refto_from.rename(columns={'taxonomy_code': 'ref_from_taxcode',
                           'facility_name': 'ref_from_facility', 
                           'grouping': 'ref_from_group',
                           'classification': 'ref_from_classification',
                           'specialization': 'ref_from_speciality'})
refto_from.columns
209/53:
#df.drop(columns=['B', 'C'])

refto_from = refto_from.drop(columns=['npi'])
209/54:
#df.drop(columns=['B', 'C'])

refto_from = refto_from.drop(columns=['npi'])
refto_from.head(2)
209/55:
#df.drop(columns=['B', 'C'])

#refto_from = refto_from.drop(columns=['npi'])
refto_from.head(2)
209/56:
refto_from = refto_from.rename(columns={'taxonomy_code': 'ref_from_taxcode',
                           'facility_name': 'ref_from_facility', 
                           'grouping': 'ref_from_group',
                           'classification': 'ref_from_classification',
                           'specialization': 'ref_from_speciality'})
refto_from.columns
209/57: refto_from.columns
209/58:
refto_from = refto_from.rename(columns={'taxonomy_code': 'ref_from_taxcode',
                           'facility_name': 'ref_from_facility', 
                           'grouping': 'ref_from_group',
                           'classification': 'ref_from_classification',
                           'specialization': 'ref_from_speciality'})
refto_from.columns
209/59:
#df.drop(columns=['B', 'C'])

#refto_from = refto_from.drop(columns=['npi'])
refto_from = refto_from.rename(columns={'ref_from_specialization':'ref_from_speciality'}
refto_from.head(2)
209/60:
#df.drop(columns=['B', 'C'])

#refto_from = refto_from.drop(columns=['npi'])
refto_from = refto_from.rename(columns={'ref_from_specialization':'ref_from_speciality'}
209/61:
#df.drop(columns=['B', 'C'])

#refto_from = refto_from.drop(columns=['npi'])
refto_from = refto_from.rename(columns={'ref_from_specialization':'ref_from_speciality'})
refto_from.head(2)
209/62: refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/63: refto_from.groupby(['ref_to_speciality', 'ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/64: refto_from.groupby(['ref_to_speciality', 'ref_from_speciality'])['patient_count',  'transaction_count'].sum()
209/65: refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/66: refto_from.groupby(['ref_to_facility'])['patient_count'].sum()
209/67: refto_from.groupby(['ref_to_facility'])['patient_count'].sum().sort_value(by = 'patient_count', ascending = False)
209/68: refto_from.groupby(['ref_to_facility'])['patient_count'].sum()
209/69: refto_from.groupby(['ref_to_facility'])['patient_count'].sum().sort_values(by = 'patient_count', ascending = False)
209/70: refto_from.groupby(['ref_to_facility']).value_counts()
209/71: refto_from.groupby(['ref_to_facility']).value_counts().to_frame()
209/72:
refto_from.groupby(['ref_to_facility']).['patient_count',  'transaction_count'].sum().to_frame()
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/73:
refto_from.groupby(['ref_to_facility'])['patient_count',  'transaction_count'].sum().to_frame()
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/74:
refto_from.groupby(['ref_to_facility'])['patient_count',  'transaction_count'].sum()
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/75:
refto_from.groupby(['ref_to_facility'])['patient_count',  'transaction_count'].sum().max()
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/76:
refto_from.groupby(['ref_to_facility'])['patient_count',  'transaction_count'].sum().top(20)
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/77:
refto_from.groupby(['ref_to_facility'])['patient_count',  'transaction_count'].sum()
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/78:
refto_from.groupby(['ref_to_facility'])['patient_count',  'transaction_count'].sum().sort_values('patient_count')
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/79:
refto_from.groupby(['ref_to_facility'])['patient_count',  'transaction_count'].sum().sort_values('patient_count', ascending =False)
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/80: refto_from.groupby(['ref_from_facility'])['patient_count',  'transaction_count'].sum().sort_values('patient_count', ascending =False)
209/81:
refto_from.groupby(['ref_to_facility'])['patient_count',  'transaction_count'].sum().sort_values('patient_count', ascending =False).head(10)
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
209/82: refto_from.groupby(['ref_from_facility'])['patient_count',  'transaction_count'].sum().sort_values('patient_count', ascending =False).head(10)
209/83: refto_from.groupby(['ref_to_group'])['patient_count'].sum().sort_values('patient_count', ascending =False).head(10)
209/84: refto_from.groupby(['ref_to_group'])['patient_count',  'transaction_count'].sum().sort_values('patient_count', ascending =False).head(10)
209/85: refto_from.groupby(['ref_to_group'])['patient_count',  'transaction_count'].sum().sort_values('patient_count', ascending =False)
209/86:
refto_from.groupby('ref_to_group')['ref_to_facility'].nunique().plot(kind="bar",
                                                             title = 'Countries_Per_Continent', 
                                                             xlabel='',
                                                             ylabel='Number)
209/87:
refto_from.groupby('ref_to_group')['ref_to_facility'].nunique().plot(kind="bar",
                                                             title = 'Countries_Per_Continent', 
                                                             xlabel='',
                                                             ylabel='Number')
209/88:
refto_from.groupby('ref_to_group')['ref_to_facility'].nunique().plot(kind="bar",
                                                             title = 'ref_to_facility/group', 
                                                             xlabel='group',
                                                             ylabel='Number')
209/89: refto_from[refto_from ['ref_to_facility']== "Hospitals"]
209/90: refto_from[refto_from ['ref_to_facility']== "HOSPITALS"]
209/91: refto_from[refto_from ['ref_to_facility']== "Hospitals"]
209/92: refto_from[refto_from ['ref_to_group'] == "Hospitals"]
209/93:
refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby['ref_to_facility'].nunique().plot(kind="bar",
                                                             title = 'ref_to_facility/group', 
                                                             xlabel='group',
                                                             ylabel='Number')
209/94: refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby['ref_to_facility']
209/95: refto_from[refto_from ['ref_to_group'] == "Hospitals"]
209/96: refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby('ref_to_facility').nunique()
209/97: refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby('ref_to_facility').nunique().to_frame()
209/98: refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby('ref_to_facility').nunique()
209/99: refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby('ref_to_facility')['ref_from_facility'].nunique()
209/100: refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby('ref_to_facility')['ref_from_facility'].nunique().sort_value('ref_from_facility', ascending=False)
209/101: refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby('ref_to_facility')['ref_from_facility'].nunique()
209/102:
refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby('ref_to_facility')['ref_from_facility'].nunique().plot(kind="bar",
                                                             title = 'ref_from_facility/ref_to_facility', 
                                                             xlabel='group',
                                                             ylabel='Number')
209/103:
#REFERALS from vandy to tristar

query = """
SELECT *
FROM npicode

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npi_explo = pd.read_sql(query, db)
npi_explo
210/1:
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm
210/2:
#checking the tables in the sqlite_db

db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
210/3:
query = """
SELECT * 
FROM npicode 
LIMIT 2
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
     explo= pd.read_sql(query, db)
explo
210/4:
query = """
SELECT * 
FROM npicode 
WHERE npi = 1093753303

"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
     explo= pd.read_sql(query, db)
explo
210/5:
refto_from.groupby(['ref_to_facility'])['patient_count',  'transaction_count'].sum().sort_values('patient_count', ascending =False).head(10)
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
210/6:
refto_from = pd.read_csv('data/refto_from.csv')
refto_from.shape
210/7:
refto_from = pd.read_csv('data/refto_from.csv')
refto_from.shape
210/8: refto_from.head(5)
210/9: refto_from.columns
210/10: refto_from.columns
210/11:
refto_from = refto_from.rename(columns={'taxonomy_code': 'ref_from_taxcode',
                           'facility_name': 'ref_from_facility', 
                           'grouping': 'ref_from_group',
                           'classification': 'ref_from_classification',
                           'specialization': 'ref_from_specialization'})
refto_from.columns
210/12:
#df.drop(columns=['B', 'C'])

#refto_from = refto_from.drop(columns=['npi'])
refto_from = refto_from.rename(columns={'ref_from_specialization':'ref_from_speciality'})
refto_from.head(2)
210/13:
#df.drop(columns=['B', 'C'])

refto_from = refto_from.drop(columns=['npi'])
210/14:

refto_from = refto_from.rename(columns={'ref_from_specialization':'ref_from_speciality'})
refto_from.head(2)
210/15: refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
210/16: refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum().sort_values('patient_count', ascending=False)
210/17:
refto_from.groupby(['ref_to_facility'])['patient_count',  'transaction_count'].sum().sort_values('patient_count', ascending =False).head(10)
#refto_from.groupby(['ref_to_speciality'])['patient_count',  'transaction_count'].sum()
210/18: refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility', 'ref_from_facility'])['patient_count'].agg('sum').sort_values()
210/19: refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility', 'ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False)
210/20: refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility', 'ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
210/21:
refto_from[refto_from ['ref_to_group'] == "Hospitals"]
    .groupby(['ref_to_facility', 'ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
210/22: refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
210/23:
refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
refto_hosonly.head()
210/24:
refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
refto_hosonly.head(10)
210/25: refto_hosonly[refto_from['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER']
210/26:
refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
refto_hosonly
210/27: refto_hosonly[refto_from['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
210/28: refto_hosonly[refto_from['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sort_values(ascending = False).to_frame()
210/29: refto_hosonly[refto_from['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].sort_values(ascending = False).to_frame()
210/30: refto_hosonly[refto_hosonly['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].sort_values(ascending = False).to_frame()
210/31: refto_hosonly[refto_hosonly['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER']
210/32: refto_hosonly[refto_hosonly['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum()
210/33: refto_hosonly[refto_hosonly['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum()
210/34:
refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
refto_hosonly.reset_index()
210/35:
refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
refto_hosonly = refto_hosonly.reset_index()
210/36: refto_hosonly[refto_hosonly['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum()
210/37: refto_hosonly[refto_hosonly['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values('patient_count', ascending =False).to_frame()
210/38: refto_hosonly[refto_hosonly['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame()
210/39:
refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
refto_hosonly = refto_hosonly.reset_index()
refto_hosonly
210/40: refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame()
210/41:
to_vady = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame()

to_vandy = to_vandy.rest_index()
210/42:
to_vandy = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame()

to_vandy = to_vandy.rest_index()
210/43: refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame()
210/44:
tovandy = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().reset_index()
tovandy
210/45:
tovandy = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().reset_index()
tovandy
tovandy[['ref_from_facility']].tolist()
210/46:
tovandy = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().reset_index()
tovandy
tovandy[['ref_from_facility']].to_list()
210/47:
tovandy = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().reset_index()
tovandy
tovandy[['ref_from_facility']].to_list
210/48:
tovandy = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().reset_index()
tovandy
tovandy[['ref_from_facility']]
210/49:
tovandy = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().reset_index()
tovandy
tovandy['ref_from_facility']
210/50:
tovandy = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().reset_index()
tovandy
tovandy['ref_from_facility'].to_list()
210/51:
tovandy = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().reset_index()
tovandy
vandlist = tovandy['ref_from_facility'].to_list()
210/52:
#df[~df.country.isin(countries_to_keep)]

refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)]
210/53:
#df[~df.country.isin(countries_to_keep)]

refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].to_frame().reset_index()
210/54:
#df[~df.country.isin(countries_to_keep)]

refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
210/55:
#df[~df.country.isin(countries_to_keep)]

not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index().
210/56:
#df[~df.country.isin(countries_to_keep)]

not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
210/57:
#df[~df.country.isin(countries_to_keep)]

not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
not_ref_tovady.head(10)
210/58:
#df[~df.country.isin(countries_to_keep)]

not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
not_ref_tovandy.head(10)
210/59:
#df[~df.country.isin(countries_to_keep)]

not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
not_ref_tovandy.groupby('ref_from_facility')['patient-count'].agg(sum)
210/60:
#df[~df.country.isin(countries_to_keep)]

not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
not_ref_tovandy.groupby('ref_from_facility')['patient_count'].agg(sum)
210/61:
#df[~df.country.isin(countries_to_keep)]

not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
not_ref_tovandy.groupby('ref_from_facility')['patient_count'].agg(sum).sort_values(ascending = False)
210/62:
#df[~df.country.isin(countries_to_keep)]

not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
not_ref_tovandy.groupby('ref_from_facility')['patient_count'].agg(sum).sort_values(ascending = False).to_frame()
210/63:
#df[~df.country.isin(countries_to_keep)]

not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
not_ref_tovandy.groupby('ref_from_facility')['patient_count'].agg(sum).sort_values(ascending = False).to_frame().head(20)
210/64:
tovandy = refto_hosonly[refto_hosonly['ref_to_facility'] == 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().reset_index()
tovandy
vandlist = tovandy['ref_from_facility'].to_list()
tovandy
210/65:
#df[~df.country.isin(countries_to_keep)]
# using ~ eliminates the content in the list from the search
not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
not_ref_tovandy.groupby('ref_from_facility')['patient_count'].agg(sum).sort_values(ascending = False).to_frame()
210/66:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
210/67:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape
210/68:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()
210/69:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape
210/70:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape ##(181436, 14)

refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()
210/71:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape ##(181436, 14)

refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()

refto_from[~refto_from.ref_from_facility. isin(vandlist)].groupby('ref_to_facility')['ref_from_facility'].nunique().plot(kind="bar",
                                                             title = 'ref_from_facility/ref_to_facility', 
                                                             xlabel='group',
                                                             ylabel='Number')
210/72:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape ##(181436, 14)

refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()

refto_from[~refto_from.ref_from_facility. isin(vandlist)].groupby('ref_to_facility')['ref_from_facility', 'patient_count'].sum()
210/73:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape ##(181436, 14)

refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()

refto_from[~refto_from.ref_from_facility. isin(vandlist)].groupby('ref_to_facility')['ref_from_facility', 'patient_count'].agg(sum).sort_values(ascending = False).to_frame()
210/74:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape ##(181436, 14)

refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()

refto_from[~refto_from.ref_from_facility. isin(vandlist)].groupby('ref_to_facility')['ref_from_facility', 'patient_count'].agg(sum).sort_values(ascending = False)
210/75:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape ##(181436, 14)

refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()

refto_from[~refto_from.ref_from_facility. isin(vandlist)].groupby('ref_to_facility')['ref_from_facility', 'patient_count'].agg(sum)
210/76:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape ##(181436, 14)

refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()

refto_from[~refto_from.ref_from_facility. isin(vandlist)].groupby('ref_to_facility')['ref_from_facility', 'patient_count'].agg(sum).sort_values('patient_count', ascennding = False)
210/77:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape ##(181436, 14)

refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()

refto_from[~refto_from.ref_from_facility. isin(vandlist)].groupby('ref_to_facility')['ref_from_facility', 'patient_count'].agg(sum).sort_values('patient_count', ascending = False)
210/78:
#df[~df.country.isin(countries_to_keep)]
# using ~ eliminates the content in the list from the search
not_ref_tovandy = refto_hosonly[~refto_hosonly.ref_from_facility. isin(vandlist)].reset_index()
not_ref_tovandy.groupby('ref_from_facility')['patient_count'].agg(sum).sort_values(ascending = False).to_frame()

not_ref_tovandy.groupby('ref_to_facility')['patient_count'].agg(sum).sort_values(ascending = False).to_frame()
210/79:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape ##(181436, 14)

refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()

refto_from[~refto_from.ref_from_facility. isin(vandlist)].groupby('ref_to_facility')['patient_count'].agg(sum).sort_values('patient_count', ascending = False)
210/80:
#there are three network Acession, Vanderbilt, HCA
refto_from.head(2)
refto_from.shape ##(241849, 13)


refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index().shape ##(181436, 14)

refto_from[~refto_from.ref_from_facility. isin(vandlist)].reset_index()

refto_from[~refto_from.ref_from_facility. isin(vandlist)].groupby('ref_to_facility')['patient_count'].agg(sum).sort_values(ascending = False)
210/81: vandlist
210/82:
refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
refto_hosonly = refto_hosonly.reset_index()
refto_hosonly

refto_from.groupby(['ref_from_group', 'ref_to_group'])['patient_count'].agg('sum')
210/83:
refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
refto_hosonly = refto_hosonly.reset_index()
refto_hosonly

refto_from.groupby(['ref_from_group', 'ref_to_group'])['patient_count'].agg('sum').sort_values().to_frame()
210/84:
refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
refto_hosonly = refto_hosonly.reset_index()
refto_hosonly

refto_from.groupby(['ref_from_group', 'ref_to_group'])['patient_count'].agg('sum').sort_values(ascending =False).to_frame()
210/85:
refto_hosonly = refto_from[refto_from ['ref_to_group'] == "Hospitals"].groupby(['ref_to_facility','ref_from_facility'])['patient_count'].agg('sum').sort_values(ascending = False).to_frame()
refto_hosonly = refto_hosonly.reset_index()
refto_hosonly

refto_from.groupby(['ref_from_group', 'ref_to_group'])['patient_count'].agg('sum').sort_values(ascending =False).to_frame().reset_index()
210/86:

refto_from = refto_from.rename(columns={'ref_from_specialization':'ref_from_speciality'})
refto_from.head(2)

refto_from.to_csv('data/npi_to_from.csv', index = False)
210/87:

refto_from = refto_from.rename(columns={'ref_from_specialization':'ref_from_speciality'})
refto_from.head(2)

refto_from.to_csv('data/npi_to_from.csv', index = False)
refto_from.head(2)
210/88: refto_from.columns
210/89:
#REFERALS from vandy to tristar

query = """
SELECT *
FROM npicode
INNER JOIN hop
ON npicode.npi = hop.from_npi
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%VANDERBILT%'
    AND 
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    vandy_tristar = pd.read_sql(query, db)
vandy_tristar
210/90: refto_from.head(2)
210/91: refto_from.head(2)
210/92: pd.read_csv('data/hop_team_commid.csv').head(5)
210/93: comid = pd.read_csv('data/hop_team_commid.csv').head(5)
210/94: comid = pd.read_csv('data/hop_team_commid.csv')
210/95:
comid = pd.read_csv('data/hop_team_commid.csv')
comid.shape
210/96:
comid = pd.read_csv('data/hop_team_commid.csv')
comid.shape #(41188, 2)
comid.columns()
210/97:
comid = pd.read_csv('data/hop_team_commid.csv')
comid.shape #(41188, 2)
comid.columns
210/98: pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi")
210/99: pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename{'communityid': "ref_to_commid"}
210/100: pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename(columns={'communityid': "ref_to_commid"})
210/101: ref_to_from = pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename(columns={'communityid': "ref_to_commid"})
210/102: pd.merge(refto_from, comid, how="left", left_on="refered_from_npi", right_on="npi").rename(columns={'communityid': "ref_from_commid"})
210/103: ref_to_from = pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename(columns={'communityid': "ref_to_commid"})
210/104:
ref_to_from = pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename(columns={'communityid': "ref_to_commid"})

ref_to_from.head(2)
210/105:
ref_to_from = pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename(columns={'communityId': "ref_to_commid"})

#ref_to_from.head(2)
210/106:
ref_to_from = pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename(columns={'communityId': "ref_to_commid"})

ref_to_from.head(2)
210/107: pd.merge(refto_from, comid, how="left", left_on="refered_from_npi", right_on="npi").rename(columns={'communityId': "ref_from_commid"})
210/108:
refto_from = pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename(columns={'communityId': "ref_to_commid"})

# refto_from.head(2)
210/109:
refto_from = pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename(columns={'communityId': "ref_to_commid"})

refto_from.head(2)
210/110: pd.merge(refto_from, comid, how="left", left_on="refered_from_npi", right_on="npi").rename(columns={'communityId': "ref_from_commid"})
210/111: refto_from = pd.read_csv('data/npi_to_from.csv')
210/112:
refto_from = pd.read_csv('data/npi_to_from.csv')
refto_from.head(2)
210/113:
 pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename(columns={'communityId': "ref_to_commid"})

# refto_from.head(2)
210/114:
refto_from = pd.merge(refto_from, comid, how="left", left_on="ref_to_npi", right_on="npi").rename(columns={'communityId': "ref_to_commid"})

# refto_from.head(2)
210/115: pd.merge(refto_from, comid, how="left", left_on="refered_from_npi", right_on="npi").rename(columns={'communityId': "ref_from_commid"})
210/116: pd.merge(refto_from, comid, how="left", left_on="refered_from_npi", right_on="npi").rename(columns={'communityId': "ref_from_commid"}).drop(columns=('npi_x', 'npi_y'))
210/117: pd.merge(refto_from, comid, how="left", left_on="refered_from_npi", right_on="npi").rename(columns={'communityId': "ref_from_commid"}).drop(columns=['npi_x', 'npi_y']))
210/118: pd.merge(refto_from, comid, how="left", left_on="refered_from_npi", right_on="npi").rename(columns={'communityId': "ref_from_commid"}).drop(columns=['npi_x', 'npi_y'])
210/119:
refto_from = (
    pd.merge(refto_from, comid, how="left", left_on="refered_from_npi", right_on="npi")
    .rename(columns={'communityId': "ref_from_commid"})
    .drop(columns=['npi_x', 'npi_y'])
)
210/120: refto_from.head(5)
210/121:
comid = pd.read_csv('data/hop_team_commid.csv')
comid.shape #(41188, 2)
comid['communityId'].value_counts()
210/122: refto_from.to_csv('data/smita_hop.csv', index=False)
210/123:
comid = pd.read_csv('data/hop_team_commid.csv')
comid.shape #(41188, 2)
comid['communityId'].value_counts() #total number of community 355
comid.columns
210/124: refto_hosonly[refto_hosonly['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_group')['patient_count'].sum().sort_values(ascending =False).to_frame()
210/125: refto_hosonly[refto_hosonly['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_group')['patient_count'].sum().sort_values(ascending =False).to_frame()
210/126: refto_from[refto_from['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_group')['patient_count'].sum().sort_values(ascending =False).to_frame()
210/127: refto_from[refto_from['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_classification')['patient_count'].sum().sort_values(ascending =False).to_frame()
210/128: refto_from[refto_from['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_speciality')['patient_count'].sum().sort_values(ascending =False).to_frame()
210/129: refto_from[refto_from['ref_to_facility'] != 'VANDERBILT UNIVERSITY MEDICAL CENTER'].groupby('ref_from_speciality')['patient_count'].sum().sort_values(ascending =False).to_frame().head(50)
210/130: refto_from[refto_from['ref_from_speciality'] == 'Cardiovascular Disease'].groupby('ref_to_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().head(50)
210/131:
refto_from[(
    refto_from['ref_from_speciality'] == 'Cardiovascular Disease']) & (refto_from['ref_to_group'] == 'Hospitals')].groupby('ref_to_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().head(50)
210/132: refto_from[( refto_from['ref_from_speciality'] == 'Cardiovascular Disease']) & (refto_from['ref_to_group'] == 'Hospitals')].groupby('ref_to_facility')['patient_count'].sum().sort_values(ascending =False).to_frame().head(50)
210/133: refto_from[( refto_from['ref_from_speciality'] == 'Cardiovascular Disease']) & (refto_from['ref_to_group'] == 'Hospitals')].groupby('ref_to_facility')['patient_count'].sum().sort_values(ascending =False).to_frame()
210/134: refto_from[( refto_from['ref_from_speciality'] == 'Cardiovascular Disease') & (refto_from['ref_to_group'] == 'Hospitals')].groupby('ref_to_facility')['patient_count'].sum().sort_values(ascending =False).to_frame()
210/135: refto_from['ref_to_facility'].value_counts()
210/136: refto_from['ref_to_facility'].nunique()
210/137:
refto_from['ref_to_facility'].nunique() #total number of facility = 1327
refto_from['ref_from_facility'].nunique()
210/138: refto_from['patient_count'].sum()
210/139: (refto_from['ref_to_facility'].value_counts()/refto_from['patient_count'].sum())*100
210/140:
refto_from['patient_count'].sum() #total number of patients =40588503

refto_from['ref_to_facility'].value_counts()
210/141: (refto_from['ref_to_facility']['patient_count'].sum()/refto_from['patient_count'].sum())*100
210/142: refto_from['ref_to_facility'].value_counts()
210/143: refto_from['ref_to_facility'].value_counts()/len(refto_from)
210/144: (refto_from['ref_to_facility'].value_counts()/len(refto_from))*100
210/145: ((refto_from['ref_to_facility'].value_counts()/len(refto_from))*100)>1
210/146: (refto_from['ref_to_facility'].value_counts()/len(refto_from))*100
210/147: refto_from(refto_from['ref_to_group']== 'Hospitals')['ref_to_facility'].value_counts()/len(refto_from))*100
210/148: (refto_from(refto_from['ref_to_group']== 'Hospitals')['ref_to_facility'].value_counts()/len(refto_from))*100
210/149: refto_from(refto_from['ref_to_group']== 'Hospitals')['ref_to_facility'].value_counts()/len(refto_from)*100
210/150: refto_from(refto_from['ref_to_group']== 'Hospitals')['ref_to_facility'].value_counts()
210/151: refto_from[refto_from['ref_to_group']== 'Hospitals']['ref_to_facility'].value_counts()
210/152: refto_from[refto_from['ref_to_group']== 'Hospitals']['ref_to_facility'].value_counts()/len(refto_from)
210/153: refto_from[refto_from['ref_to_group']== 'Hospitals']['ref_to_facility'].value_counts()/len(refto_from)*100
210/154: refto_from['ref_to_facility'].isin(['SAINT THOMAS'])
210/155: refto_from['ref_to_facility'].isin(['SAINT THOMAS'])['patient_cout'].sum()
210/156: refto_from['ref_to_facility'].isin(['SAINT THOMAS'])['patient_count'].sum()
210/157: refto_from[(refto_from['ref_to_facility'].isin[('SAINT THOMAS')])]['patient_count'].sum()
210/158: refto_from[(refto_from['ref_to_facility'].isin[('SAINT THOMAS')])]
210/159: refto_from[(refto_from['ref_to_facility'].isin[('SAINT THOMAS')])]
210/160: refto_from[(refto_from['ref_to_facility'].isin[("SAINT THOMAS")])]
210/161: refto_from[(refto_from['ref_to_facility'].isin("SAINT THOMAS"))]
210/162: refto_from[(refto_from['ref_to_facility'].isin[("SAINT THOMAS", "VANDERBILT")])]
210/163: refto_from(refto_from['ref_to_facility'].isin[("SAINT THOMAS", "VANDERBILT")])
210/164: refto_from['ref_to_facility'].isin[("SAINT THOMAS", "VANDERBILT")]
210/165: refto_from['ref_to_facility'].isin(["SAINT THOMAS", "VANDERBILT"])
210/166: refto_from['ref_to_facility'].isin(["SAINT THOMAS"])
210/167: refto_from['ref_to_facility'].isin(["SAINT THOMAS"]).value_counts()
210/168: refto_from[refto_from['ref_to_facility']== 'VANDERBILT UNIVERSITY MEDICAL CENTER']['ref_to_npi'].value_counts()
210/169: refto_from[refto_from['ref_to_facility']== 'VANDERBILT UNIVERSITY MEDICAL CENTER']['ref_to_npi', 'ref_to_commid' ].value_counts()
210/170: refto_from[refto_from['ref_to_facility']== 'VANDERBILT UNIVERSITY MEDICAL CENTER']['ref_to_commid' ].value_counts()
210/171: refto_from[refto_from['ref_to_facility']== 'VANDERBILT UNIVERSITY MEDICAL CENTER']['ref_to_npi'].unique()
210/172: refto_from[refto_from['ref_to_facility']== 'VANDERBILT UNIVERSITY MEDICAL CENTER']['ref_to_commid' ].unique()
210/173: vandy['provider_organization_name_(legal_business_name)'].to_list()
210/174:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%VANDERBILT%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
     vandy= pd.read_sql(query, db)
vandy
210/175: vandy['provider_organization_name_(legal_business_name)'].to_list()
210/176:
vandy['provider_organization_name_(legal_business_name)'].to_list()
vandy['provider_organization_name_(legal_business_name)'].value_counts()
210/177:
vandy['provider_organization_name_(legal_business_name)'].to_list()
vandy['provider_organization_name_(legal_business_name)'].nunique()
210/178:
vandy['provider_organization_name_(legal_business_name)'].to_list()
vandy['provider_organization_name_(legal_business_name)'].nunique() #33
vandy['provider_organization_name_(legal_business_name)'].unique()
210/179:
refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])['ref_to_commid' ].unique()
210/180:
refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])
210/181:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))]
210/182:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))]['ref_to_commid'].value_counts()
210/183:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))]['ref_to_npi'].value_counts()
210/184:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))]['ref_to_npi'].unique()
210/185:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))]
210/186:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))]['ref_from_commid'].unique()
210/187:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))]['ref_from_commid'].nunique()
210/188:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))]['ref_from_commid'].value_counts()
210/189:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%SAINT THOMAS%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
     vandy= pd.read_sql(query, db)
vandy
210/190:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
    AND`provider_organization_name_(legal_business_name)` LIKE '%SAINT THOMAS%'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    saint_thomas= pd.read_sql(query, db)
saint_thomas
210/191:
saint_thomas['provider_organization_name_(legal_business_name)'].to_list()
saint_thomas['provider_organization_name_(legal_business_name)'].nunique() #33
saint_thomas['provider_organization_name_(legal_business_name)'].unique()
210/192:
saint_thomas['provider_organization_name_(legal_business_name)'].to_list()
saint_thomas['provider_organization_name_(legal_business_name)'].nunique() #33
# saint_thomas['provider_organization_name_(legal_business_name)'].unique()
210/193:
saint_thomas['provider_organization_name_(legal_business_name)'].to_list()
saint_thomas['provider_organization_name_(legal_business_name)'].nunique() #26
saint_thomas['provider_organization_name_(legal_business_name)'].unique()
210/194:
refto_from[(refto_from['ref_to_facility'].isin(['SAINT THOMAS OUTPATIENT NEUROSURGICAL CENTER',
       'SAINT THOMAS OUTPATIENT CARDIAC CATHETERIZATION CENTER, LLC',
       'WILLIAMSON SAINT THOMAS COMMUNITY HEALTH', 'SAINT THOMAS HEART',
       'SAINT THOMAS HEALTH',
       'SAINT THOMAS EMERGENCY MEDICAL SERVICES, LLC',
       'SAINT THOMAS WEST HOSPITAL', 'SAINT THOMAS HOME HEALTH',
       'SAINT THOMAS MEDICAL PARTNERS',
       'SAINT THOMAS CAMPUS SURGICARE LP',
       'SAINT THOMAS HOME RECOVERY CARE, LLC',
       'SAINT THOMAS DEKALB HOSPITAL, LLC',
       'SAINT THOMAS STONES RIVER HOSPITAL, LLC',
       'SAINT THOMAS HICKMAN HOSPITAL',
       'SAINT THOMAS HIGHLANDS HOSPITAL, LLC',
       'SAINT THOMAS RIVER PARK HOSPITAL, LLC',
       'SAINT THOMAS RUTHERFORD HOSPITAL',
       'SAINT THOMAS RUTHERFORD HOSPITAL LAB',
       'SAINT THOMAS WEST HOSPITAL LAB', 'SAINT THOMAS NETWORK',
       'CENTRAL SETON SAINT THOMAS HEALTH SERVICES',
       'SAINT THOMAS CARDIOLOGY CONSULTANTS, PC',
       'ASCENSION SAINT THOMAS LEBANON SURGERY CENTER LLC',
       'SAINT THOMAS SURGERY CENTER NEW SALEM, LLC',
       'SAINT THOMAS HOME HEALTH, LLC',
       'SAINT THOMAS REHABILITATION HOSPITAL, LLC']['ref_from_commid'].value_counts()
210/195:
refto_from[(refto_from['ref_to_facility'].isin(['SAINT THOMAS OUTPATIENT NEUROSURGICAL CENTER',
       'SAINT THOMAS OUTPATIENT CARDIAC CATHETERIZATION CENTER, LLC',
       'WILLIAMSON SAINT THOMAS COMMUNITY HEALTH', 'SAINT THOMAS HEART',
       'SAINT THOMAS HEALTH',
       'SAINT THOMAS EMERGENCY MEDICAL SERVICES, LLC',
       'SAINT THOMAS WEST HOSPITAL', 'SAINT THOMAS HOME HEALTH',
       'SAINT THOMAS MEDICAL PARTNERS',
       'SAINT THOMAS CAMPUS SURGICARE LP',
       'SAINT THOMAS HOME RECOVERY CARE, LLC',
       'SAINT THOMAS DEKALB HOSPITAL, LLC',
       'SAINT THOMAS STONES RIVER HOSPITAL, LLC',
       'SAINT THOMAS HICKMAN HOSPITAL',
       'SAINT THOMAS HIGHLANDS HOSPITAL, LLC',
       'SAINT THOMAS RIVER PARK HOSPITAL, LLC',
       'SAINT THOMAS RUTHERFORD HOSPITAL',
       'SAINT THOMAS RUTHERFORD HOSPITAL LAB',
       'SAINT THOMAS WEST HOSPITAL LAB', 'SAINT THOMAS NETWORK',
       'CENTRAL SETON SAINT THOMAS HEALTH SERVICES',
       'SAINT THOMAS CARDIOLOGY CONSULTANTS, PC',
       'ASCENSION SAINT THOMAS LEBANON SURGERY CENTER LLC',
       'SAINT THOMAS SURGERY CENTER NEW SALEM, LLC',
       'SAINT THOMAS HOME HEALTH, LLC',
       'SAINT THOMAS REHABILITATION HOSPITAL, LLC'])]['ref_from_commid'].value_counts()
210/196:
refto_from[(refto_from['ref_to_facility'].isin(['SAINT THOMAS OUTPATIENT NEUROSURGICAL CENTER',
       'SAINT THOMAS OUTPATIENT CARDIAC CATHETERIZATION CENTER, LLC',
       'WILLIAMSON SAINT THOMAS COMMUNITY HEALTH', 'SAINT THOMAS HEART',
       'SAINT THOMAS HEALTH',
       'SAINT THOMAS EMERGENCY MEDICAL SERVICES, LLC',
       'SAINT THOMAS WEST HOSPITAL', 'SAINT THOMAS HOME HEALTH',
       'SAINT THOMAS MEDICAL PARTNERS',
       'SAINT THOMAS CAMPUS SURGICARE LP',
       'SAINT THOMAS HOME RECOVERY CARE, LLC',
       'SAINT THOMAS DEKALB HOSPITAL, LLC',
       'SAINT THOMAS STONES RIVER HOSPITAL, LLC',
       'SAINT THOMAS HICKMAN HOSPITAL',
       'SAINT THOMAS HIGHLANDS HOSPITAL, LLC',
       'SAINT THOMAS RIVER PARK HOSPITAL, LLC',
       'SAINT THOMAS RUTHERFORD HOSPITAL',
       'SAINT THOMAS RUTHERFORD HOSPITAL LAB',
       'SAINT THOMAS WEST HOSPITAL LAB', 'SAINT THOMAS NETWORK',
       'CENTRAL SETON SAINT THOMAS HEALTH SERVICES',
       'SAINT THOMAS CARDIOLOGY CONSULTANTS, PC',
       'ASCENSION SAINT THOMAS LEBANON SURGERY CENTER LLC',
       'SAINT THOMAS SURGERY CENTER NEW SALEM, LLC',
       'SAINT THOMAS HOME HEALTH, LLC',
       'SAINT THOMAS REHABILITATION HOSPITAL, LLC'])])['ref_from_commid'].value_counts()
210/197:
refto_from[(refto_from['ref_to_facility'].isin(['SAINT THOMAS OUTPATIENT NEUROSURGICAL CENTER',
       'SAINT THOMAS OUTPATIENT CARDIAC CATHETERIZATION CENTER, LLC',
       'WILLIAMSON SAINT THOMAS COMMUNITY HEALTH', 'SAINT THOMAS HEART',
       'SAINT THOMAS HEALTH',
       'SAINT THOMAS EMERGENCY MEDICAL SERVICES, LLC',
       'SAINT THOMAS WEST HOSPITAL', 'SAINT THOMAS HOME HEALTH',
       'SAINT THOMAS MEDICAL PARTNERS',
       'SAINT THOMAS CAMPUS SURGICARE LP',
       'SAINT THOMAS HOME RECOVERY CARE, LLC',
       'SAINT THOMAS DEKALB HOSPITAL, LLC',
       'SAINT THOMAS STONES RIVER HOSPITAL, LLC',
       'SAINT THOMAS HICKMAN HOSPITAL',
       'SAINT THOMAS HIGHLANDS HOSPITAL, LLC',
       'SAINT THOMAS RIVER PARK HOSPITAL, LLC',
       'SAINT THOMAS RUTHERFORD HOSPITAL',
       'SAINT THOMAS RUTHERFORD HOSPITAL LAB',
       'SAINT THOMAS WEST HOSPITAL LAB', 'SAINT THOMAS NETWORK',
       'CENTRAL SETON SAINT THOMAS HEALTH SERVICES',
       'SAINT THOMAS CARDIOLOGY CONSULTANTS, PC',
       'ASCENSION SAINT THOMAS LEBANON SURGERY CENTER LLC',
       'SAINT THOMAS SURGERY CENTER NEW SALEM, LLC',
       'SAINT THOMAS HOME HEALTH, LLC',
       'SAINT THOMAS REHABILITATION HOSPITAL, LLC']))]['ref_from_commid'].value_counts()
210/198:
refto_from[(refto_from['ref_to_facility'].isin(['SAINT THOMAS OUTPATIENT NEUROSURGICAL CENTER',
       'SAINT THOMAS OUTPATIENT CARDIAC CATHETERIZATION CENTER, LLC',
       'WILLIAMSON SAINT THOMAS COMMUNITY HEALTH', 'SAINT THOMAS HEART',
       'SAINT THOMAS HEALTH',
       'SAINT THOMAS EMERGENCY MEDICAL SERVICES, LLC',
       'SAINT THOMAS WEST HOSPITAL', 'SAINT THOMAS HOME HEALTH',
       'SAINT THOMAS MEDICAL PARTNERS',
       'SAINT THOMAS CAMPUS SURGICARE LP',
       'SAINT THOMAS HOME RECOVERY CARE, LLC',
       'SAINT THOMAS DEKALB HOSPITAL, LLC',
       'SAINT THOMAS STONES RIVER HOSPITAL, LLC',
       'SAINT THOMAS HICKMAN HOSPITAL',
       'SAINT THOMAS HIGHLANDS HOSPITAL, LLC',
       'SAINT THOMAS RIVER PARK HOSPITAL, LLC',
       'SAINT THOMAS RUTHERFORD HOSPITAL',
       'SAINT THOMAS RUTHERFORD HOSPITAL LAB',
       'SAINT THOMAS WEST HOSPITAL LAB', 'SAINT THOMAS NETWORK',
       'CENTRAL SETON SAINT THOMAS HEALTH SERVICES',
       'SAINT THOMAS CARDIOLOGY CONSULTANTS, PC',
       'ASCENSION SAINT THOMAS LEBANON SURGERY CENTER LLC',
       'SAINT THOMAS SURGERY CENTER NEW SALEM, LLC',
       'SAINT THOMAS HOME HEALTH, LLC',
       'SAINT THOMAS REHABILITATION HOSPITAL, LLC']))]['ref_to_commid'].value_counts()
210/199:
refto_from[(refto_from['ref_to_facility'].isin(['SAINT THOMAS OUTPATIENT NEUROSURGICAL CENTER',
       'SAINT THOMAS OUTPATIENT CARDIAC CATHETERIZATION CENTER, LLC',
       'WILLIAMSON SAINT THOMAS COMMUNITY HEALTH', 'SAINT THOMAS HEART',
       'SAINT THOMAS HEALTH',
       'SAINT THOMAS EMERGENCY MEDICAL SERVICES, LLC',
       'SAINT THOMAS WEST HOSPITAL', 'SAINT THOMAS HOME HEALTH',
       'SAINT THOMAS MEDICAL PARTNERS',
       'SAINT THOMAS CAMPUS SURGICARE LP',
       'SAINT THOMAS HOME RECOVERY CARE, LLC',
       'SAINT THOMAS DEKALB HOSPITAL, LLC',
       'SAINT THOMAS STONES RIVER HOSPITAL, LLC',
       'SAINT THOMAS HICKMAN HOSPITAL',
       'SAINT THOMAS HIGHLANDS HOSPITAL, LLC',
       'SAINT THOMAS RIVER PARK HOSPITAL, LLC',
       'SAINT THOMAS RUTHERFORD HOSPITAL',
       'SAINT THOMAS RUTHERFORD HOSPITAL LAB',
       'SAINT THOMAS WEST HOSPITAL LAB', 'SAINT THOMAS NETWORK',
       'CENTRAL SETON SAINT THOMAS HEALTH SERVICES',
       'SAINT THOMAS CARDIOLOGY CONSULTANTS, PC',
       'ASCENSION SAINT THOMAS LEBANON SURGERY CENTER LLC',
       'SAINT THOMAS SURGERY CENTER NEW SALEM, LLC',
       'SAINT THOMAS HOME HEALTH, LLC',
       'SAINT THOMAS REHABILITATION HOSPITAL, LLC']))]['ref_from_commid'].value_counts()
210/200: refto_from['ref_from_commid'].nunique()
210/201:
refto_from['ref_from_commid'].nunique() #355

refto_from[refto_from['ref_from_commid']== 9]['refto_facility'].value_counts()
210/202:
refto_from['ref_from_commid'].nunique() #355

refto_from[(refto_from['ref_from_commid']== 9)]['refto_facility'].value_counts()
210/203:
refto_from['ref_from_commid'].nunique() #355

refto_from[(refto_from['ref_from_commid']== '9')]['refto_facility'].value_counts()
210/204:
refto_from['ref_from_commid'].nunique() #355

refto_from[(refto_from['ref_from_commid']== 9)]['ref_to_facility'].value_counts()
210/205:
refto_from['ref_from_commid'].nunique() #355

refto_from[refto_from['ref_from_commid']== 9]['ref_to_facility'].value_counts()
210/206:
refto_from['ref_from_commid'].nunique() #355

refto_from[refto_from['ref_from_commid']== 11117]['ref_to_facility'].value_counts()
210/207:
refto_from['ref_from_commid'].nunique() #355

refto_from[refto_from['ref_from_commid']== 363]['ref_to_facility'].value_counts()
210/208:
refto_from['ref_from_commid'].nunique() #355

refto_from[refto_from['ref_to_commid']== 363]['ref_to_facility'].value_counts()
210/209:
refto_from['ref_from_commid'].nunique() #355

refto_from[refto_from['ref_to_commid']== 11117]['ref_to_facility'].value_counts()
210/210:
refto_from['ref_from_commid'].nunique() #355

refto_from[refto_from['ref_to_commid']== 7]['ref_to_facility'].value_counts()
210/211: ref_to_from.groupby('ref_from_commid')['patient_count'].sum().sort_values(ascending =False).to_frame().head(50)
210/212: ref_to_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame().head(50)
210/213: ref_to_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum()
210/214: refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame().head(50)
210/215: refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/216: refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame().head(100)
210/217: refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame().head(10)
210/218:
refto_from['ref_from_commid'].nunique() #355

refto_from[refto_from['ref_to_commid']== 83]['ref_to_facility'].value_counts()
210/219:
refto_from['ref_from_commid'].nunique() #355

refto_from[refto_from['ref_to_commid']== 83]['ref_to_facility'].value_counts().sort_values(ascending =False).to_frame().head(10)
210/220:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].group_by(['ref_from_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame().head(10)
210/221:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame().head(10)
210/222:
refto_from[(refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/223: refto_from[refto_from['ref_to_commid']== 83]['ref_to_facility'].nunique()
210/224:
refto_from[refto_from['ref_to_commid']== 83]['ref_to_facility'].nunique() #246

refto_from[refto_from['ref_to_commid']== 83]['ref_to_facility'].unique()
210/225:
refto_from[refto_from['ref_to_commid']== 83]['ref_to_facility'].nunique() #246

refto_from[refto_from['ref_from_commid']== 83]['ref_from_facility'].unique()
210/226:
refto_from[refto_from['ref_to_commid']== 83]['ref_to_facility'].nunique() #246

refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_facility'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/227:
refto_from[refto_from['ref_to_commid']== 83]['ref_to_facility'].nunique() #246

refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_facility'])['patient_count'].sum().sort_values(ascending =False).to_frame().head(30)
210/228:
refto_from[refto_from['ref_to_commid']== 83]['ref_to_facility'].nunique() #246

refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_facility'])['patient_count'].sum().sort_values(ascending =False).to_frame().tail(30)
210/229: refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum().sort_values(ascending =False).to_frame().tail(30)
210/230: refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/231: refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum().sort_values(ascending =False).to_frame().head(30)
210/232: refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum()/(refto_from['patient_count'].sum()).sort_values(ascending =False).to_frame().head(30)
210/233: (refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()).sort_values(ascending =False).to_frame().head(30)
210/234: (refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum())
210/235: ((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100
210/236: (((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False)
210/237: (((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame()
210/238: (((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30)
210/239: (((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(40)
210/240: (((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(30)
210/241:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).plot(kind="bar", 
title = 'Referals from cluste 83', xlabel='', ylabel='% of total patient number in cbsa 34980'
210/242:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).plot(kind="bar", 
title = 'Referals from cluste 83', xlabel='', ylabel='% of total patient number in cbsa 34980')
210/243:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).plot(kind="bar", 
title = 'Referals from cluste 83', xlabel='', ylabel='% of total patient number in cbsa 34980')
210/244:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(10).plot(kind="bar", 
title = 'Referals from cluste 83', xlabel='', ylabel='% of total patient number in cbsa 34980')
210/245:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(10).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='', ylabel='% of total patient number in cbsa 34980')
210/246:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(10).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/247:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980', )
210/248:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980', )
210/249:

(((refto_from[(refto_from['ref_from_commid']== 83]) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']).groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30)
210/250:

(((refto_from[(refto_from['ref_from_commid']== 83]) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30)
210/251:

(((refto_from[(refto_from['ref_from_commid']== 83]) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']).groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30)
210/252:

(((refto_from[(refto_from['ref_from_commid']== 83]) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']).groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980'.plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980' )
210/253:

(((refto_from[(refto_from['ref_from_commid']== 83]) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])).groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980'.plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/254:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])).groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980'.plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/255:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']).groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980'.plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/256:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']).groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980'.plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/257:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980'.plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/258:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980'.plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/259:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980'.plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980',)
210/260:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/261:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().plot(kind="bar", 
title = 'Referrals from cluste 83 into VUMC', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/262:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(30).plot(kind="bar", 
title = 'Referrals from cluste 83 into VUMC', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/263:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(30).plot(kind="bar", 
title = 'Referrals from cluste 83 into VUMC', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/264:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(40).plot(kind="bar", 
title = 'Referrals from cluste 83 into VUMC', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/265:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = 'Referrals from cluste 83 into VUMC', xlabel='speciality', ylabel='% of total patient number in cbsa 34980')
210/266:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = 'Referrals from cluste 83 into VUMC', xlabel='least referred speciality', ylabel='% of total patient number in cbsa 34980')
210/267:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = 'Referrals from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/268:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = 'Top 20 least Referrals from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/269:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(20).plot(kind="bar", 
title = '2o most Referred specilaity from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/270:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/271:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(20).plot(kind="bar", 
title = '20 most Referred specilaities from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980', )
210/272:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = 'Referrals from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980', )
210/273: (((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().shape
210/274:
#68 speciality
(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(10)
210/275:

(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = '20 least referred specilaities from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980', )
210/276:
#68 speciality
(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame()
.loc[refto_from['ref_from_speciality'] != 'Diagnostic Radiology'].head(10)
210/277:
#68 speciality
(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().loc[refto_from['ref_from_speciality'] != 'Diagnostic Radiology'].head(10)
210/278:
#68 speciality
(((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(10)
210/279: (refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum())//(refto_from['patient_count'].sum()))*100).sort_values(ascending =False).to_frame().head(10)
210/280: (refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum())/(refto_from['patient_count'].sum())*100).sort_values(ascending =False).to_frame().head(10)
210/281: ((refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum())/(refto_from['patient_count'].sum())*100).sort_values(ascending =False).to_frame().head(10)
210/282: ((refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum())/(refto_from['patient_count'].sum())*100).sort_values(ascending =False).to_frame().head(20)
210/283: ((refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum())/(refto_from['patient_count'].sum())*100).sort_values(ascending =False).to_frame()
210/284: ((refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum())/(refto_from['patient_count'].sum())*100).sort_values(ascending =False).to_frame().head(10)
210/285:
(
    ((refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum())/(refto_from['patient_count'].sum())*100)
    .sort_values(ascending =False)
    .to_frame().head(10)
)
210/286:
(
    ((refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum())/(refto_from['patient_count'].sum())*100)
    .sort_values(ascending =False)
    .to_frame().rename(columns={'patient_count': '%of total patient_count'})
    .head(10)
)
210/287:
(
    ((refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum())/(refto_from['patient_count'].sum())*100)
    .sort_values(ascending =False)
    .to_frame().rename(columns={'patient_count': '%of total patient_count', 
                                'ref_from_commid': 'Referred_From_Community#',
                               'ref_to_commid': 'Referred_To_Community#' })
    .head(10)
)
210/288:
(
    ((refto_from.groupby(['ref_from_commid', 'ref_to_commid'])['patient_count'].sum())/(refto_from['patient_count'].sum())*100)
    .sort_values(ascending =False)
    .to_frame().rename(columns={'patient_count': '%of total patient_count', 
                                'ref_from_commid': 'Referred_From_Community#',
                               'ref_to_commid': 'Referred_To_Community#' })
    .head(10)
)
210/289:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(20).plot(kind="bar", 
title = '20 most Referred specilaity from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/290:
(
    ((refto_from.groupby(['ref_from_commid', 'ref_to_commid', 'ref_to_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum())*100)
    .sort_values(ascending =False)
    .to_frame().rename(columns={'patient_count': '%of total patient_count', 
                                'ref_from_commid': 'Referred_From_Community#',
                               'ref_to_commid': 'Referred_To_Community#' })
    .head(10)
)
210/291: refto_from['ref_to_speciality'].value_counts()
210/292: refto_from['ref_to_speciality'].unique()
210/293: refto_from[(refto_from['ref_from_commid']== 83)['ref_to_speciality'].unique()
210/294: refto_from[(refto_from['ref_from_commid']== 83)]['ref_to_speciality'].unique()
210/295: refto_from[(refto_from['ref_from_commid']== 83)]['ref_to_speciality'].value_counts()
210/296: refto_from[(refto_from['ref_from_commid']== 83)]['ref_to_speciality'].unique()
210/297:
(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from[(refto_from['ref_to_speciality'].isin([nan, 'Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))]
 .groupby(['ref_from_commid'])['patient_count'].sum()
 .sort_values(ascending =False)
 .to_frame())
210/298:
(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))]
 .groupby(['ref_to_facility'])['patient_count'].sum()
 .sort_values(ascending =False)
 .to_frame())
210/299:
(
    (((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))]
 .groupby(['ref_to_facility'])['patient_count'].sum()
 .sort_values(ascending =False)
 .to_frame())
210/300:
(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(20).plot(kind="bar", 
title = '20 most Referred specilaity from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/301:
(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))]
 .groupby(['ref_to_facility'])['patient_count'].sum()
 .sort_values(ascending =False)
 .to_frame()
210/302:
(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_to_facility'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/303:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/304:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin([nan, 'Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/305:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_facility'].isin([nan, 'Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/306:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/307:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame(
210/308:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame()
210/309:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame()
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/310:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(30)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/311:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport])
                                                                                            )].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20).plot(kind="bar", 
title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/312:

((((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport]))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20)
                                                                                            )                                                                                        
#                                                                                             .plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/313:

((((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport]))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20))
                                                                                                                                                                                  
#                                                                                             .plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/314:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport]))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().tail(20)
                                                                                                                                                                                  
#                                                                                             .plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/315:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(30)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/316:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 'Critical Care Medicine', 'Pain Medicine',
       'Dermatopathology', 'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Clinical Pathology/Laboratory Medicine', 'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
       'Home Infusion Therapy Pharmacy', 'Air Transport',
       'Land Transport']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(30)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/317:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Diagnostic Radiology', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
        'Home Infusion Therapy Pharmacy']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(30)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/318: refto_from[(refto_from['ref_from_commid']== 83)]['ref_to_speciality'].unique().sort_values()
210/319: refto_from[(refto_from['ref_from_commid']== 83)]['ref_to_speciality'].unique().to_list()
210/320: refto_from[(refto_from['ref_from_commid']== 83)]['ref_to_speciality'].unique()
210/321:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
       'Anatomic Pathology & Clinical Pathology',
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
        'Home Infusion Therapy Pharmacy']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(30)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/322:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       'Customized Equipment', 'Oxygen Equipment & Supplies',
       'Parenteral & Enteral Nutrition', 'Community/Retail Pharmacy',
        'Home Infusion Therapy Pharmacy']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(30)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/323:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       
       'Parenteral & Enteral Nutrition' 
       ]))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(30)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/324:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       
       'Parenteral & Enteral Nutrition' 
       ]))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(30)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/325:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       
       'Parenteral & Enteral Nutrition' 
       ]))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame()
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/326:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology', 'Radiation Oncology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 'Prescribing (Medical)',
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
       'Medical', 'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Federally Qualified Health Center (FQHC)', 'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
       'Multi-Specialty', 'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       
       'Parenteral & Enteral Nutrition' 
       ]))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(50)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/327:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
       'Counseling', 
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
        'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
      'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       
       'Parenteral & Enteral Nutrition' 
       ]))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(50)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/328:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
     
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
        'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       'Medical Specialty',
       'Mental Health (Including Community Mental Health Center)',
      'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       
       'Parenteral & Enteral Nutrition' 
       ]))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(50)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/329:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
     
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
        'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       
       'Mental Health (Including Community Mental Health Center)',
      'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       
       'Parenteral & Enteral Nutrition' 
       ]))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(50)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/330: refto_from.shape
210/331:
refto_from.shape #241849, 15)
refto_from.columns
210/332:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
210/333:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from['ref_to_group'].value_counts()
210/334:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from.groupby(['ref_to_group'])['ref_to-specilaity'].value_counts()
210/335:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from.groupby('ref_to_group')['ref_to-specilaity'].value_counts()
210/336:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from.groupby(['ref_to_group'])['ref_to_specilaity'].value_counts()
210/337:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts()
210/338:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame()
210/339:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().head(50)
210/340:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[50:100]
210/341:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[100:168]
210/342:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[100:150]
210/343:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[150:168]
210/344:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
     
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
        'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       
       'Mental Health (Including Community Mental Health Center)',
      'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       
       'Parenteral & Enteral Nutrition' 
       ]))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(50)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/345:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
     
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
        'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       
       'Mental Health (Including Community Mental Health Center)',
      'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       
       'Parenteral & Enteral Nutrition' 
       ]))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().head(50)
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/346:

(((refto_from[(refto_from['ref_from_commid']== 83) & (refto_from['ref_to_speciality'].isin(['Allergy', 
                                                                                            'Critical Care Medicine',
                                                                                            'Pain Medicine',
       'Dermatopathology', 
                                                                                            'MOHS-Micrographic Surgery',
       'Emergency Medical Services', 
                                                                                            'Pediatric Emergency Medicine',
       'Addiction Medicine', 'Adult Medicine', 'Geriatric Medicine',
       'Hospice and Palliative Medicine', 'Sports Medicine',
       'Advanced Heart Failure and Transplant Cardiology',
       'Allergy & Immunology', 'Cardiovascular Disease',
       'Clinical Cardiac Electrophysiology',
       'Endocrinology, Diabetes & Metabolism', 'Gastroenterology',
       'Hematology', 'Hematology & Oncology', 'Infectious Disease',
       'Interventional Cardiology', 'Medical Oncology', 'Nephrology',
       'Pulmonary Disease', 'Rheumatology', 'Sleep Medicine',
       'Female Pelvic Medicine and Reconstructive Surgery',
       'Gynecologic Oncology', 'Gynecology', 'Obstetrics',
       'Ophthalmic Plastic and Reconstructive Surgery',
       'Retina Specialist', 'Adult Reconstructive Orthopaedic Surgery',
       'Foot and Ankle Surgery', 'Hand Surgery',
       'Orthopaedic Surgery of the Spine', 'Orthopaedic Trauma',
       'Facial Plastic Surgery', 'Plastic Surgery within the Head & Neck',
       'Interventional Pain Medicine',
      
       'Cytopathology',
       'Brain Injury Medicine', 'Geriatric Psychiatry', 'Neurology',
       'Psychiatry', 'Neuroradiology',
       'Pediatric Radiology',
       'Therapeutic Radiology', 'Vascular & Interventional Radiology',
       'Plastic and Reconstructive Surgery', 'Surgery of the Hand',
       'Surgical Oncology', 'Trauma Surgery', 'Vascular Surgery',
       'Mental Health', 'Clinical', 'Clinical Child & Adolescent',
     
       'Oral and Maxillofacial Surgery', 'Gerontology', 'Pain Management',
       'Acute Care', 'Medical-Surgical',
       'Psychiatric/Mental Health, Adult', 'Adult Health', 'Family',
       'Primary Care', 'Psychiatric/Mental Health', "Women's Health",
        'Surgical', 'Foot & Ankle Surgery', 'Foot Surgery',
       'Primary Podiatric Medicine', 'Ambulatory Surgical',
       'End-Stage Renal Disease (ESRD) Treatment', 'Endoscopy',
       
       'Mental Health (Including Community Mental Health Center)',
      'Physical Therapy', 'Podiatric', 'Radiology',
       'Radiology, Mobile', 'Rehabilitation', 'Rural Health',
       'Sleep Disorder Diagnostic', 'Urgent Care', 'Critical Access',
       
       'Parenteral & Enteral Nutrition' 
       ]))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame()
# .tail(20).plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
210/347:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from['refto_group'].nunique()
# refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[150:168]
210/348:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from['refto_group'].nunique
# refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[150:168]
210/349:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from['refto_group'].unique()
# refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[150:168]
210/350:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from['ref_to_group'].unique()
# refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[150:168]
210/351:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from['ref_to_group'].nunique()
# refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[150:168]
210/352:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from['ref_to_group'].nunique() #24 groups
refto_from['ref_to_speciality'].nunique()
# refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[150:168]
210/353:
refto_from.shape #241849, 15)
refto_from.columns 

# ['ref_to_npi', 'ref_to_facility', 'ref_to_group',
#        'ref_to_classification', 'ref_to_speciality', 'refered_from_npi',
#        'patient_count', 'transaction_count', 'ref_from_taxcode',
#        'ref_from_facility', 'ref_from_group', 'ref_from_classification',
#        'ref_from_speciality', 'ref_to_commid', 'ref_from_commid']

refto_from.head(2)
refto_from['ref_to_group'].nunique() #24 groups
refto_from['ref_to_speciality'].nunique() #161
refto_from['ref_to_facility'].nunique() 
# refto_from.groupby(['ref_to_group'])['ref_to_speciality'].value_counts().to_frame().iloc[150:168]
210/354: refto_from.groupby(['ref_to_group', 'ref_to_apeciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame()
210/355: ((refto_from.groupby(['ref_to_group', 'ref_to_apeciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame()
210/356:
(
    (((refto_from.groupby(['ref_to_group', 'ref_to_apeciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count']
                                                                                                             .sum()))*100).to_frame())
210/357:

(((refto_from.groupby(['ref_to_group', 'ref_to_apeciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame()
210/358:

(((refto_from.groupby(['ref_to_group', 'ref_to_apeciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame()
210/359:

(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame()
210/360:
()
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
.to_frame())
210/361:
()
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
    .to_frame())
210/362:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
    .to_frame())
210/363:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
    .to_frame()
    .sort_values(ascending = False)
)
210/364:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
)
210/365:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .iloc(1:50)
)
210/366:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .iloc[1:50]
)
210/367:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .iloc[1:50]
    .plot(kind="bar", 
title = '20 least referred specilaities from cluste 83', xlabel='speciality', ylabel='% of total patient number in cbsa 34980', )
)
210/368:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .iloc[1:50]
    
)
210/369:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .iloc[1:20]
    
)
210/370:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    
    
)
210/371:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .head(10)
    
    
)

#total477
210/372:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .head(10)
    
    
)

#total477
210/373:
(
(((refto_from.groupby(['ref_to_group', 'ref_to_speciality'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .head(10)
    
    
)
210/374:
(
(((refto_from.groupby(['ref_to_speciality'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .head(10)
    
    
)
210/375:
(
(((refto_from.groupby(['ref_to_speciality'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[2:20]
    
    
)
210/376:
(
(((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[2:20]
    
    
)
210/377:
(
(((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[0:20]
    
    
)
210/378:
(
    (refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[0:20]
    )
210/379:
(
   ((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[0:20]
    )
210/380:
(
   (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[0:20]
    )
210/382:
(
   (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False).to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[0:20]
    )
210/383:
(
(((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False).to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[0:20]
    )
210/384:

(((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False).to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[0:20]
210/385:

(((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False).to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[0:20]
    )
210/386:
(
(((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 
                                        'Hematology & Oncology', 
                                        'Nephrology', 
                                        'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology'].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[2:20]
    
    
)





# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/387:
(
((refto_from(refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 
                                        'Hematology & Oncology', 
                                        'Nephrology', 
                                        'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[2:20]
    
    
)





# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/389:
(
((refto_from[(refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 
                                        'Hematology & Oncology', 
                                        'Nephrology', 
                                        'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology'])].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False)
 .to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[2:20]
    
    
)





# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/391:
(
((refto_from[(refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 
                                        'Hematology & Oncology', 
                                        'Nephrology', 
                                        'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology'])].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100)
   .sort_values(ascending = False).to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[2:20]
    
    
)





# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/392:
(
((refto_from[(refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 
                                        'Hematology & Oncology', 
                                        'Nephrology', 
                                        'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology'])].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending = False).to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[2:20]
    
    
)





# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/393:
(
(((refto_from[(refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 
                                        'Hematology & Oncology', 
                                        'Nephrology', 
                                        'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology'])].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending = False).to_frame()
    .rename(columns={'patient_count': '% of total patient_count'})
    .iloc[2:20]
    
    
)





# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/394:
(
((refto_from[(refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 
                                        'Hematology & Oncology', 
                                        'Nephrology', 
                                        'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology'])].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending = False).to_frame().rename(columns={'patient_count': '% of total patient_count'}).iloc[2:20]
    
    
)





# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/395:
(
((refto_from[(refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 
                                        'Hematology & Oncology', 
                                        'Nephrology', 
                                        'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology'])].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending = False).to_frame().rename(columns={'patient_count': '% of total patient_count'}).iloc[2:20]
    
    






# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/396:
(((refto_from[(refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 
                                        'Hematology & Oncology', 
                                        'Nephrology', 
                                        'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology'])].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
   .sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending = False).to_frame().rename(columns={'patient_count': '% of total patient_count'}).iloc[2:20]
    
    






# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/397:
(((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame()..iloc(50:100)
    
    



['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/398:
(((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
    



['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/399:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
)   

((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_facility'])['patient_count'].sum())
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/400:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_facility'])['patient_count'].sum())
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/401:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_facility'])['patient_count'].sum())//(refto_from['patient_count'].sum()))*100)
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/402:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_facility'])['patient_count'].sum())//(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/403:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_specilaity', 'ref_to_facility'])['patient_count'].sum())//(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/404:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_facility'])['patient_count'].sum())//(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/405:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality'])['patient_count'].sum())//(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/406:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/407:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/408:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[1:20]
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/409:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[1:20]
)
210/410:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:20]
)
210/411:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:20]
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/412:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:20]..rename(columns={'patient_count': '% of total patient_count'})
 .plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
)
210/413:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:20]
 .rename(columns={'patient_count': '% of total patient_count'})
 .plot(kind="bar", 
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
)
210/414:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:20]
 .rename(columns={'patient_count': '% of total patient_count'})
 .plot(kind="bar")
# title = '20 least Referred speciality from cluste 83 into VUMC', xlabel='Referred speciality in VUMC', ylabel='% of total patient number in cbsa 34980')
)
210/415:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:20]
 .rename(columns={'patient_count': '% of total patient_count'})
 .plot(kind="bar",
       title = 'Most Referred specialities apart from Diagnostic Radiology and Anatomic Pathology & Clinical Pathology',
       xlabel='SLECIALITY', 
       ylabel='% of total patient count in cbsa 34980')
)
210/416:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:20]
 .plot(kind="bar")
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/417:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:10]
 .plot(kind="bar")
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/418:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:10]
 
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/419:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:10]
 
)
210/420:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:20]
 
)
210/421:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 
)
210/422:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_from_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 
)
210/423:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/424:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_from_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/425:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:10]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/426:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_from_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/427:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:20]
 
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/428:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:15]
  .rename(columns={'patient_count': '% of total patient_count'})
 
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/429:
# (
# (((refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_group', 'ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).to_frame().iloc(50:100)
    
  

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:20]
  .rename(columns={'patient_count': '% of total patient_count'})
 
)

# ['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']


# (((refto_from['ref_to_specilaity'].isin['Cardiovascular Disease', 'Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']).groupby(['ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False).to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[0:20]
#     )
210/430:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_from_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:10]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/431: (((refto_from[refto_from['ref_from_commid']== 83].groupby(['ref_from_speciality'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().head(40)
210/432:
refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/433:
refto_from.groupby['']

refto_from.columns
210/434:
# refto_from.groupby['']

refto_from.columns()
210/435:
# refto_from.groupby['']

refto_from.columns
210/436:
refto_from.groupby[' 'ref_to_commid', 'ref_from_commid']['patient_count'].sum()

# refto_from.columns
210/437:
refto_from.groupby['ref_to_commid', 'ref_from_commid']['patient_count'].sum()

# refto_from.columns
210/438:
refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum()

# refto_from.columns
210/439:
refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum().to_frame()

# refto_from.columns
210/440:
refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum().to_frame().sort_values(ascending=False)

# refto_from.columns
210/441:
refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum().sort_values(ascending=False).to_frame()

# refto_from.columns
210/442:
comm = refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum().sort_values(ascending=False).to_frame()
comm.to_csv('data/comm.csv')
# refto_from.columns
210/443:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby(['ref_to_speciality', 'ref_from_facility'])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/444:
comm = refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum().sort_values(ascending=False).to_frame()
comm.to_csv('data/comm.csv')

comm.head(20)
# refto_from.columns
210/445:
comm = (refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum()/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame()
comm.to_csv('data/comm.csv')

comm.head(20)
# refto_from.columns
210/446:
comm = ((refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum()/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame()
comm.to_csv('data/comm.csv')

# comm.head(20)
# refto_from.columns
210/447:
comm = ((refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum()/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame()
comm.to_csv('data/comm.csv')

# comm.head(20)
# refto_from.columns
210/448:
comm = ((refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum()/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame()
comm.to_csv('data/comm.csv')

comm.head(20)
# refto_from.columns
210/449:
comm = ((refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum()/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().rename(columns={'patient_count':'%patient_count'})
# comm.to_csv('data/comm.csv')

comm.head(20)
# refto_from.columns
210/450:
comm = ((refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum()/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().rename(columns={'patient_count':'%patient_count'})
comm.to_csv('data/comm.csv')

comm.head(20)
# refto_from.columns
210/451:
comm = ((refto_from.groupby(['ref_to_commid', 'ref_from_commid'])['patient_count'].sum()/(refto_from['patient_count'].sum()))*100).sort_values(ascending=False).to_frame().rename(columns={'patient_count':'per_patient_count'})
comm.to_csv('data/comm.csv')

comm.head(20)
# refto_from.columns
210/452:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby([ 'ref_from_facility','ref_to_speciality'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/453:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/454:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)

((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/455:
((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)

# ((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
#   .sort_values(ascending = False).to_frame()
#  .iloc[0:40]
#  .rename(columns={'patient_count': '% of total patient_count'})
# )
210/456:
(
(((refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])) & (refto_from["ref_to_specilaity"].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))]
    .groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility' ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)

# ((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
#   .sort_values(ascending = False).to_frame()
#  .iloc[0:40]
#  .rename(columns={'patient_count': '% of total patient_count'})
# )

# refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
#        'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
#        'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
#        "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
#        'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
#        'VANDERBILT HEART AND VASCULAR INSTITUTE',
#        'VANDERBILT ORTHOPAEDIC INSTITUTE',
#        'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
#        'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
#        'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
#        "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
#        'VANDERBILT UNIVERSITY HOSPITAL',
#        'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
#        "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
#        'VANDERBILT PSYCHIATRIC HOSPITAL',
#        'VANDERBILT ST. THOMAS IMAGING, GP',
#        'VANDERBILT HEALTH PHARMACY GROUP LLC',
#        'VANDERBILT HOME CARE SERVICES, LLC',
#        'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
#        'VANDERBILT UNIV. MEDICAL CENTER',
#        'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
#        'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
#        'VANDERBILT COMMUNITY AND HOME SERVICES',
#        'VANDERBILT BEDFORD HOSPITAL, LLC',
#        'VANDERBILT COFFEE HOSPITAL, LLC',
#        'VANDERBILT INTEGRATED PROVIDERS, LLC',
#        'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
#        'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
#        'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/457:
(
(((refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])) & (refto_from["ref_to_speciality"].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))]
    .groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility' ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)

# ((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
#   .sort_values(ascending = False).to_frame()
#  .iloc[0:40]
#  .rename(columns={'patient_count': '% of total patient_count'})
# )

# refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
#        'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
#        'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
#        "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
#        'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
#        'VANDERBILT HEART AND VASCULAR INSTITUTE',
#        'VANDERBILT ORTHOPAEDIC INSTITUTE',
#        'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
#        'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
#        'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
#        "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
#        'VANDERBILT UNIVERSITY HOSPITAL',
#        'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
#        "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
#        'VANDERBILT PSYCHIATRIC HOSPITAL',
#        'VANDERBILT ST. THOMAS IMAGING, GP',
#        'VANDERBILT HEALTH PHARMACY GROUP LLC',
#        'VANDERBILT HOME CARE SERVICES, LLC',
#        'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
#        'VANDERBILT UNIV. MEDICAL CENTER',
#        'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
#        'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
#        'VANDERBILT COMMUNITY AND HOME SERVICES',
#        'VANDERBILT BEDFORD HOSPITAL, LLC',
#        'VANDERBILT COFFEE HOSPITAL, LLC',
#        'VANDERBILT INTEGRATED PROVIDERS, LLC',
#        'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
#        'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
#        'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/458:
(
(((refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])) & (refto_from["ref_to_speciality"].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))]
    .groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility' ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
#  .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)

# ((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
#   .sort_values(ascending = False).to_frame()
#  .iloc[0:40]
#  .rename(columns={'patient_count': '% of total patient_count'})
# )

# refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
#        'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
#        'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
#        "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
#        'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
#        'VANDERBILT HEART AND VASCULAR INSTITUTE',
#        'VANDERBILT ORTHOPAEDIC INSTITUTE',
#        'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
#        'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
#        'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
#        "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
#        'VANDERBILT UNIVERSITY HOSPITAL',
#        'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
#        "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
#        'VANDERBILT PSYCHIATRIC HOSPITAL',
#        'VANDERBILT ST. THOMAS IMAGING, GP',
#        'VANDERBILT HEALTH PHARMACY GROUP LLC',
#        'VANDERBILT HOME CARE SERVICES, LLC',
#        'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
#        'VANDERBILT UNIV. MEDICAL CENTER',
#        'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
#        'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
#        'VANDERBILT COMMUNITY AND HOME SERVICES',
#        'VANDERBILT BEDFORD HOSPITAL, LLC',
#        'VANDERBILT COFFEE HOSPITAL, LLC',
#        'VANDERBILT INTEGRATED PROVIDERS, LLC',
#        'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
#        'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
#        'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/459:
(
(((refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])) & (refto_from["ref_to_speciality"].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))]
    .groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility' ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)

# ((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
#   .sort_values(ascending = False).to_frame()
#  .iloc[0:40]
#  .rename(columns={'patient_count': '% of total patient_count'})
# )

# refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
#        'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
#        'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
#        "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
#        'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
#        'VANDERBILT HEART AND VASCULAR INSTITUTE',
#        'VANDERBILT ORTHOPAEDIC INSTITUTE',
#        'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
#        'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
#        'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
#        "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
#        'VANDERBILT UNIVERSITY HOSPITAL',
#        'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
#        "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
#        'VANDERBILT PSYCHIATRIC HOSPITAL',
#        'VANDERBILT ST. THOMAS IMAGING, GP',
#        'VANDERBILT HEALTH PHARMACY GROUP LLC',
#        'VANDERBILT HOME CARE SERVICES, LLC',
#        'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
#        'VANDERBILT UNIV. MEDICAL CENTER',
#        'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
#        'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
#        'VANDERBILT COMMUNITY AND HOME SERVICES',
#        'VANDERBILT BEDFORD HOSPITAL, LLC',
#        'VANDERBILT COFFEE HOSPITAL, LLC',
#        'VANDERBILT INTEGRATED PROVIDERS, LLC',
#        'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
#        'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
#        'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/460:
(
(((refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])) & (refto_from["ref_to_speciality"].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))]
    .groupby([ 'ref_to_speciality', 'ref_to_facility' ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending = False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)

# ((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
#   .sort_values(ascending = False).to_frame()
#  .iloc[0:40]
#  .rename(columns={'patient_count': '% of total patient_count'})
# )

# refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
#        'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
#        'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
#        "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
#        'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
#        'VANDERBILT HEART AND VASCULAR INSTITUTE',
#        'VANDERBILT ORTHOPAEDIC INSTITUTE',
#        'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
#        'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
#        'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
#        "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
#        'VANDERBILT UNIVERSITY HOSPITAL',
#        'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
#        "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
#        'VANDERBILT PSYCHIATRIC HOSPITAL',
#        'VANDERBILT ST. THOMAS IMAGING, GP',
#        'VANDERBILT HEALTH PHARMACY GROUP LLC',
#        'VANDERBILT HOME CARE SERVICES, LLC',
#        'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
#        'VANDERBILT UNIV. MEDICAL CENTER',
#        'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
#        'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
#        'VANDERBILT COMMUNITY AND HOME SERVICES',
#        'VANDERBILT BEDFORD HOSPITAL, LLC',
#        'VANDERBILT COFFEE HOSPITAL, LLC',
#        'VANDERBILT INTEGRATED PROVIDERS, LLC',
#        'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
#        'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
#        'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/461:
(
(((refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
       'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
       'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
       "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
       'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
       'VANDERBILT HEART AND VASCULAR INSTITUTE',
       'VANDERBILT ORTHOPAEDIC INSTITUTE',
       'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
       'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
       'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
       "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
       'VANDERBILT UNIVERSITY HOSPITAL',
       'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
       "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
       'VANDERBILT PSYCHIATRIC HOSPITAL',
       'VANDERBILT ST. THOMAS IMAGING, GP',
       'VANDERBILT HEALTH PHARMACY GROUP LLC',
       'VANDERBILT HOME CARE SERVICES, LLC',
       'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
       'VANDERBILT UNIV. MEDICAL CENTER',
       'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
       'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
       'VANDERBILT COMMUNITY AND HOME SERVICES',
       'VANDERBILT BEDFORD HOSPITAL, LLC',
       'VANDERBILT COFFEE HOSPITAL, LLC',
       'VANDERBILT INTEGRATED PROVIDERS, LLC',
       'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
       'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
       'VANDERBILT MAURY RADIATION ONCOLOGY, LLC'])) & (refto_from["ref_to_speciality"].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))]
    .groupby([ 'ref_to_speciality', 'ref_to_facility' ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)

# ((((refto_from[(refto_from['ref_to_speciality'].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
#  'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))].groupby([ 'ref_from_facility','ref_to_speciality', 'ref_to_facility'  ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
#   .sort_values(ascending = False).to_frame()
#  .iloc[0:40]
#  .rename(columns={'patient_count': '% of total patient_count'})
# )

# refto_from[(refto_from['ref_from_facility'].isin(["VANDERBILT CHILDREN'S", 'VANDERBILT CHILDRENS HOSPITAL',
#        'VANDERBILT MEDICAL CENTER', 'VANDERBILT UNIVERSITY',
#        'VANDERBILT UNIVERSITY MEDICAL CENTER', 'CONCENTRA VANDERBILT LLC',
#        "MONROE CARELL JR VANDERBILT CHILDREN'S HOSPITAL",
#        'VANDERBILT PSYCHIATRIC HOSPITAL, LLC',
#        'VANDERBILT HEART AND VASCULAR INSTITUTE',
#        'VANDERBILT ORTHOPAEDIC INSTITUTE',
#        'DOUGLAS L. VANDERBILT, M.D. ,P.C.',
#        'VANDERBILT ASTHMA SINUS AND ALLERGY PROGRAM, LLC',
#        'VANDERBILT HOSPITAL', 'VANDERBILT IMAGING SERVICES, LLC',
#        "MONROE CARELL JR. CHILDREN'S HOSPITAL AT VANDERBILT",
#        'VANDERBILT UNIVERSITY HOSPITAL',
#        'VANDERBILT ORTHOPAEDICS AND REHABILITATION COOL SPRINGS',
#        "VANDERBILT CHILDREN'S HOSPITAL PHARMACY",
#        'VANDERBILT PSYCHIATRIC HOSPITAL',
#        'VANDERBILT ST. THOMAS IMAGING, GP',
#        'VANDERBILT HEALTH PHARMACY GROUP LLC',
#        'VANDERBILT HOME CARE SERVICES, LLC',
#        'VANDERBILT UNIVERSITY MEDICAL SCHOOL',
#        'VANDERBILT UNIV. MEDICAL CENTER',
#        'VANDERBILT ASTHMA SINUS ALLERGY PROGRAM',
#        'VANDERBILT COMMUNITY MENTAL HEALTH CENTER',
#        'VANDERBILT COMMUNITY AND HOME SERVICES',
#        'VANDERBILT BEDFORD HOSPITAL, LLC',
#        'VANDERBILT COFFEE HOSPITAL, LLC',
#        'VANDERBILT INTEGRATED PROVIDERS, LLC',
#        'VANDERBILT HEALTH AND WILLIAMSON MEDICAL CENTER CLINICS AND SERVICES,',
#        'VANDERBILT-INGRAM CANCER CENTER AT TENNOVA HEALTHCARE-CLARKSVILLE',
#        'VANDERBILT MAURY RADIATION ONCOLOGY, LLC']))].groupby(['ref_to_commid'])['patient_count'].sum().sort_values(ascending =False).to_frame()
210/462:
(
(((refto_from[(refto_from['ref_from_facility'].isin(['DICKSON MEDICAL ASSOCIATES PC',
       'HCA HEALTH SERVICES OF TENNESSEE, INC.', 'RADIOLOGY ALLIANCE PC',
       'MEDICAL NECESSITIES & SERVICES LLC',
       'ALLEGRY & ENT ASSOCIATES OF MIDDLE TENNESSEE, P.C.',
       'MIDDLE TENNESSEE PULMONARY ASSOCIA', 'CENTENNIAL HEART LLC',
       'ASSOCIATES IN GASTRONETEROLOGY', nan,
       'LONGS PARK EMERGENCY PHYSICIANS PLLC',
       'COLUMBIA MEDICAL GROUP - THE FRIST CLINIC INC',
       'ANESTHESIA MEDICAL GROUP, PC', 'MARATHON INPATIENT SERVICES PLLC',
       'THE SURGICAL CLINIC PLLC', 'NEPHROLOGY ASSOCIATES, PC',
       'NASHVILLE ANESTHESIA PLLC', 'PATHOLOGISTS LABORATORY INC',
       'TWO RIVERS EMERGENCY PHYSICIANS PLLC',
       'CENTENNIAL HOSPITALISTS, LLC',
       'TRISTAR JOINT REPLACEMENT INSTITUTE, LLC',
       'HENDERSONVILLE HOSPITAL CORPORATION',
       'NORTHRIDGE SURGERY CENTER, LP', 'SUMMIT SURGERY CENTER LP',
       'CENTENNIAL SURGERY CENTER LP',
       'CATARACT AND EYECARE CENTER,PROFESSIONAL LLC',
       'CENTRAL TENNESSEE HOSPITAL CORPORATION',
       'DICKSON MEDICAL ASSOCIATES, PC',
       'MATHIS DRIVE INPATIENT SERVICES PLLC',
       'LUTHER LAKE EMERGENCY PHYSICIANS, PLLC',
       'TRISTAR CARDIOVASCULAR SURGERY LLC',
       'HERMITAGE INPATIENT SERVICES PLLC',
       'HTI MEMORIAL HOSPITAL CORPORATION',
       'MIDDLE TENNESSEE HOSPITALIST, PLC',
       'DOVERSIDE EMERGENCY PHYSICIANS PLLC',
       'CENTENNIAL PSYCHIATRIC ASSOCIATES, LLC', 'COUNTY OF CHEATHAM',
       'GASTROENTEROLOGY & HEPATOLOGY ASSOCIATES PLLC',
       'GINA MENDOZA DPM PC', 'DIALYSIS ASSOCIATES, LLC',
       'TENNESSEE ONCOLOGY PLLC', 'MEDICAL GROUP SUMMIT INC',
       'OLD HICKORY LANE EMERGENCY PHYSICIANS PLLC',
       'ROBERTSON COUNTY GOVERNMENT FINANCE OFFICE',
       'DICKSON COUNTY OFFICE OF COUNTY MAYOR',
       'BRADLEYS HOME HEALTH CARE CENTER INC', 'DIALYSIS CLINIC INC.',
       'PERRIGIN MEDICAL PLLC', 'INTERVENTIONAL PAIN CENTER, PLLC',
       'NHC HEALTHCARE-SUMNER LLC', 'MEADOWLARK INPATIENT SERVICES PLLC',
       'SKYLINE MEDICAL GROUP LLC', 'AMEDISYS SP-TN, LLC',
       'TENNESSEE PHYSICIANS ALLIANCE PC',
       'NEWBERRY EMERGENCY PHYSICIANS PLLC', 'TRISTAR FAMILY CARE, LLC',
       'NEUROSURGICAL ASSOCIATES',
       'HOSPITAL BASED MEDICAL SERVICES OF TENNESSEE-I PC',
       'NASHVILLE ACUTE TRAUMA LLC', 'NASHVILLE FAMILY FOOT CARE, PLLC',
       'HERMITAGE PSYCHIATRIC GROUP', 'OXYGEN AND SLEEP ASSOCIATES, INC.',
       'OLYMPIC CONSULTING, LLC', 'SOUTHERN MEDICAL',
       'NHC HEALTHCARE-DICKSON LLC', 'FRED NORDQUIST, MD, PC',
       'ADVANCED MEDICAL SOLUTIONS, INC.',
       'TRISTAR GYNECOLOGY ONCOLOGY, LLC', 'DANIEL ADKISSON',
       'SKYLINE NEUROSCIENCE ASSOCIATES, LLC',
       'PORTLAND PRIMARY CARE LLC', 'DRS. REED & WILKERSON',
       'TRISTAR MEDICAL GROUP - LEGACY HEALTH, LLC',
       'TRISTAR BONE MARROW TRANSPLANT, LLC',
       'MEDICONE MEDICAL RESPONSE OF MIDDLE TENNESSEE INC',
       'NASHVILLE VASCULAR AND VEIN INSTITUTE, PLLC',
       'GASTROENTEROLOGY ASSOCIATES AT THE SUMMIT, P.C.',
       'SASH HEALTHCARE, PLC',
       'NASHVILLE PLASTIC SURGERY INSTITUTE, PLLC',
       'CENTENNIAL NEUROSCIENCE, LLC', 'NASHVILLE BONE AND JOINT PLLC',
       'RELIANCE MEDICAL ASSOCIATES PLLC',
       'CENTENNIAL SURGICAL ASSOCIATES LLC',
       'METROPOLITAN GOVERNMENT OF NASHVILLE AND DAVIDSON COUNTY',
       'ORAL SURGICAL INSTITUTE INC', 'AMERICAN HOMEPATIENT, INC.',
       'TRISTAR RADIATION ONCOLOGY, LLC', 'APRIA HEALTHCARE LLC',
       'SUMMIT PRIMARY CARE', 'SPIROCARE DME LLC', 'LINCARE INC.',
       'HENDERSONVILLE SURGEONS', 'CENTENNIAL SURGICAL CLINIC, LLC',
       'FIRST CALL AMBULANCE LLC', 'INTEGUMETRIX',
       'DICKSON ORTHOPAEDIC AND SPORTS MEDICINE PLLC',
       'FIRST CALL AMBULANCE SERVICE, LLC',
       'SUNCREST HEALTHCARE OF MIDDLE TN, LLC',
       'LEAVITT FAMILY MEDICINE, PLLC',
       'NORTHCREST PHYSICIAN SERVICES INC', 'NORTHCREST MEDICAL CENTER',
       'STERLING PRIMARY CARE ASSOCIATES LLC',
       'TENNESSEE COMPREHENSIVE LUNG AND SLEEP CENTER,PC',
       'COLUMBIA MEDICAL GROUP-SOUTHERN HILLS INC',
       'MEDICAL GROUP-STONECREST INC',
       'MID TENNESSEE NEUROLOGY ASSOCIATES,PLC',
       'MENDOZA FOOT & ANKLE CENTER PC',
       'DAVIS FOOT AND ANKLE CENTERS, INC',
       'N MIDDLE TN EMERGENCY PHYSICIANS PLLC',
       'MONADNOCK EMERGENCY PHYSICIANS PLLC',
       'MEDICINE BOW INPATIENT SERVICES PLLC',
       'PEAKVIEW EMERGENCY PHYSICIANS PLLC',
       'GERIATRIC CONSULTING GROUP PC', 'OURAY INPATIENT SERVICES PLLC',
       'PHYSICIAN SERVICES OF MIDDLE TENNESSEE, LLC',
       'MCKENDREE VILLAGE, INC.',
       'INNOVATIVE SENIOR CARE HOME HEALTH OF NASHVILLE LLC',
       'NASHVILLE MEDICAL INVESTORS LLC',
       'WEST WILSON FAMILY PRACTICE CENTER, P.C.',
       'ASHOK K. MEHTA, MD, PC', 'DVA RENAL HEALTHCARE INC',
       'CREATIVE HEALTHCARE, PLLC', 'NASHVILLE SENIOR CARE LLC',
       'PINEWOOD MEDICAL LLC', 'NHC-OP LP', 'DICKSON OPERATOR LLC',
       'HILLCREST HEALTHCARE, LLC',
       'CHRISTIAN CARE CENTER OF CHEATHAM COUNTY INC',
       'GENTIVA CERTIFIED HEALTHCARE CORP.',
       'MADISON BEHAVIORAL HEALTH, LLC', 'MADISON ADULT MEDICINE, INC.',
       'VANCO HEALTH CARE AND REHABILITATION LLC',
       'TCMC MADISON-PORTLAND, INC.', 'LEMMON EMERGENCY PHYSICIANS PLLC',
       'HIGHLAND PARK MEDICAL INVESTORS, LLC', 'MARY A. MCELANEY, M.D.',
       'SOUTHERN HILLS NEUROLOGY CONSULTANTS LLC',
       'TENNESSEE FOOT & ANKLE SPECIALISTS PC',
       'JOHAN VAN JAARSVELD MD PLLC',
       'INTERNAL MEDICINE ASSOCIATES OF SOUTHERN HILLS LLC',
       'HOMETOWN RESPIRATORY CONSULTANTS INC',
       'GETHSEMANE CARDIOVASCULAR CLINIC PLLC',
       'GASTROENTEROLOGY SPECIALISTS OF MIDDLE TENNESSEE LLC',
       'AMEDISYS TENNESSEE, LLC', 'ROBERT C. RIPLEY, M.D., P.C.',
       'MEDICAL GROUP - SOUTHERN HILLS OF BRENTWOOD LLC',
       'CARDIOVASCULAR INSTITUTE PC', 'MIDDLE TENNESSEE NEUROLOGY LLC',
       'SPRINGFIELD SURGERY,P.C.',
       'SURGICAL ALLIANCE OF MIDDLE TENNESSEE, PLC',
       'NHC HEALTHCARE HENDERSONVILLE LLC',
       'NHC HEALTHCARE-HENDERSONVILLE LLC',
       'LHC HOMECARE OF TENNESSEE, LLC', 'CARIS HEALTHCARE, LP',
       'DICKSON EAR, NOSE & THROAT PLC',
       'CLINICAL AND FORENSIC PSYCHOLOGY PROFESSIONAL LLC',
       'REGENTS MEDICAL CENTER PC', 'STEEL FAMILY MEDICINE',
       'ANGELA WILLIS FAMILY PRACTICE PLLC', 'ALIVE HOSPICE, INC',
       'HOME HEALTH CARE OF MIDDLE TENNESSEE, LLC',
       'EAR NOSE & THROAT SPECIALISTS OF NASHVILLE PLC',
       'POST-ACUTE PHYSICIANS OF TENNESSEE PLLC',
       'RAPHA FAMILY WELLNESS PLLC', 'WAL-MART STORES EAST LP',
       'SOUTH SIDE DRUG CO INC', 'HILLCREST HEALTHCARE LLC',
       'THE WATERS OF CHEATHAM LLC', 'EMPSON DRUG CO INC',
       'LAKESHORE ESTATES, INC.', 'LIFELINC ANESTHESIA II, PLLC',
       'STERLING PRIMARY CARE', 'COMMUNITY PHARMACY CARE, INC',
       'DICKSON OPTICAL PC', 'KROGER LIMITED PARTNERSHIP I',
       'TENNESSEE CVS PHARMACY, L.L.C.',
       'SELECT PHYSICAL THERAPY HOLDINGS INC',
       "CHARLOTTE'S HOMETOWN PHARMACY", 'CDK LLC',
       'DICKSON ORTHOTICS & PROSTHETICS', 'FAMILY HEALTH, LLC',
       'WALGREEN CO', 'NASHVILLE LUNG CENTER, INC',
       'DRS. TIDWELL, FAULKS, AND ALLEN - OPTOMETRY, PLLC',
       'LP NORTH NASHVILLE, LLC', 'NHC HEALTHCARE-SPRINGFIELD LLC',
       'CURAHEALTH NASHVILLE, LLC', 'XUHAN PC LLP',
       'CRESCENT MEDICAL GROUP PLLC', 'NEURO RESOURCE PC',
       'PRAMOD B. WASUDEV, M.D., PLLC',
       'THE HEART AND VASCULAR CLINIC P.C.',
       'ROBERTSON COUNTY PHYSICAL MEDICINE LLC', 'STEWART MENTAL HEALTH',
       'SPRINGFIELD NEUROLOGY', 'JOHN C WESTERKAMM MD PLLC',
       'LIFELINE HOME HEALTH CARE OF SPRINGFIELD, LLC',
       'THE WATERS OF SPRINGFIELD LLC', 'THE WATERS OF ROBERTSON LLC',
       'CHRISTIAN CARE CENTER OF SPRINGFIELD LLC',
       'WILLIAM J. BINKLEY, M.D. LLC', 'STEVEN G SCHOEMER PC',
       'ASERACARE HOSPICE - NEW HORIZONS, LLC',
       'PARKS PRACTITIONER AND CONSULTING SERVICES LLC',
       'ADORATION HOSPICE, LLC', 'VALLEY MEDICAL CENTER, PLLC',
       'CONTINUOUS CARE SVC LLC', 'PUBLIX TENNESSEE LLC', 'WALGREEN CO.',
       'RAMA MEDICAL GROUP', 'ORTHOPAEDIC SPECIALISTS PLLC',
       'ALIVE HOSPICE, INC.', 'LUNG CONSULTANTS PLLC',
       'THE SURGICAL CLINIC, PLLC',
       'BLUEGRASS INTERNAL MEDICINE ASSOCIATES, PC', 'VICTOR KHA DO PLLC',
       'TENNRX, LLC', 'RIVERGATE PSYCHIATRIC & BEHAVIORAL HEALTH',
       'SLEEP PROFESSIONALS, LLC', 'HENDERSONVILLE OBGYN LLC',
       'TOTAL RENAL CARE INC', 'AMSURG HERMITAGE ANESTHESIA, LLC',
       'HERMITAGE TN ENDOSCOPY ASC, LLC',
       'COLUMBIA MEDICAL GROUP-CENTENNIAL INC',
       'LABORATORY FOR KIDNEY PATHOLOGY INC', 'TOTAL RENAL CARE, INC',
       'STEVEN D GRAHAM MD, NEUROLOGY PC', 'CENTENNIAL WOMENS GROUP, LLC',
       'DONELSON OBSTETRICS & GYN', 'SOUTHERN WOMANS CARE',
       'COOL SPRINGS SURGICAL ASSOCIATES LLC', 'DAVID A WEST',
       'PREMIER ASC LLC', 'DAVID A GILPIN MD PLC',
       'GENESIS WOMENS CARE INC', 'MISTYE TAYLOR MD PLLC',
       'SOUTHERN PAIN INSTITUTE PLLC',
       'STATE OF TENNESSEE STATE F&A PAYROLL',
       'PANACEA PSYCHIATRIC CENTER PLC', 'AIR EVAC EMS INC',
       'TENNESSEE BREAST CARE CENTER PLC',
       'OMFS PHYSICIANS OF ATHENS, PLLC',
       'HEARING SERVICES OF NASHVILLE, LLC',
       'NHC HEALTHCARE SPRINGFIELD LLC', 'GGNSC SPRINGFIELD LLC',
       'DICKSON HEALTHCARE LLC',
       'FIVE STAR REHABILITATION AND WELLNESS SERVICES, LLC.',
       'HARMONY FAMILY HEALTH CARE, LLC', 'NP HOUSECALLS, PLLC',
       'ADAMSPLACE, LLC', 'TENNESSEE CVS PHARMACY LLC',
       'BECKWITH EMERGENCY PHYSICIAN PLLC'])) & (refto_from["ref_to_speciality"].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))]
    .groupby([ 'ref_to_speciality', 'ref_to_facility' ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/463:
(
(((refto_from[(refto_from['ref_from_facility'].isin(['DICKSON MEDICAL ASSOCIATES PC',
       'HCA HEALTH SERVICES OF TENNESSEE, INC.', 'RADIOLOGY ALLIANCE PC',
       'MEDICAL NECESSITIES & SERVICES LLC',
       'ALLEGRY & ENT ASSOCIATES OF MIDDLE TENNESSEE, P.C.',
       'MIDDLE TENNESSEE PULMONARY ASSOCIA', 'CENTENNIAL HEART LLC',
       'ASSOCIATES IN GASTRONETEROLOGY',
       'LONGS PARK EMERGENCY PHYSICIANS PLLC',
       'COLUMBIA MEDICAL GROUP - THE FRIST CLINIC INC',
       'ANESTHESIA MEDICAL GROUP, PC', 'MARATHON INPATIENT SERVICES PLLC',
       'THE SURGICAL CLINIC PLLC', 'NEPHROLOGY ASSOCIATES, PC',
       'NASHVILLE ANESTHESIA PLLC', 'PATHOLOGISTS LABORATORY INC',
       'TWO RIVERS EMERGENCY PHYSICIANS PLLC',
       'CENTENNIAL HOSPITALISTS, LLC',
       'TRISTAR JOINT REPLACEMENT INSTITUTE, LLC',
       'HENDERSONVILLE HOSPITAL CORPORATION',
       'NORTHRIDGE SURGERY CENTER, LP', 'SUMMIT SURGERY CENTER LP',
       'CENTENNIAL SURGERY CENTER LP',
       'CATARACT AND EYECARE CENTER,PROFESSIONAL LLC',
       'CENTRAL TENNESSEE HOSPITAL CORPORATION',
       'DICKSON MEDICAL ASSOCIATES, PC',
       'MATHIS DRIVE INPATIENT SERVICES PLLC',
       'LUTHER LAKE EMERGENCY PHYSICIANS, PLLC',
       'TRISTAR CARDIOVASCULAR SURGERY LLC',
       'HERMITAGE INPATIENT SERVICES PLLC',
       'HTI MEMORIAL HOSPITAL CORPORATION',
       'MIDDLE TENNESSEE HOSPITALIST, PLC',
       'DOVERSIDE EMERGENCY PHYSICIANS PLLC',
       'CENTENNIAL PSYCHIATRIC ASSOCIATES, LLC', 'COUNTY OF CHEATHAM',
       'GASTROENTEROLOGY & HEPATOLOGY ASSOCIATES PLLC',
       'GINA MENDOZA DPM PC', 'DIALYSIS ASSOCIATES, LLC',
       'TENNESSEE ONCOLOGY PLLC', 'MEDICAL GROUP SUMMIT INC',
       'OLD HICKORY LANE EMERGENCY PHYSICIANS PLLC',
       'ROBERTSON COUNTY GOVERNMENT FINANCE OFFICE',
       'DICKSON COUNTY OFFICE OF COUNTY MAYOR',
       'BRADLEYS HOME HEALTH CARE CENTER INC', 'DIALYSIS CLINIC INC.',
       'PERRIGIN MEDICAL PLLC', 'INTERVENTIONAL PAIN CENTER, PLLC',
       'NHC HEALTHCARE-SUMNER LLC', 'MEADOWLARK INPATIENT SERVICES PLLC',
       'SKYLINE MEDICAL GROUP LLC', 'AMEDISYS SP-TN, LLC',
       'TENNESSEE PHYSICIANS ALLIANCE PC',
       'NEWBERRY EMERGENCY PHYSICIANS PLLC', 'TRISTAR FAMILY CARE, LLC',
       'NEUROSURGICAL ASSOCIATES',
       'HOSPITAL BASED MEDICAL SERVICES OF TENNESSEE-I PC',
       'NASHVILLE ACUTE TRAUMA LLC', 'NASHVILLE FAMILY FOOT CARE, PLLC',
       'HERMITAGE PSYCHIATRIC GROUP', 'OXYGEN AND SLEEP ASSOCIATES, INC.',
       'OLYMPIC CONSULTING, LLC', 'SOUTHERN MEDICAL',
       'NHC HEALTHCARE-DICKSON LLC', 'FRED NORDQUIST, MD, PC',
       'ADVANCED MEDICAL SOLUTIONS, INC.',
       'TRISTAR GYNECOLOGY ONCOLOGY, LLC', 'DANIEL ADKISSON',
       'SKYLINE NEUROSCIENCE ASSOCIATES, LLC',
       'PORTLAND PRIMARY CARE LLC', 'DRS. REED & WILKERSON',
       'TRISTAR MEDICAL GROUP - LEGACY HEALTH, LLC',
       'TRISTAR BONE MARROW TRANSPLANT, LLC',
       'MEDICONE MEDICAL RESPONSE OF MIDDLE TENNESSEE INC',
       'NASHVILLE VASCULAR AND VEIN INSTITUTE, PLLC',
       'GASTROENTEROLOGY ASSOCIATES AT THE SUMMIT, P.C.',
       'SASH HEALTHCARE, PLC',
       'NASHVILLE PLASTIC SURGERY INSTITUTE, PLLC',
       'CENTENNIAL NEUROSCIENCE, LLC', 'NASHVILLE BONE AND JOINT PLLC',
       'RELIANCE MEDICAL ASSOCIATES PLLC',
       'CENTENNIAL SURGICAL ASSOCIATES LLC',
       'METROPOLITAN GOVERNMENT OF NASHVILLE AND DAVIDSON COUNTY',
       'ORAL SURGICAL INSTITUTE INC', 'AMERICAN HOMEPATIENT, INC.',
       'TRISTAR RADIATION ONCOLOGY, LLC', 'APRIA HEALTHCARE LLC',
       'SUMMIT PRIMARY CARE', 'SPIROCARE DME LLC', 'LINCARE INC.',
       'HENDERSONVILLE SURGEONS', 'CENTENNIAL SURGICAL CLINIC, LLC',
       'FIRST CALL AMBULANCE LLC', 'INTEGUMETRIX',
       'DICKSON ORTHOPAEDIC AND SPORTS MEDICINE PLLC',
       'FIRST CALL AMBULANCE SERVICE, LLC',
       'SUNCREST HEALTHCARE OF MIDDLE TN, LLC',
       'LEAVITT FAMILY MEDICINE, PLLC',
       'NORTHCREST PHYSICIAN SERVICES INC', 'NORTHCREST MEDICAL CENTER',
       'STERLING PRIMARY CARE ASSOCIATES LLC',
       'TENNESSEE COMPREHENSIVE LUNG AND SLEEP CENTER,PC',
       'COLUMBIA MEDICAL GROUP-SOUTHERN HILLS INC',
       'MEDICAL GROUP-STONECREST INC',
       'MID TENNESSEE NEUROLOGY ASSOCIATES,PLC',
       'MENDOZA FOOT & ANKLE CENTER PC',
       'DAVIS FOOT AND ANKLE CENTERS, INC',
       'N MIDDLE TN EMERGENCY PHYSICIANS PLLC',
       'MONADNOCK EMERGENCY PHYSICIANS PLLC',
       'MEDICINE BOW INPATIENT SERVICES PLLC',
       'PEAKVIEW EMERGENCY PHYSICIANS PLLC',
       'GERIATRIC CONSULTING GROUP PC', 'OURAY INPATIENT SERVICES PLLC',
       'PHYSICIAN SERVICES OF MIDDLE TENNESSEE, LLC',
       'MCKENDREE VILLAGE, INC.',
       'INNOVATIVE SENIOR CARE HOME HEALTH OF NASHVILLE LLC',
       'NASHVILLE MEDICAL INVESTORS LLC',
       'WEST WILSON FAMILY PRACTICE CENTER, P.C.',
       'ASHOK K. MEHTA, MD, PC', 'DVA RENAL HEALTHCARE INC',
       'CREATIVE HEALTHCARE, PLLC', 'NASHVILLE SENIOR CARE LLC',
       'PINEWOOD MEDICAL LLC', 'NHC-OP LP', 'DICKSON OPERATOR LLC',
       'HILLCREST HEALTHCARE, LLC',
       'CHRISTIAN CARE CENTER OF CHEATHAM COUNTY INC',
       'GENTIVA CERTIFIED HEALTHCARE CORP.',
       'MADISON BEHAVIORAL HEALTH, LLC', 'MADISON ADULT MEDICINE, INC.',
       'VANCO HEALTH CARE AND REHABILITATION LLC',
       'TCMC MADISON-PORTLAND, INC.', 'LEMMON EMERGENCY PHYSICIANS PLLC',
       'HIGHLAND PARK MEDICAL INVESTORS, LLC', 'MARY A. MCELANEY, M.D.',
       'SOUTHERN HILLS NEUROLOGY CONSULTANTS LLC',
       'TENNESSEE FOOT & ANKLE SPECIALISTS PC',
       'JOHAN VAN JAARSVELD MD PLLC',
       'INTERNAL MEDICINE ASSOCIATES OF SOUTHERN HILLS LLC',
       'HOMETOWN RESPIRATORY CONSULTANTS INC',
       'GETHSEMANE CARDIOVASCULAR CLINIC PLLC',
       'GASTROENTEROLOGY SPECIALISTS OF MIDDLE TENNESSEE LLC',
       'AMEDISYS TENNESSEE, LLC', 'ROBERT C. RIPLEY, M.D., P.C.',
       'MEDICAL GROUP - SOUTHERN HILLS OF BRENTWOOD LLC',
       'CARDIOVASCULAR INSTITUTE PC', 'MIDDLE TENNESSEE NEUROLOGY LLC',
       'SPRINGFIELD SURGERY,P.C.',
       'SURGICAL ALLIANCE OF MIDDLE TENNESSEE, PLC',
       'NHC HEALTHCARE HENDERSONVILLE LLC',
       'NHC HEALTHCARE-HENDERSONVILLE LLC',
       'LHC HOMECARE OF TENNESSEE, LLC', 'CARIS HEALTHCARE, LP',
       'DICKSON EAR, NOSE & THROAT PLC',
       'CLINICAL AND FORENSIC PSYCHOLOGY PROFESSIONAL LLC',
       'REGENTS MEDICAL CENTER PC', 'STEEL FAMILY MEDICINE',
       'ANGELA WILLIS FAMILY PRACTICE PLLC', 'ALIVE HOSPICE, INC',
       'HOME HEALTH CARE OF MIDDLE TENNESSEE, LLC',
       'EAR NOSE & THROAT SPECIALISTS OF NASHVILLE PLC',
       'POST-ACUTE PHYSICIANS OF TENNESSEE PLLC',
       'RAPHA FAMILY WELLNESS PLLC', 'WAL-MART STORES EAST LP',
       'SOUTH SIDE DRUG CO INC', 'HILLCREST HEALTHCARE LLC',
       'THE WATERS OF CHEATHAM LLC', 'EMPSON DRUG CO INC',
       'LAKESHORE ESTATES, INC.', 'LIFELINC ANESTHESIA II, PLLC',
       'STERLING PRIMARY CARE', 'COMMUNITY PHARMACY CARE, INC',
       'DICKSON OPTICAL PC', 'KROGER LIMITED PARTNERSHIP I',
       'TENNESSEE CVS PHARMACY, L.L.C.',
       'SELECT PHYSICAL THERAPY HOLDINGS INC',
       "CHARLOTTE'S HOMETOWN PHARMACY", 'CDK LLC',
       'DICKSON ORTHOTICS & PROSTHETICS', 'FAMILY HEALTH, LLC',
       'WALGREEN CO', 'NASHVILLE LUNG CENTER, INC',
       'DRS. TIDWELL, FAULKS, AND ALLEN - OPTOMETRY, PLLC',
       'LP NORTH NASHVILLE, LLC', 'NHC HEALTHCARE-SPRINGFIELD LLC',
       'CURAHEALTH NASHVILLE, LLC', 'XUHAN PC LLP',
       'CRESCENT MEDICAL GROUP PLLC', 'NEURO RESOURCE PC',
       'PRAMOD B. WASUDEV, M.D., PLLC',
       'THE HEART AND VASCULAR CLINIC P.C.',
       'ROBERTSON COUNTY PHYSICAL MEDICINE LLC', 'STEWART MENTAL HEALTH',
       'SPRINGFIELD NEUROLOGY', 'JOHN C WESTERKAMM MD PLLC',
       'LIFELINE HOME HEALTH CARE OF SPRINGFIELD, LLC',
       'THE WATERS OF SPRINGFIELD LLC', 'THE WATERS OF ROBERTSON LLC',
       'CHRISTIAN CARE CENTER OF SPRINGFIELD LLC',
       'WILLIAM J. BINKLEY, M.D. LLC', 'STEVEN G SCHOEMER PC',
       'ASERACARE HOSPICE - NEW HORIZONS, LLC',
       'PARKS PRACTITIONER AND CONSULTING SERVICES LLC',
       'ADORATION HOSPICE, LLC', 'VALLEY MEDICAL CENTER, PLLC',
       'CONTINUOUS CARE SVC LLC', 'PUBLIX TENNESSEE LLC', 'WALGREEN CO.',
       'RAMA MEDICAL GROUP', 'ORTHOPAEDIC SPECIALISTS PLLC',
       'ALIVE HOSPICE, INC.', 'LUNG CONSULTANTS PLLC',
       'THE SURGICAL CLINIC, PLLC',
       'BLUEGRASS INTERNAL MEDICINE ASSOCIATES, PC', 'VICTOR KHA DO PLLC',
       'TENNRX, LLC', 'RIVERGATE PSYCHIATRIC & BEHAVIORAL HEALTH',
       'SLEEP PROFESSIONALS, LLC', 'HENDERSONVILLE OBGYN LLC',
       'TOTAL RENAL CARE INC', 'AMSURG HERMITAGE ANESTHESIA, LLC',
       'HERMITAGE TN ENDOSCOPY ASC, LLC',
       'COLUMBIA MEDICAL GROUP-CENTENNIAL INC',
       'LABORATORY FOR KIDNEY PATHOLOGY INC', 'TOTAL RENAL CARE, INC',
       'STEVEN D GRAHAM MD, NEUROLOGY PC', 'CENTENNIAL WOMENS GROUP, LLC',
       'DONELSON OBSTETRICS & GYN', 'SOUTHERN WOMANS CARE',
       'COOL SPRINGS SURGICAL ASSOCIATES LLC', 'DAVID A WEST',
       'PREMIER ASC LLC', 'DAVID A GILPIN MD PLC',
       'GENESIS WOMENS CARE INC', 'MISTYE TAYLOR MD PLLC',
       'SOUTHERN PAIN INSTITUTE PLLC',
       'STATE OF TENNESSEE STATE F&A PAYROLL',
       'PANACEA PSYCHIATRIC CENTER PLC', 'AIR EVAC EMS INC',
       'TENNESSEE BREAST CARE CENTER PLC',
       'OMFS PHYSICIANS OF ATHENS, PLLC',
       'HEARING SERVICES OF NASHVILLE, LLC',
       'NHC HEALTHCARE SPRINGFIELD LLC', 'GGNSC SPRINGFIELD LLC',
       'DICKSON HEALTHCARE LLC',
       'FIVE STAR REHABILITATION AND WELLNESS SERVICES, LLC.',
       'HARMONY FAMILY HEALTH CARE, LLC', 'NP HOUSECALLS, PLLC',
       'ADAMSPLACE, LLC', 'TENNESSEE CVS PHARMACY LLC',
       'BECKWITH EMERGENCY PHYSICIAN PLLC'])) & (refto_from["ref_to_speciality"].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))]
    .groupby([ 'ref_to_speciality', 'ref_to_facility' ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/464:
(
(((refto_from[(refto_from['ref_from_facility'].isin(['DICKSON MEDICAL ASSOCIATES PC',
       'HCA HEALTH SERVICES OF TENNESSEE, INC.', 'RADIOLOGY ALLIANCE PC',
       'MEDICAL NECESSITIES & SERVICES LLC',
       'ALLEGRY & ENT ASSOCIATES OF MIDDLE TENNESSEE, P.C.',
       'MIDDLE TENNESSEE PULMONARY ASSOCIA', 'CENTENNIAL HEART LLC',
       'ASSOCIATES IN GASTRONETEROLOGY',
       'LONGS PARK EMERGENCY PHYSICIANS PLLC',
       'COLUMBIA MEDICAL GROUP - THE FRIST CLINIC INC',
       'ANESTHESIA MEDICAL GROUP, PC', 'MARATHON INPATIENT SERVICES PLLC',
       'THE SURGICAL CLINIC PLLC', 'NEPHROLOGY ASSOCIATES, PC',
       'NASHVILLE ANESTHESIA PLLC', 'PATHOLOGISTS LABORATORY INC',
       'TWO RIVERS EMERGENCY PHYSICIANS PLLC',
       'CENTENNIAL HOSPITALISTS, LLC',
       'TRISTAR JOINT REPLACEMENT INSTITUTE, LLC',
       'HENDERSONVILLE HOSPITAL CORPORATION',
       'NORTHRIDGE SURGERY CENTER, LP', 'SUMMIT SURGERY CENTER LP',
       'CENTENNIAL SURGERY CENTER LP',
       'CATARACT AND EYECARE CENTER,PROFESSIONAL LLC',
       'CENTRAL TENNESSEE HOSPITAL CORPORATION',
       'DICKSON MEDICAL ASSOCIATES, PC',
       'MATHIS DRIVE INPATIENT SERVICES PLLC',
       'LUTHER LAKE EMERGENCY PHYSICIANS, PLLC',
       'TRISTAR CARDIOVASCULAR SURGERY LLC',
       'HERMITAGE INPATIENT SERVICES PLLC',
       'HTI MEMORIAL HOSPITAL CORPORATION',
       'MIDDLE TENNESSEE HOSPITALIST, PLC',
       'DOVERSIDE EMERGENCY PHYSICIANS PLLC',
       'CENTENNIAL PSYCHIATRIC ASSOCIATES, LLC', 'COUNTY OF CHEATHAM',
       'GASTROENTEROLOGY & HEPATOLOGY ASSOCIATES PLLC',
       'GINA MENDOZA DPM PC', 'DIALYSIS ASSOCIATES, LLC',
       'TENNESSEE ONCOLOGY PLLC', 'MEDICAL GROUP SUMMIT INC',
       'OLD HICKORY LANE EMERGENCY PHYSICIANS PLLC',
       'ROBERTSON COUNTY GOVERNMENT FINANCE OFFICE',
       'DICKSON COUNTY OFFICE OF COUNTY MAYOR',
       'BRADLEYS HOME HEALTH CARE CENTER INC', 'DIALYSIS CLINIC INC.',
       'PERRIGIN MEDICAL PLLC', 'INTERVENTIONAL PAIN CENTER, PLLC',
       'NHC HEALTHCARE-SUMNER LLC', 'MEADOWLARK INPATIENT SERVICES PLLC',
       'SKYLINE MEDICAL GROUP LLC', 'AMEDISYS SP-TN, LLC',
       'TENNESSEE PHYSICIANS ALLIANCE PC',
       'NEWBERRY EMERGENCY PHYSICIANS PLLC', 'TRISTAR FAMILY CARE, LLC',
       'NEUROSURGICAL ASSOCIATES',
       'HOSPITAL BASED MEDICAL SERVICES OF TENNESSEE-I PC',
       'NASHVILLE ACUTE TRAUMA LLC', 'NASHVILLE FAMILY FOOT CARE, PLLC',
       'HERMITAGE PSYCHIATRIC GROUP', 'OXYGEN AND SLEEP ASSOCIATES, INC.',
       'OLYMPIC CONSULTING, LLC', 'SOUTHERN MEDICAL',
       'NHC HEALTHCARE-DICKSON LLC', 'FRED NORDQUIST, MD, PC',
       'ADVANCED MEDICAL SOLUTIONS, INC.',
       'TRISTAR GYNECOLOGY ONCOLOGY, LLC', 'DANIEL ADKISSON',
       'SKYLINE NEUROSCIENCE ASSOCIATES, LLC',
       'PORTLAND PRIMARY CARE LLC', 'DRS. REED & WILKERSON',
       'TRISTAR MEDICAL GROUP - LEGACY HEALTH, LLC',
       'TRISTAR BONE MARROW TRANSPLANT, LLC',
       'MEDICONE MEDICAL RESPONSE OF MIDDLE TENNESSEE INC',
       'NASHVILLE VASCULAR AND VEIN INSTITUTE, PLLC',
       'GASTROENTEROLOGY ASSOCIATES AT THE SUMMIT, P.C.',
       'SASH HEALTHCARE, PLC',
       'NASHVILLE PLASTIC SURGERY INSTITUTE, PLLC',
       'CENTENNIAL NEUROSCIENCE, LLC', 'NASHVILLE BONE AND JOINT PLLC',
       'RELIANCE MEDICAL ASSOCIATES PLLC',
       'CENTENNIAL SURGICAL ASSOCIATES LLC',
       'METROPOLITAN GOVERNMENT OF NASHVILLE AND DAVIDSON COUNTY',
       'ORAL SURGICAL INSTITUTE INC', 'AMERICAN HOMEPATIENT, INC.',
       'TRISTAR RADIATION ONCOLOGY, LLC', 'APRIA HEALTHCARE LLC',
       'SUMMIT PRIMARY CARE', 'SPIROCARE DME LLC', 'LINCARE INC.',
       'HENDERSONVILLE SURGEONS', 'CENTENNIAL SURGICAL CLINIC, LLC',
       'FIRST CALL AMBULANCE LLC', 'INTEGUMETRIX',
       'DICKSON ORTHOPAEDIC AND SPORTS MEDICINE PLLC',
       'FIRST CALL AMBULANCE SERVICE, LLC',
       'SUNCREST HEALTHCARE OF MIDDLE TN, LLC',
       'LEAVITT FAMILY MEDICINE, PLLC',
       'NORTHCREST PHYSICIAN SERVICES INC', 'NORTHCREST MEDICAL CENTER',
       'STERLING PRIMARY CARE ASSOCIATES LLC',
       'TENNESSEE COMPREHENSIVE LUNG AND SLEEP CENTER,PC',
       'COLUMBIA MEDICAL GROUP-SOUTHERN HILLS INC',
       'MEDICAL GROUP-STONECREST INC',
       'MID TENNESSEE NEUROLOGY ASSOCIATES,PLC',
       'MENDOZA FOOT & ANKLE CENTER PC',
       'DAVIS FOOT AND ANKLE CENTERS, INC',
       'N MIDDLE TN EMERGENCY PHYSICIANS PLLC',
       'MONADNOCK EMERGENCY PHYSICIANS PLLC',
       'MEDICINE BOW INPATIENT SERVICES PLLC',
       'PEAKVIEW EMERGENCY PHYSICIANS PLLC',
       'GERIATRIC CONSULTING GROUP PC', 'OURAY INPATIENT SERVICES PLLC',
       'PHYSICIAN SERVICES OF MIDDLE TENNESSEE, LLC',
       'MCKENDREE VILLAGE, INC.',
       'INNOVATIVE SENIOR CARE HOME HEALTH OF NASHVILLE LLC',
       'NASHVILLE MEDICAL INVESTORS LLC',
       'WEST WILSON FAMILY PRACTICE CENTER, P.C.',
       'ASHOK K. MEHTA, MD, PC', 'DVA RENAL HEALTHCARE INC',
       'CREATIVE HEALTHCARE, PLLC', 'NASHVILLE SENIOR CARE LLC',
       'PINEWOOD MEDICAL LLC', 'NHC-OP LP', 'DICKSON OPERATOR LLC',
       'HILLCREST HEALTHCARE, LLC',
       'CHRISTIAN CARE CENTER OF CHEATHAM COUNTY INC',
       'GENTIVA CERTIFIED HEALTHCARE CORP.',
       'MADISON BEHAVIORAL HEALTH, LLC', 'MADISON ADULT MEDICINE, INC.',
       'VANCO HEALTH CARE AND REHABILITATION LLC',
       'TCMC MADISON-PORTLAND, INC.', 'LEMMON EMERGENCY PHYSICIANS PLLC',
       'HIGHLAND PARK MEDICAL INVESTORS, LLC', 'MARY A. MCELANEY, M.D.',
       'SOUTHERN HILLS NEUROLOGY CONSULTANTS LLC',
       'TENNESSEE FOOT & ANKLE SPECIALISTS PC',
       'JOHAN VAN JAARSVELD MD PLLC',
       'INTERNAL MEDICINE ASSOCIATES OF SOUTHERN HILLS LLC',
       'HOMETOWN RESPIRATORY CONSULTANTS INC',
       'GETHSEMANE CARDIOVASCULAR CLINIC PLLC',
       'GASTROENTEROLOGY SPECIALISTS OF MIDDLE TENNESSEE LLC',
       'AMEDISYS TENNESSEE, LLC', 'ROBERT C. RIPLEY, M.D., P.C.',
       'MEDICAL GROUP - SOUTHERN HILLS OF BRENTWOOD LLC',
       'CARDIOVASCULAR INSTITUTE PC', 'MIDDLE TENNESSEE NEUROLOGY LLC',
       'SPRINGFIELD SURGERY,P.C.',
       'SURGICAL ALLIANCE OF MIDDLE TENNESSEE, PLC',
       'NHC HEALTHCARE HENDERSONVILLE LLC',
       'NHC HEALTHCARE-HENDERSONVILLE LLC',
       'LHC HOMECARE OF TENNESSEE, LLC', 'CARIS HEALTHCARE, LP',
       'DICKSON EAR, NOSE & THROAT PLC',
       'CLINICAL AND FORENSIC PSYCHOLOGY PROFESSIONAL LLC',
       'REGENTS MEDICAL CENTER PC', 'STEEL FAMILY MEDICINE',
       'ANGELA WILLIS FAMILY PRACTICE PLLC', 'ALIVE HOSPICE, INC',
       'HOME HEALTH CARE OF MIDDLE TENNESSEE, LLC',
       'EAR NOSE & THROAT SPECIALISTS OF NASHVILLE PLC',
       'POST-ACUTE PHYSICIANS OF TENNESSEE PLLC',
       'RAPHA FAMILY WELLNESS PLLC', 'WAL-MART STORES EAST LP',
       'SOUTH SIDE DRUG CO INC', 'HILLCREST HEALTHCARE LLC',
       'THE WATERS OF CHEATHAM LLC', 'EMPSON DRUG CO INC',
       'LAKESHORE ESTATES, INC.', 'LIFELINC ANESTHESIA II, PLLC',
       'STERLING PRIMARY CARE', 'COMMUNITY PHARMACY CARE, INC',
       'DICKSON OPTICAL PC', 'KROGER LIMITED PARTNERSHIP I',
       'TENNESSEE CVS PHARMACY, L.L.C.',
       'SELECT PHYSICAL THERAPY HOLDINGS INC',
       "CHARLOTTE'S HOMETOWN PHARMACY", 'CDK LLC',
       'DICKSON ORTHOTICS & PROSTHETICS', 'FAMILY HEALTH, LLC',
       'WALGREEN CO', 'NASHVILLE LUNG CENTER, INC',
       'DRS. TIDWELL, FAULKS, AND ALLEN - OPTOMETRY, PLLC',
       'LP NORTH NASHVILLE, LLC', 'NHC HEALTHCARE-SPRINGFIELD LLC',
       'CURAHEALTH NASHVILLE, LLC', 'XUHAN PC LLP',
       'CRESCENT MEDICAL GROUP PLLC', 'NEURO RESOURCE PC',
       'PRAMOD B. WASUDEV, M.D., PLLC',
       'THE HEART AND VASCULAR CLINIC P.C.',
       'ROBERTSON COUNTY PHYSICAL MEDICINE LLC', 'STEWART MENTAL HEALTH',
       'SPRINGFIELD NEUROLOGY', 'JOHN C WESTERKAMM MD PLLC',
       'LIFELINE HOME HEALTH CARE OF SPRINGFIELD, LLC',
       'THE WATERS OF SPRINGFIELD LLC', 'THE WATERS OF ROBERTSON LLC',
       'CHRISTIAN CARE CENTER OF SPRINGFIELD LLC',
       'WILLIAM J. BINKLEY, M.D. LLC', 'STEVEN G SCHOEMER PC',
       'ASERACARE HOSPICE - NEW HORIZONS, LLC',
       'PARKS PRACTITIONER AND CONSULTING SERVICES LLC',
       'ADORATION HOSPICE, LLC', 'VALLEY MEDICAL CENTER, PLLC',
       'CONTINUOUS CARE SVC LLC', 'PUBLIX TENNESSEE LLC', 'WALGREEN CO.',
       'RAMA MEDICAL GROUP', 'ORTHOPAEDIC SPECIALISTS PLLC',
       'ALIVE HOSPICE, INC.', 'LUNG CONSULTANTS PLLC',
       'THE SURGICAL CLINIC, PLLC',
       'BLUEGRASS INTERNAL MEDICINE ASSOCIATES, PC', 'VICTOR KHA DO PLLC',
       'TENNRX, LLC', 'RIVERGATE PSYCHIATRIC & BEHAVIORAL HEALTH',
       'SLEEP PROFESSIONALS, LLC', 'HENDERSONVILLE OBGYN LLC',
       'TOTAL RENAL CARE INC', 'AMSURG HERMITAGE ANESTHESIA, LLC',
       'HERMITAGE TN ENDOSCOPY ASC, LLC',
       'COLUMBIA MEDICAL GROUP-CENTENNIAL INC',
       'LABORATORY FOR KIDNEY PATHOLOGY INC', 'TOTAL RENAL CARE, INC',
       'STEVEN D GRAHAM MD, NEUROLOGY PC', 'CENTENNIAL WOMENS GROUP, LLC',
       'DONELSON OBSTETRICS & GYN', 'SOUTHERN WOMANS CARE',
       'COOL SPRINGS SURGICAL ASSOCIATES LLC', 'DAVID A WEST',
       'PREMIER ASC LLC', 'DAVID A GILPIN MD PLC',
       'GENESIS WOMENS CARE INC', 'MISTYE TAYLOR MD PLLC',
       'SOUTHERN PAIN INSTITUTE PLLC',
       'STATE OF TENNESSEE STATE F&A PAYROLL',
       'PANACEA PSYCHIATRIC CENTER PLC', 'AIR EVAC EMS INC',
       'TENNESSEE BREAST CARE CENTER PLC',
       'OMFS PHYSICIANS OF ATHENS, PLLC',
       'HEARING SERVICES OF NASHVILLE, LLC',
       'NHC HEALTHCARE SPRINGFIELD LLC', 'GGNSC SPRINGFIELD LLC',
       'DICKSON HEALTHCARE LLC',
       'FIVE STAR REHABILITATION AND WELLNESS SERVICES, LLC.',
       'HARMONY FAMILY HEALTH CARE, LLC', 'NP HOUSECALLS, PLLC',
       'ADAMSPLACE, LLC', 'TENNESSEE CVS PHARMACY LLC',
       'BECKWITH EMERGENCY PHYSICIAN PLLC'])) & (refto_from["ref_to_speciality"].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))]
    .groupby([ 'ref_to_speciality', 'ref_to_facility' ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_value(ascending=False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/465:
(
(((refto_from[(refto_from['ref_from_facility'].isin(['DICKSON MEDICAL ASSOCIATES PC',
       'HCA HEALTH SERVICES OF TENNESSEE, INC.', 'RADIOLOGY ALLIANCE PC',
       'MEDICAL NECESSITIES & SERVICES LLC',
       'ALLEGRY & ENT ASSOCIATES OF MIDDLE TENNESSEE, P.C.',
       'MIDDLE TENNESSEE PULMONARY ASSOCIA', 'CENTENNIAL HEART LLC',
       'ASSOCIATES IN GASTRONETEROLOGY',
       'LONGS PARK EMERGENCY PHYSICIANS PLLC',
       'COLUMBIA MEDICAL GROUP - THE FRIST CLINIC INC',
       'ANESTHESIA MEDICAL GROUP, PC', 'MARATHON INPATIENT SERVICES PLLC',
       'THE SURGICAL CLINIC PLLC', 'NEPHROLOGY ASSOCIATES, PC',
       'NASHVILLE ANESTHESIA PLLC', 'PATHOLOGISTS LABORATORY INC',
       'TWO RIVERS EMERGENCY PHYSICIANS PLLC',
       'CENTENNIAL HOSPITALISTS, LLC',
       'TRISTAR JOINT REPLACEMENT INSTITUTE, LLC',
       'HENDERSONVILLE HOSPITAL CORPORATION',
       'NORTHRIDGE SURGERY CENTER, LP', 'SUMMIT SURGERY CENTER LP',
       'CENTENNIAL SURGERY CENTER LP',
       'CATARACT AND EYECARE CENTER,PROFESSIONAL LLC',
       'CENTRAL TENNESSEE HOSPITAL CORPORATION',
       'DICKSON MEDICAL ASSOCIATES, PC',
       'MATHIS DRIVE INPATIENT SERVICES PLLC',
       'LUTHER LAKE EMERGENCY PHYSICIANS, PLLC',
       'TRISTAR CARDIOVASCULAR SURGERY LLC',
       'HERMITAGE INPATIENT SERVICES PLLC',
       'HTI MEMORIAL HOSPITAL CORPORATION',
       'MIDDLE TENNESSEE HOSPITALIST, PLC',
       'DOVERSIDE EMERGENCY PHYSICIANS PLLC',
       'CENTENNIAL PSYCHIATRIC ASSOCIATES, LLC', 'COUNTY OF CHEATHAM',
       'GASTROENTEROLOGY & HEPATOLOGY ASSOCIATES PLLC',
       'GINA MENDOZA DPM PC', 'DIALYSIS ASSOCIATES, LLC',
       'TENNESSEE ONCOLOGY PLLC', 'MEDICAL GROUP SUMMIT INC',
       'OLD HICKORY LANE EMERGENCY PHYSICIANS PLLC',
       'ROBERTSON COUNTY GOVERNMENT FINANCE OFFICE',
       'DICKSON COUNTY OFFICE OF COUNTY MAYOR',
       'BRADLEYS HOME HEALTH CARE CENTER INC', 'DIALYSIS CLINIC INC.',
       'PERRIGIN MEDICAL PLLC', 'INTERVENTIONAL PAIN CENTER, PLLC',
       'NHC HEALTHCARE-SUMNER LLC', 'MEADOWLARK INPATIENT SERVICES PLLC',
       'SKYLINE MEDICAL GROUP LLC', 'AMEDISYS SP-TN, LLC',
       'TENNESSEE PHYSICIANS ALLIANCE PC',
       'NEWBERRY EMERGENCY PHYSICIANS PLLC', 'TRISTAR FAMILY CARE, LLC',
       'NEUROSURGICAL ASSOCIATES',
       'HOSPITAL BASED MEDICAL SERVICES OF TENNESSEE-I PC',
       'NASHVILLE ACUTE TRAUMA LLC', 'NASHVILLE FAMILY FOOT CARE, PLLC',
       'HERMITAGE PSYCHIATRIC GROUP', 'OXYGEN AND SLEEP ASSOCIATES, INC.',
       'OLYMPIC CONSULTING, LLC', 'SOUTHERN MEDICAL',
       'NHC HEALTHCARE-DICKSON LLC', 'FRED NORDQUIST, MD, PC',
       'ADVANCED MEDICAL SOLUTIONS, INC.',
       'TRISTAR GYNECOLOGY ONCOLOGY, LLC', 'DANIEL ADKISSON',
       'SKYLINE NEUROSCIENCE ASSOCIATES, LLC',
       'PORTLAND PRIMARY CARE LLC', 'DRS. REED & WILKERSON',
       'TRISTAR MEDICAL GROUP - LEGACY HEALTH, LLC',
       'TRISTAR BONE MARROW TRANSPLANT, LLC',
       'MEDICONE MEDICAL RESPONSE OF MIDDLE TENNESSEE INC',
       'NASHVILLE VASCULAR AND VEIN INSTITUTE, PLLC',
       'GASTROENTEROLOGY ASSOCIATES AT THE SUMMIT, P.C.',
       'SASH HEALTHCARE, PLC',
       'NASHVILLE PLASTIC SURGERY INSTITUTE, PLLC',
       'CENTENNIAL NEUROSCIENCE, LLC', 'NASHVILLE BONE AND JOINT PLLC',
       'RELIANCE MEDICAL ASSOCIATES PLLC',
       'CENTENNIAL SURGICAL ASSOCIATES LLC',
       'METROPOLITAN GOVERNMENT OF NASHVILLE AND DAVIDSON COUNTY',
       'ORAL SURGICAL INSTITUTE INC', 'AMERICAN HOMEPATIENT, INC.',
       'TRISTAR RADIATION ONCOLOGY, LLC', 'APRIA HEALTHCARE LLC',
       'SUMMIT PRIMARY CARE', 'SPIROCARE DME LLC', 'LINCARE INC.',
       'HENDERSONVILLE SURGEONS', 'CENTENNIAL SURGICAL CLINIC, LLC',
       'FIRST CALL AMBULANCE LLC', 'INTEGUMETRIX',
       'DICKSON ORTHOPAEDIC AND SPORTS MEDICINE PLLC',
       'FIRST CALL AMBULANCE SERVICE, LLC',
       'SUNCREST HEALTHCARE OF MIDDLE TN, LLC',
       'LEAVITT FAMILY MEDICINE, PLLC',
       'NORTHCREST PHYSICIAN SERVICES INC', 'NORTHCREST MEDICAL CENTER',
       'STERLING PRIMARY CARE ASSOCIATES LLC',
       'TENNESSEE COMPREHENSIVE LUNG AND SLEEP CENTER,PC',
       'COLUMBIA MEDICAL GROUP-SOUTHERN HILLS INC',
       'MEDICAL GROUP-STONECREST INC',
       'MID TENNESSEE NEUROLOGY ASSOCIATES,PLC',
       'MENDOZA FOOT & ANKLE CENTER PC',
       'DAVIS FOOT AND ANKLE CENTERS, INC',
       'N MIDDLE TN EMERGENCY PHYSICIANS PLLC',
       'MONADNOCK EMERGENCY PHYSICIANS PLLC',
       'MEDICINE BOW INPATIENT SERVICES PLLC',
       'PEAKVIEW EMERGENCY PHYSICIANS PLLC',
       'GERIATRIC CONSULTING GROUP PC', 'OURAY INPATIENT SERVICES PLLC',
       'PHYSICIAN SERVICES OF MIDDLE TENNESSEE, LLC',
       'MCKENDREE VILLAGE, INC.',
       'INNOVATIVE SENIOR CARE HOME HEALTH OF NASHVILLE LLC',
       'NASHVILLE MEDICAL INVESTORS LLC',
       'WEST WILSON FAMILY PRACTICE CENTER, P.C.',
       'ASHOK K. MEHTA, MD, PC', 'DVA RENAL HEALTHCARE INC',
       'CREATIVE HEALTHCARE, PLLC', 'NASHVILLE SENIOR CARE LLC',
       'PINEWOOD MEDICAL LLC', 'NHC-OP LP', 'DICKSON OPERATOR LLC',
       'HILLCREST HEALTHCARE, LLC',
       'CHRISTIAN CARE CENTER OF CHEATHAM COUNTY INC',
       'GENTIVA CERTIFIED HEALTHCARE CORP.',
       'MADISON BEHAVIORAL HEALTH, LLC', 'MADISON ADULT MEDICINE, INC.',
       'VANCO HEALTH CARE AND REHABILITATION LLC',
       'TCMC MADISON-PORTLAND, INC.', 'LEMMON EMERGENCY PHYSICIANS PLLC',
       'HIGHLAND PARK MEDICAL INVESTORS, LLC', 'MARY A. MCELANEY, M.D.',
       'SOUTHERN HILLS NEUROLOGY CONSULTANTS LLC',
       'TENNESSEE FOOT & ANKLE SPECIALISTS PC',
       'JOHAN VAN JAARSVELD MD PLLC',
       'INTERNAL MEDICINE ASSOCIATES OF SOUTHERN HILLS LLC',
       'HOMETOWN RESPIRATORY CONSULTANTS INC',
       'GETHSEMANE CARDIOVASCULAR CLINIC PLLC',
       'GASTROENTEROLOGY SPECIALISTS OF MIDDLE TENNESSEE LLC',
       'AMEDISYS TENNESSEE, LLC', 'ROBERT C. RIPLEY, M.D., P.C.',
       'MEDICAL GROUP - SOUTHERN HILLS OF BRENTWOOD LLC',
       'CARDIOVASCULAR INSTITUTE PC', 'MIDDLE TENNESSEE NEUROLOGY LLC',
       'SPRINGFIELD SURGERY,P.C.',
       'SURGICAL ALLIANCE OF MIDDLE TENNESSEE, PLC',
       'NHC HEALTHCARE HENDERSONVILLE LLC',
       'NHC HEALTHCARE-HENDERSONVILLE LLC',
       'LHC HOMECARE OF TENNESSEE, LLC', 'CARIS HEALTHCARE, LP',
       'DICKSON EAR, NOSE & THROAT PLC',
       'CLINICAL AND FORENSIC PSYCHOLOGY PROFESSIONAL LLC',
       'REGENTS MEDICAL CENTER PC', 'STEEL FAMILY MEDICINE',
       'ANGELA WILLIS FAMILY PRACTICE PLLC', 'ALIVE HOSPICE, INC',
       'HOME HEALTH CARE OF MIDDLE TENNESSEE, LLC',
       'EAR NOSE & THROAT SPECIALISTS OF NASHVILLE PLC',
       'POST-ACUTE PHYSICIANS OF TENNESSEE PLLC',
       'RAPHA FAMILY WELLNESS PLLC', 'WAL-MART STORES EAST LP',
       'SOUTH SIDE DRUG CO INC', 'HILLCREST HEALTHCARE LLC',
       'THE WATERS OF CHEATHAM LLC', 'EMPSON DRUG CO INC',
       'LAKESHORE ESTATES, INC.', 'LIFELINC ANESTHESIA II, PLLC',
       'STERLING PRIMARY CARE', 'COMMUNITY PHARMACY CARE, INC',
       'DICKSON OPTICAL PC', 'KROGER LIMITED PARTNERSHIP I',
       'TENNESSEE CVS PHARMACY, L.L.C.',
       'SELECT PHYSICAL THERAPY HOLDINGS INC',
       "CHARLOTTE'S HOMETOWN PHARMACY", 'CDK LLC',
       'DICKSON ORTHOTICS & PROSTHETICS', 'FAMILY HEALTH, LLC',
       'WALGREEN CO', 'NASHVILLE LUNG CENTER, INC',
       'DRS. TIDWELL, FAULKS, AND ALLEN - OPTOMETRY, PLLC',
       'LP NORTH NASHVILLE, LLC', 'NHC HEALTHCARE-SPRINGFIELD LLC',
       'CURAHEALTH NASHVILLE, LLC', 'XUHAN PC LLP',
       'CRESCENT MEDICAL GROUP PLLC', 'NEURO RESOURCE PC',
       'PRAMOD B. WASUDEV, M.D., PLLC',
       'THE HEART AND VASCULAR CLINIC P.C.',
       'ROBERTSON COUNTY PHYSICAL MEDICINE LLC', 'STEWART MENTAL HEALTH',
       'SPRINGFIELD NEUROLOGY', 'JOHN C WESTERKAMM MD PLLC',
       'LIFELINE HOME HEALTH CARE OF SPRINGFIELD, LLC',
       'THE WATERS OF SPRINGFIELD LLC', 'THE WATERS OF ROBERTSON LLC',
       'CHRISTIAN CARE CENTER OF SPRINGFIELD LLC',
       'WILLIAM J. BINKLEY, M.D. LLC', 'STEVEN G SCHOEMER PC',
       'ASERACARE HOSPICE - NEW HORIZONS, LLC',
       'PARKS PRACTITIONER AND CONSULTING SERVICES LLC',
       'ADORATION HOSPICE, LLC', 'VALLEY MEDICAL CENTER, PLLC',
       'CONTINUOUS CARE SVC LLC', 'PUBLIX TENNESSEE LLC', 'WALGREEN CO.',
       'RAMA MEDICAL GROUP', 'ORTHOPAEDIC SPECIALISTS PLLC',
       'ALIVE HOSPICE, INC.', 'LUNG CONSULTANTS PLLC',
       'THE SURGICAL CLINIC, PLLC',
       'BLUEGRASS INTERNAL MEDICINE ASSOCIATES, PC', 'VICTOR KHA DO PLLC',
       'TENNRX, LLC', 'RIVERGATE PSYCHIATRIC & BEHAVIORAL HEALTH',
       'SLEEP PROFESSIONALS, LLC', 'HENDERSONVILLE OBGYN LLC',
       'TOTAL RENAL CARE INC', 'AMSURG HERMITAGE ANESTHESIA, LLC',
       'HERMITAGE TN ENDOSCOPY ASC, LLC',
       'COLUMBIA MEDICAL GROUP-CENTENNIAL INC',
       'LABORATORY FOR KIDNEY PATHOLOGY INC', 'TOTAL RENAL CARE, INC',
       'STEVEN D GRAHAM MD, NEUROLOGY PC', 'CENTENNIAL WOMENS GROUP, LLC',
       'DONELSON OBSTETRICS & GYN', 'SOUTHERN WOMANS CARE',
       'COOL SPRINGS SURGICAL ASSOCIATES LLC', 'DAVID A WEST',
       'PREMIER ASC LLC', 'DAVID A GILPIN MD PLC',
       'GENESIS WOMENS CARE INC', 'MISTYE TAYLOR MD PLLC',
       'SOUTHERN PAIN INSTITUTE PLLC',
       'STATE OF TENNESSEE STATE F&A PAYROLL',
       'PANACEA PSYCHIATRIC CENTER PLC', 'AIR EVAC EMS INC',
       'TENNESSEE BREAST CARE CENTER PLC',
       'OMFS PHYSICIANS OF ATHENS, PLLC',
       'HEARING SERVICES OF NASHVILLE, LLC',
       'NHC HEALTHCARE SPRINGFIELD LLC', 'GGNSC SPRINGFIELD LLC',
       'DICKSON HEALTHCARE LLC',
       'FIVE STAR REHABILITATION AND WELLNESS SERVICES, LLC.',
       'HARMONY FAMILY HEALTH CARE, LLC', 'NP HOUSECALLS, PLLC',
       'ADAMSPLACE, LLC', 'TENNESSEE CVS PHARMACY LLC',
       'BECKWITH EMERGENCY PHYSICIAN PLLC'])) & (refto_from["ref_to_speciality"].isin(['Cardiovascular Disease','Hematology & Oncology', 'Nephrology', 'Family', 
 'Pulmonary Disease', 'Interventional Cardiology', 'Neurology']))]
    .groupby([ 'ref_to_speciality', 'ref_to_facility' ])['patient_count'].sum())/(refto_from['patient_count'].sum()))*100)
  .sort_values(ascending=False).to_frame()
 .iloc[0:40]
 .rename(columns={'patient_count': '% of total patient_count'})
)
210/466:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

refto_from[(refto_from['ref_from_commid']== 11117)]['ref_from_facility'].nunique()
210/467:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# 11117 = 80; 

refto_from[(refto_from['ref_from_commid']== 9)]['ref_from_facility'].nunique()
210/468:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# 11117 = 80; 9:202

refto_from[(refto_from['ref_from_commid']== 9)]['ref_to_facility'].nunique()
210/469:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# 11117 = 80; 9:202

refto_from[(refto_from['ref_from_commid']== 83)]['ref_from_facility'].nunique()
210/470:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# 11117 = 80; 9:202 : 83:250

refto_from[(refto_from['ref_from_commid']== 83)]['ref_to_facility'].nunique()
210/471:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:, 9:,83:729)

refto_from[(refto_from['ref_from_commid']== 9)]['ref_to_facility'].nunique()
210/472:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:, 9:810,83:729)

refto_from[(refto_from['ref_from_commid']== 11117)]['ref_to_facility'].nunique()
210/473:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)

refto_from[(refto_from['ref_from_commid']== 11117)]['ref_to_npi'].nunique()
210/474:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)

refto_from[(refto_from['ref_from_commid']== 11117)]['ref_to_speciality'].nunique()
210/475:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(11117:121)

refto_from[(refto_from['ref_from_commid']== 9)]['ref_to_speciality'].nunique()
210/476:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(11117:121, 9:117, )

refto_from[(refto_from['ref_from_commid']== 83)]['ref_to_speciality'].nunique()
210/477:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(speciality 11117:121, 9:117, 83:103)

refto_from[(refto_from['ref_from_commid']== 83)]['ref_to_npi'].nunique()
210/478:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(speciality 11117:121, 9:117, 83:103)

refto_from[(refto_from['ref_from_commid']== 11117)]['ref_to_npi'].nunique()
210/479:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(speciality 11117:121, 9:117, 83:103)

refto_from[(refto_from['ref_from_commid']== 11117)]['ref_from_npi'].nunique()
210/480:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(speciality 11117:121, 9:117, 83:103)

refto_from[(refto_from['ref_from_commid']== 11117)]['refered_from_npi'].nunique()
210/481:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(speciality 11117:121, 9:117, 83:103) (refered_from_npi:2561,)

refto_from[(refto_from['ref_from_commid']== 9)]['refered_from_npi'].nunique()
210/482:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(speciality 11117:121, 9:117, 83:103) (refered_from_npi:2561,2267)

refto_from[(refto_from['ref_from_commid']== 83)]['refered_from_npi'].nunique()
210/483:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(speciality 11117:121, 9:117, 83:103) (refered_from_npi:2561,2267, 2177)

refto_from[(refto_from['ref_from_commid']== 11117)]['refered_from_facility'].unique()
210/484:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(speciality 11117:121, 9:117, 83:103) (refered_from_npi:2561,2267, 2177)

refto_from[(refto_from['ref_from_commid']== 11117)]['refered_from_facility'].nunique()
210/485:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(speciality 11117:121, 9:117, 83:103) (refered_from_npi:2561,2267, 2177)

refto_from[(refto_from['ref_from_commid']== 11117)]['ref_from_facility'].nunique()
210/486:
# (
# (((refto_from.groupby(['ref_to_speciality', 'ref_to_facility'])['patient_count']
#    .sum())/(refto_from['patient_count'].sum()))*100)
#    .sort_values(ascending = False)
#  .to_frame()
#     .rename(columns={'patient_count': '% of total patient_count'})
#     .iloc[2:20]
    
    
# )

# (11117 = 80; 9:202 : 83:250)(refered_to from 11117:563, 9:810,83:729)(speciality 11117:121, 9:117, 83:103) (refered_from_npi:2561,2267, 2177)

refto_from[(refto_from['ref_from_commid']== 11117)]['ref_from_facility'].unique()
210/487: refto_from.groupby('ref_to_group')['ref_to_speciality'].value_counts().to_frame()
210/488: refto_from.groupby('ref_to_group')['ref_to_speciality'].value_counts().to_frame().iloc[0:50]
210/489: refto_from.groupby('ref_to_group')['ref_to_speciality'].value_counts().to_frame().iloc[51:100]
210/490: refto_from.groupby('ref_to_group')['ref_to_speciality'].unique()
210/491: refto_from.groupby('ref_to_group')['ref_to_speciality'].unique().to_frame()
210/492: refto_from['ref_to_group'].unique()
210/493: refto_from['ref_to_classification'].unique()
215/1:
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
215/2:
# Load in the dataset and remove rows with missing values

penguins = pd.read_csv('data/penguins.csv').dropna().reset_index(drop = True)

penguins.head(1)
215/3:
# Create a list to hold the variables we will be working with.
variables = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
215/4:
# First, instantiate a KMeans instance which will fit 3 clusters
n_clusters = 3

kmeans = KMeans(n-clusters=3)### Fill in the code to instantiate your model

# Then fit it to the numeric variables of the penguins dataset
kmeans.fit(penguins[variables]) Fill in the rest of this line to fit this model on the selected variables from the penguins dataset
215/5:
# First, instantiate a KMeans instance which will fit 3 clusters
n_clusters = 3

kmeans = KMeans(n_clusters=3)### Fill in the code to instantiate your model

# Then fit it to the numeric variables of the penguins dataset
kmeans.fit(penguins[variables]) Fill in the rest of this line to fit this model on the selected variables from the penguins dataset
215/6:
# First, instantiate a KMeans instance which will fit 3 clusters
n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters)### Fill in the code to instantiate your model

# Then fit it to the numeric variables of the penguins dataset
kmeans.fit(penguins[[variables]]) Fill in the rest of this line to fit this model on the selected variables from the penguins dataset]
215/7:
# First, instantiate a KMeans instance which will fit 3 clusters
n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters)### Fill in the code to instantiate your model

# Then fit it to the numeric variables of the penguins dataset
kmeans.fit(penguins[[variables]]) #Fill in the rest of this line to fit this model on the selected variables from the penguins dataset]
215/8:
# Create a list to hold the variables we will be working with.
variables = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
215/9:
# First, instantiate a KMeans instance which will fit 3 clusters
n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters)### Fill in the code to instantiate your model

# Then fit it to the numeric variables of the penguins dataset
kmeans.fit(penguins[[variables]]) #Fill in the rest of this line to fit this model on the selected variables from the penguins dataset]
215/10:
# First, instantiate a KMeans instance which will fit 3 clusters
n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters)### Fill in the code to instantiate your model

# Then fit it to the numeric variables of the penguins dataset
kmeans.fit(penguins[variables]) #Fill in the rest of this line to fit this model on the selected variables from the penguins dataset]
214/1:
#the packages used
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm

#in addition to this we will use Neo4j to generate the community clusters and find the network ot of
216/1:
#the packages used
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm

#in addition to this we will use Neo4j to generate the community clusters and find the network ot of
216/2:
#the packages used
import pandas as pd
import sqlite3
from tqdm.notebook import tqdm

#in addition to this we will use Neo4j to generate the community clusters and find the network ot of
216/3:
#checking the tables in the sqlite_db

db = sqlite3.connect('data/hop_referal.sqlite')
db.execute("""SELECT 
    name
FROM 
    sqlite_schema
WHERE 
    type ='table' AND 
    name NOT LIKE 'sqlite_%';""").fetchall()
216/4:
query = """
SELECT * 
FROM npicode 
WHERE provider_business_practice_location_address_state_name = 'TN'
"""

with sqlite3.connect('data/hop_referal.sqlite') as db: 
    npicode_sqlite = pd.read_sql(query, db)
npicode_sqlite
217/1:
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
217/2:
# Load in the dataset and remove rows with missing values

penguins = pd.read_csv('data/penguins.csv').dropna().reset_index(drop = True)

penguins.head(1)
217/3:
# Create a list to hold the variables we will be working with.
variables = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
217/4:
# First, instantiate a KMeans instance which will fit 3 clusters
n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters ### Fill in the code to instantiate your model

# Then fit it to the numeric variables of the penguins dataset
kmeans.fit(penguins[variables]) #Fill in the rest of this line to fit this model on the selected variables from the penguins dataset]
217/5:
# Create a list to hold the variables we will be working with.
variables = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
217/6:
# First, instantiate a KMeans instance which will fit 3 clusters
n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters ### Fill in the code to instantiate your model

# Then fit it to the numeric variables of the penguins dataset
kmeans.fit(penguins[variables]) #Fill in the rest of this line to fit this model on the selected variables from the penguins dataset]
217/7:
# First, instantiate a KMeans instance which will fit 3 clusters
n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters) ### Fill in the code to instantiate your model

# Then fit it to the numeric variables of the penguins dataset
kmeans.fit(penguins[variables]) #Fill in the rest of this line to fit this model on the selected variables from the penguins dataset]
217/8:
# Extract the inertia value
inertia_value = kmeans.inertia_
inertia_value
217/9:
#testing the model if incraesing the n_clusters number decrases the inertia

for i in range(1,10):
    kmeans = KMeans(n_clusters=i)

    # Then fit it to the numeric variables of the penguins dataset
    kmeans.fit(penguins[variables])
    print(i, ' ', kmeans.inertia_)
217/10:
# Choose the variables you want to visualize.
# i and j indicate the index of the variable from the variables list 
# ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
i = 0
j = 1

plt.figure(figsize = (10,6))
sns.scatterplot(data = penguins,
               x = variables[i],
               y = variables[j],
               hue = kmeans.labels_)

sns.scatterplot(x = kmeans.cluster_centers_[:,i],
                y = kmeans.cluster_centers_[:,j],
                s = 500, 
                hue = list(range(n_clusters)), 
                marker = 'D',
                legend = False);
217/11:
# Choose the variables you want to visualize.
# i and j indicate the index of the variable from the variables list 
# ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
i = 1
j = 3

plt.figure(figsize = (10,6))
sns.scatterplot(data = penguins,
               x = variables[i],
               y = variables[j],
               hue = kmeans.labels_)

sns.scatterplot(x = kmeans.cluster_centers_[:,i],
                y = kmeans.cluster_centers_[:,j],
                s = 500, 
                hue = list(range(n_clusters)), 
                marker = 'D',
                legend = False);
217/12:
i = 1
j = 3

plt.figure(figsize = (10,6))
sns.scatterplot(data = penguins,
               x = variables[i],
               y = variables[j],
               hue = kmeans.labels_)

sns.scatterplot(x = kmeans.cluster_centers_[:,i],
                y = kmeans.cluster_centers_[:,j],
                s = 500, 
                hue = list(range(n_clusters)), 
                marker = 'D',
                legend = False);
217/13: kmeans.cluster_centers_[:,i]
217/14: kmeans.cluster_centers_[:,j]
217/15:
# kmeans.cluster_centers_[:,j]
n_clusters
217/16:
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
217/17:
# Load in the dataset and remove rows with missing values

penguins = pd.read_csv('data/penguins.csv').dropna().reset_index(drop = True)

penguins.head(1)
217/18:
# Create a list to hold the variables we will be working with.
variables = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
217/19:
# First, instantiate a KMeans instance which will fit 3 clusters
n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters) ### Fill in the code to instantiate your model

# Then fit it to the numeric variables of the penguins dataset
kmeans.fit(penguins[variables]) #Fill in the rest of this line to fit this model on the selected variables from the penguins dataset]
217/20: kmeans.
217/21:
# Choose the variables you want to visualize.
# i and j indicate the index of the variable from the variables list 
# ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
i = 1
j = 3

plt.figure(figsize = (10,6))
sns.scatterplot(data = penguins,
               x = variables[i],
               y = variables[j],
               hue = kmeans.labels_)

sns.scatterplot(x = kmeans.cluster_centers_[:,i],
                y = kmeans.cluster_centers_[:,j],
                s = 500, 
                hue = list(range(n_clusters)), 
                marker = 'D',
                legend = False);
217/22:
# Choose the variables you want to visualize.
# i and j indicate the index of the variable from the variables list 
# ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
i = 0
j = 3

plt.figure(figsize = (10,6))
sns.scatterplot(data = penguins,
               x = variables[i],
               y = variables[j],
               hue = kmeans.labels_)

sns.scatterplot(x = kmeans.cluster_centers_[:,i],
                y = kmeans.cluster_centers_[:,j],
                s = 500, 
                hue = list(range(n_clusters)), 
                marker = 'D',
                legend = False);

##the variables having different scales use the bigger numbers to put clsters, 
##thus scaling the data by preprocessing is needed
217/23:
# Choose the variables you want to visualize.
# i and j indicate the index of the variable from the variables list 
# ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
i = 2
j = 3

plt.figure(figsize = (10,6))
sns.scatterplot(data = penguins,
               x = variables[i],
               y = variables[j],
               hue = kmeans.labels_)

sns.scatterplot(x = kmeans.cluster_centers_[:,i],
                y = kmeans.cluster_centers_[:,j],
                s = 500, 
                hue = list(range(n_clusters)), 
                marker = 'D',
                legend = False);
217/24:
# Choose the variables you want to visualize.
# i and j indicate the index of the variable from the variables list 
# ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
i = 1
j = 3

plt.figure(figsize = (10,6))
sns.scatterplot(data = penguins,
               x = variables[i],
               y = variables[j],
               hue = kmeans.labels_)

sns.scatterplot(x = kmeans.cluster_centers_[:,i],
                y = kmeans.cluster_centers_[:,j],
                s = 500, 
                hue = list(range(n_clusters)), 
                marker = 'D',
                legend = False);

##if uisng i=1 and j=3, the variables having different scales use the bigger numbers to put clsters, 
##thus scaling the data by preprocessing is needed
217/25:
n_clusters = 3

pipeline = Pipeline(
    steps = [
        ('scaler', StandardScaler()),
        ('cluster', KMeans(n_clusters = n_clusters))
    ]
)

pipeline.fit(penguins[variables])
217/26:
# Extract the inertia value from our new model
pipeline['cluster']
217/27:
# Extract the inertia value from our new model
pipeline['cluster']
pipeline['cluster'].inertia_
217/28:
i = 0
j = 1

plt.figure(figsize = (10,6))
sns.scatterplot(data = penguins,
               x = variables[i],
               y = variables[j],
               hue = pipeline[1].labels_)

sns.scatterplot(x = pipeline['scaler'].inverse_transform(pipeline['cluster'].cluster_centers_)[:,i],
                y = pipeline['scaler'].inverse_transform(pipeline['cluster'].cluster_centers_)[:,j],
                s = 500, 
                hue = list(range(n_clusters)), 
                marker = 'D',
                legend = False);
217/29:
# Choose the variables you want to visualize.
# i and j indicate the index of the variable from the variables list 
# ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
i = 0
j =1

plt.figure(figsize = (10,6))
sns.scatterplot(data = penguins,
               x = variables[i],
               y = variables[j],
               hue = kmeans.labels_)

sns.scatterplot(x = kmeans.cluster_centers_[:,i],
                y = kmeans.cluster_centers_[:,j],
                s = 500, 
                hue = list(range(n_clusters)), 
                marker = 'D',
                legend = False);

##if uisng i=1 and j=3, the variables having different scales use the bigger numbers to put clsters, 
##thus scaling the data by preprocessing is needed
217/30:
inertias = []

max_clusters = 5
for n_clusters in range (1, max_clusters+1) ##### Fill this in
    
    pipeline = Pipeline(
        steps = [
            ('scaler', StandardScaler()),
            ('cluster', KMeans(n_clusters = n_clusters))
        ]
    )

    pipeline.fit(penguins[variables])
    
    inertias.append(pipeline['cluster'].inertia_)
217/31:
inertias = []

max_clusters = 5
for n_clusters in range (1, max_clusters+1); ##### Fill this in
    
    pipeline = Pipeline(
        steps = [
            ('scaler', StandardScaler()),
            ('cluster', KMeans(n_clusters = n_clusters))
        ]
    )

    pipeline.fit(penguins[variables])
    
    inertias.append(pipeline['cluster'].inertia_)
217/32:
inertias = []

max_clusters = 5
for n_clusters in range (1, max_clusters+1), ##### Fill this in
    
    pipeline = Pipeline(
        steps = [
            ('scaler', StandardScaler()),
            ('cluster', KMeans(n_clusters = n_clusters))
        ]
    )

    pipeline.fit(penguins[variables])
    
    inertias.append(pipeline['cluster'].inertia_)
217/33:
inertias = []

max_clusters = 5
for n_clusters in range (1, max_clusters+1): ##### Fill this in
    
    pipeline = Pipeline(
        steps = [
            ('scaler', StandardScaler()),
            ('cluster', KMeans(n_clusters = n_clusters))
        ]
    )

    pipeline.fit(penguins[variables])
    
    inertias.append(pipeline['cluster'].inertia_)
217/34:
plt.figure(figsize = (10,6))
plt.plot(range(1, max_clusters + 1), inertias)
plt.scatter(range(1, max_clusters + 1), inertias, s = 100);
217/35:
n_clusters = 2 #### Fill this in

pipeline = Pipeline(
    steps = [
        ('scaler', StandardScaler()),
        ('cluster', KMeans(n_clusters = n_clusters))
    ]
)

pipeline.fit(penguins[variables])
217/36: pd.crosstab(penguins['species'], pipeline['cluster'].labels_)
217/37:
n_clusters = 2 #### Fill this in

pipeline = Pipeline(
    steps = [
        ('scaler', StandardScaler()),
        ('cluster', KMeans(n_clusters = n_clusters, random_state=321))
    ]
)

pipeline.fit(penguins[variables])
217/38: pd.crosstab(penguins['species'], pipeline['cluster'].labels_)
217/39:
n_clusters = 3 #### Fill this in

pipeline = Pipeline(
    steps = [
        ('scaler', StandardScaler()),
        ('cluster', KMeans(n_clusters = n_clusters, random_state=321))
    ]
)

pipeline.fit(penguins[variables])
217/40: pd.crosstab(penguins['species'], pipeline['cluster'].labels_)
218/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
218/2: houses = pd.read_csv('../data/kc_house_data.csv')
218/3:
#data comes from https://www.kaggle.com/datasets/harlfoxem/housesalesprediction
houses = pd.read_csv('../data/kc_house_data.csv')

houses
218/4: from sklearn.linear_model import LinearRegression
218/5: linreg = LinearRegression()
218/6:
X = houses[['sqft_living']]
y = houses['price']
218/7: from sklearn.model_selection import train_test_split
218/8: X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
218/9: linreg.fit(X_train, y_train)
218/10: linreg.intercept_
218/11: linreg.coef_
218/12: linreg.predict(houses[['sqft_living']])
218/13: from sklearn.metrics import mean_squared_error
218/14: mean_squared_error(y_train, linreg.predict(X_train))
218/15: mean_squared_error(y_test, linreg.predict(X_test))
218/16:
x_grid = np.linspace(start = 0, stop = np.max(houses['sqft_living'].head(25)))
x_grid = pd.DataFrame({
    'sqft_living': x_grid
})
y_grid = linreg.predict(x_grid)

fontsize = 16

fig, ax = plt.subplots(figsize = (10,6))


plt.plot(x_grid, y_grid, label = 'prediction')

houses.head(50).plot(kind = 'scatter',
                    x = 'sqft_living',
                    y = 'price',
                    ax = ax,
                    color = 'black',
                    label = 'observed')

plt.legend(fontsize = fontsize - 2)


plt.xlabel('sqft_living', fontsize = fontsize)
plt.ylabel('price', fontsize = fontsize)
plt.xticks(fontsize = fontsize - 2)
plt.yticks(fontsize = fontsize - 2)
ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'));
218/17:
x_grid = np.linspace(start = 0, stop = np.max(houses['sqft_living'].head(25)))
x_grid = pd.DataFrame({
    'sqft_living': x_grid
})
y_grid = linreg.predict(x_grid)

fontsize = 16

fig, ax = plt.subplots(figsize = (10,6))


plt.plot(x_grid, y_grid, label = 'prediction')

houses.head(50).plot(kind = 'scatter',
                    x = 'sqft_living',
                    y = 'price',
                    ax = ax,
                    color = 'black',
                    label = 'observed')

plt.legend(fontsize = fontsize - 2)


plt.xlabel('sqft_living', fontsize = fontsize)
plt.ylabel('price', fontsize = fontsize)
plt.xticks(fontsize = fontsize - 2)
plt.yticks(fontsize = fontsize - 2)
ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'));
218/18:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
218/19:
#data comes from https://www.kaggle.com/datasets/harlfoxem/housesalesprediction
houses = pd.read_csv('../data/kc_house_data.csv')

houses
218/20: from sklearn.linear_model import LinearRegression
218/21: linreg = LinearRegression()
218/22:
X = houses[['sqft_living']] #to get the output is dataframe
y = houses['price']
218/23: from sklearn.model_selection import train_test_split
218/24: X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
218/25: linreg.fit(X_train, y_train)
218/26: linreg.intercept_  #y=mx+b
218/27: linreg.coef_
218/28: linreg.predict(houses[['sqft_living']])
218/29: from sklearn.metrics import mean_squared_error
218/30: mean_squared_error(y_train, linreg.predict(X_train))
218/31: mean_squared_error(y_test, linreg.predict(X_test))
218/32:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
218/33:
#data comes from https://www.kaggle.com/datasets/harlfoxem/housesalesprediction
houses = pd.read_csv('../data/kc_house_data.csv')

houses
218/34: from sklearn.linear_model import LinearRegression
218/35: linreg = LinearRegression()
218/36:
X = houses[['sqft_living']] #to get the output is dataframe
y = houses['price']
218/37: from sklearn.model_selection import train_test_split
218/38: X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
218/39: linreg.fit(X_train, y_train)
218/40: linreg.intercept_  #y=mx+b
218/41: linreg.coef_
218/42: linreg.predict(houses[['sqft_living']])
218/43: from sklearn.metrics import mean_squared_error
218/44: mean_squared_error(y_train, linreg.predict(X_train))
218/45: mean_squared_error(y_test, linreg.predict(X_test))
218/46: pd._version__
218/47: pd._version__
218/48: pd.version__
218/49: pd.version_
218/50: pd._version_
218/51: pd._version__
218/52: pd.__version__
218/53:
x_grid = np.linspace(start = 0, stop = np.max(houses['sqft_living'].head(25)))
x_grid = pd.DataFrame({
    'sqft_living': x_grid
})
y_grid = linreg.predict(x_grid)

fontsize = 16

fig, ax = plt.subplots(figsize = (10,6))


plt.plot(x_grid, y_grid, label = 'prediction')

houses.head(50).plot(kind = 'scatter',
                    x = 'sqft_living',
                    y = 'price',
                    ax = ax,
                    color = 'black',
                    label = 'observed')

plt.legend(fontsize = fontsize - 2)


plt.xlabel('sqft_living', fontsize = fontsize)
plt.ylabel('price', fontsize = fontsize)
plt.xticks(fontsize = fontsize - 2)
plt.yticks(fontsize = fontsize - 2)
ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'));
218/54:
x_grid = np.linspace(start = 0, stop = np.max(houses['sqft_living'].head(25)))
x_grid = pd.DataFrame({
    'sqft_living': x_grid
})
y_grid = linreg.predict(x_grid)

fontsize = 16

fig, ax = plt.subplots(figsize = (10,6))


plt.plot(x_grid, y_grid, label = 'prediction')

houses.head(50).plot(kind = 'scatter',
                    x = 'sqft_living',
                    y = 'price',
                    ax = ax,
                    color = 'black',
                    label = 'observed')

plt.legend(fontsize = fontsize - 2)


plt.xlabel('sqft_living', fontsize = fontsize)
plt.ylabel('price', fontsize = fontsize)
plt.xticks(fontsize = fontsize - 2)
plt.yticks(fontsize = fontsize - 2)
ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'));
218/55:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
218/56:
#data comes from https://www.kaggle.com/datasets/harlfoxem/housesalesprediction
houses = pd.read_csv('../data/kc_house_data.csv')

houses
218/57: from sklearn.linear_model import LinearRegression
218/58: linreg = LinearRegression()
218/59:
X = houses[['sqft_living']] #to get the output is dataframe
y = houses['price']
218/60: from sklearn.model_selection import train_test_split
218/61: X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
218/62: linreg.fit(X_train, y_train)
218/63: linreg.intercept_  #y=mx+b
218/64: linreg.coef_
218/65: linreg.predict(houses[['sqft_living']])
218/66: from sklearn.metrics import mean_squared_error
218/67: mean_squared_error(y_train, linreg.predict(X_train))
218/68: mean_squared_error(y_test, linreg.predict(X_test))
218/69:
x_grid = np.linspace(start = 0, stop = np.max(houses['sqft_living'].head(25)))
x_grid = pd.DataFrame({
    'sqft_living': x_grid
})
y_grid = linreg.predict(x_grid)

fontsize = 16

fig, ax = plt.subplots(figsize = (10,6))


plt.plot(x_grid, y_grid, label = 'prediction')

houses.head(50).plot(kind = 'scatter',
                    x = 'sqft_living',
                    y = 'price',
                    ax = ax,
                    color = 'black',
                    label = 'observed')

plt.legend(fontsize = fontsize - 2)


plt.xlabel('sqft_living', fontsize = fontsize)
plt.ylabel('price', fontsize = fontsize)
plt.xticks(fontsize = fontsize - 2)
plt.yticks(fontsize = fontsize - 2)
ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'));
218/70: pd.__version__ #(pd.two underscoreversion two undersocres)
218/71:
x_grid = np.linspace(start = 0, stop = np.max(houses['sqft_living'].head(25)))
x_grid = pd.DataFrame({
    'sqft_living': x_grid
})
y_grid = linreg.predict(x_grid)

fontsize = 16

fig, ax = plt.subplots(figsize = (10,6))


plt.plot(x_grid, y_grid, label = 'prediction')

houses.head(50).plot(kind = 'scatter',
                    x = 'sqft_living',
                    y = 'price',
                    ax = ax,
                    color = 'black',
                    label = 'observed')

plt.legend(fontsize = fontsize - 2)


plt.xlabel('sqft_living', fontsize = fontsize)
plt.ylabel('price', fontsize = fontsize)
plt.xticks(fontsize = fontsize - 2)
plt.yticks(fontsize = fontsize - 2)
ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'));
218/72:
x_grid = np.linspace(start = 0, stop = np.max(houses['sqft_living'].head(25)))
x_grid = pd.DataFrame({
    'sqft_living': x_grid
})
y_grid = linreg.predict(x_grid)

fontsize = 16

fig, ax = plt.subplots(figsize = (10,6))


plt.plot(x_grid, y_grid, label = 'prediction')

houses.head(50).plot(kind = 'scatter',
                    x = 'sqft_living',
                    y = 'price',
                    ax = ax,
                    color = 'black',
                    label = 'observed')

plt.legend(fontsize = fontsize - 2)


plt.xlabel('sqft_living', fontsize = fontsize)
plt.ylabel('price', fontsize = fontsize)
plt.xticks(fontsize = fontsize - 2)
plt.yticks(fontsize = fontsize - 2)
ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'));
218/73:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
218/74:
#data comes from https://www.kaggle.com/datasets/harlfoxem/housesalesprediction
houses = pd.read_csv('../data/kc_house_data.csv')

houses
218/75: from sklearn.linear_model import LinearRegression
218/76: linreg = LinearRegression()
218/77:
X = houses[['sqft_living']] #to get the output is dataframe
y = houses['price']
218/78: from sklearn.model_selection import train_test_split
218/79: X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
218/80: linreg.fit(X_train, y_train)
218/81: linreg.intercept_  #y=mx+b
218/82: linreg.coef_
218/83: linreg.predict(houses[['sqft_living']])
218/84: from sklearn.metrics import mean_squared_error
218/85: mean_squared_error(y_train, linreg.predict(X_train))
218/86: mean_squared_error(y_test, linreg.predict(X_test))
218/87:
x_grid = np.linspace(start = 0, stop = np.max(houses['sqft_living'].head(25)))
x_grid = pd.DataFrame({
    'sqft_living': x_grid
})
y_grid = linreg.predict(x_grid)

fontsize = 16

fig, ax = plt.subplots(figsize = (10,6))


plt.plot(x_grid, y_grid, label = 'prediction')

houses.head(50).plot(kind = 'scatter',
                    x = 'sqft_living',
                    y = 'price',
                    ax = ax,
                    color = 'black',
                    label = 'observed')

plt.legend(fontsize = fontsize - 2)


plt.xlabel('sqft_living', fontsize = fontsize)
plt.ylabel('price', fontsize = fontsize)
plt.xticks(fontsize = fontsize - 2)
plt.yticks(fontsize = fontsize - 2)
ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'));
218/88: pd.__version__ #(pd.two underscoreversion two undersocres)
220/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
220/2: pd.__version__
220/3:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
220/4: pd.__version__
220/5:
#data comes from https://www.kaggle.com/datasets/harlfoxem/housesalesprediction
houses = pd.read_csv('../data/kc_house_data.csv')

houses
220/6: from sklearn.linear_model import LinearRegression
220/7: linreg = LinearRegression()
220/8:
X = houses[['sqft_living']] #to get the output is dataframe
y = houses['price']
220/9: from sklearn.model_selection import train_test_split
220/10: X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
220/11: linreg.fit(X_train, y_train)
220/12: linreg.intercept_  #y=mx+b
220/13: linreg.coef_
220/14: linreg.predict(houses[['sqft_living']])
220/15: from sklearn.metrics import mean_squared_error
220/16: mean_squared_error(y_train, linreg.predict(X_train))
220/17: mean_squared_error(y_test, linreg.predict(X_test))
220/18:
x_grid = np.linspace(start = 0, stop = np.max(houses['sqft_living'].head(25)))
x_grid = pd.DataFrame({
    'sqft_living': x_grid
})
y_grid = linreg.predict(x_grid)

fontsize = 16

fig, ax = plt.subplots(figsize = (10,6))


plt.plot(x_grid, y_grid, label = 'prediction')

houses.head(50).plot(kind = 'scatter',
                    x = 'sqft_living',
                    y = 'price',
                    ax = ax,
                    color = 'black',
                    label = 'observed')

plt.legend(fontsize = fontsize - 2)


plt.xlabel('sqft_living', fontsize = fontsize)
plt.ylabel('price', fontsize = fontsize)
plt.xticks(fontsize = fontsize - 2)
plt.yticks(fontsize = fontsize - 2)
ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'));
220/19: pd.__version__ #(pd.two underscoreversion two undersocres)
221/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
221/2: pd.__version__
221/3: from sklearn.linear_model import LinearRegression
221/4:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
221/5: pd.__version__
221/6:
#data comes from https://www.kaggle.com/datasets/harlfoxem/housesalesprediction
houses = pd.read_csv('../data/kc_house_data.csv')

houses
221/7: from sklearn.linear_model import LinearRegression
221/8: linreg = LinearRegression()
221/9:
X = houses[['sqft_living']] #to get the output is dataframe
y = houses['price']
221/10: from sklearn.model_selection import train_test_split
221/11: X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
221/12: linreg.fit(X_train, y_train)
221/13: linreg.intercept_  #y=mx+b
221/14: linreg.coef_
221/15: linreg.predict(houses[['sqft_living']])
221/16: from sklearn.metrics import mean_squared_error
221/17: mean_squared_error(y_train, linreg.predict(X_train))
221/18: mean_squared_error(y_test, linreg.predict(X_test))
221/19:
x_grid = np.linspace(start = 0, stop = np.max(houses['sqft_living'].head(25)))
x_grid = pd.DataFrame({
    'sqft_living': x_grid
})
y_grid = linreg.predict(x_grid)

fontsize = 16

fig, ax = plt.subplots(figsize = (10,6))


plt.plot(x_grid, y_grid, label = 'prediction')

houses.head(50).plot(kind = 'scatter',
                    x = 'sqft_living',
                    y = 'price',
                    ax = ax,
                    color = 'black',
                    label = 'observed')

plt.legend(fontsize = fontsize - 2)


plt.xlabel('sqft_living', fontsize = fontsize)
plt.ylabel('price', fontsize = fontsize)
plt.xticks(fontsize = fontsize - 2)
plt.yticks(fontsize = fontsize - 2)
ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'));
222/1: import pandas as pd
222/2:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
submission = pd.read_csv('../data/sample_submission.csv')
222/3:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
submission = pd.read_csv('../data/sample_submission.csv')
222/4: X_train
222/5:
train_predictions = ( 
    X_train
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions
222/6: from sklearn.metrics import accuracy_score
222/7:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions['teamId']
)
222/8:
y_pred = ( 
    X_test
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    ['teamId']
    .reset_index(drop = True)
)

y_pred
222/9:
y_pred = ( 
    X_test
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    ['teamId']
    .reset_index(drop = True)
)

y_pred
222/10:
submission['winner'] = y_pred
submission.head()
222/11: submission.to_csv('../data/first_submission.csv', index = False)
221/20:
predictors = ['sqft_living', 'condition']

X = houses[predictors]
y = houses['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
221/21: linreg = LinearRegression().fit(X_train, y_train)
221/22: linreg.intercept_
221/23: linreg.coef_
221/24:
coefficients = pd.DataFrame({
    'variable': ['intercept'] + predictors,
    'coefficient': [linreg.intercept_] + list(linreg.coef_)
})

coefficients
221/25: mean_squared_error(y_train, linreg.predict(X_train))
223/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
223/2: kc = pd.read_csv('data/kc_house_data.csv')
223/3: kc = pd.read_csv('../data/kc_house_data.csv')
223/4:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
223/5: kc = pd.read_csv('../data/kc_house_data.csv')
223/6:
X = kc[['sqft_living', 'condition']]
y = kc['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)

linreg_base = LinearRegression().fit(X_train, y_train)
223/7:
coefficients = pd.DataFrame({
    'variable': linreg_base.feature_names_in_,
    'coefficient': linreg_base.coef_
})

coefficients
223/8:
for condition in range(1, 6):
    prediction_df = pd.DataFrame({
      'sqft_living': np.linspace(
          start = kc['sqft_living'].quantile(0.05),
          stop = kc['sqft_living'].quantile(0.95),
      ),
        'condition': condition
    })

    prediction_df['prediction'] = linreg_base.predict(prediction_df)

    plt.plot(prediction_df['sqft_living'],
             prediction_df['prediction'],
             label = condition)
    
plt.xlabel('sqft_living')
plt.ylabel('prediction')

plt.legend();
223/9:
print(f'Training Set: {r2_score(y_train, linreg_base.predict(X_train))}')
print(f'Training Set: {r2_score(y_test, linreg_base.predict(X_test))}')
223/10:
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
223/11: PolynomialFeatures(interaction_only = True, include_bias = False).fit_transform(X_train)
223/12:
lr_interactions = Pipeline(steps = 
                          [
                              ('pf', PolynomialFeatures(interaction_only = True,
                                                        include_bias = False)),
                              ('lr', LinearRegression())
                          ]
                          )

lr_interactions.fit(X_train, y_train)
223/13:
coefficients = pd.DataFrame({
    'variable': lr_interactions['pf'].get_feature_names_out(),
    'coefficient': lr_interactions['lr'].coef_
})

coefficients
223/14:
for condition in range(1, 6):
    prediction_df = pd.DataFrame({
      'sqft_living': np.linspace(
          start = kc['sqft_living'].quantile(0.05),
          stop = kc['sqft_living'].quantile(0.95),
      ),
        'condition': condition
    })

    prediction_df['prediction'] = lr_interactions.predict(prediction_df)

    plt.plot(prediction_df['sqft_living'],
             prediction_df['prediction'],
             label = condition)
    
plt.xlabel('sqft_living')
plt.ylabel('prediction')

plt.legend();
223/15:
print(f'Training Set: {r2_score(y_train, lr_interactions.predict(X_train))}')
print(f'Training Set: {r2_score(y_test, lr_interactions.predict(X_test))}')
223/16: from sklearn.model_selection import cross_val_score, KFold
223/17: kfold = KFold(n_splits = 10, shuffle = True, random_state = 321)
223/18:
base_cv_scores = cross_val_score(
    estimator = linreg_base,
    X = X_train,
    y = y_train,
    cv = kfold
)

print(base_cv_scores)
print(np.mean(base_cv_scores))
223/19:
interactions_cv_scores = cross_val_score(
    estimator = lr_interactions,
    X = X_train,
    y = y_train,
    cv = kfold
)

print(interactions_cv_scores)
print(np.mean(interactions_cv_scores))
223/20: from sklearn.preprocessing import OneHotEncoder
223/21: OneHotEncoder(sparse = False).fit_transform(kc[['zipcode']])
223/22: from sklearn.compose import ColumnTransformer
223/23:
X = kc[['sqft_living', 'condition', 'zipcode']]
y = kc['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
223/24:
lr_zip_int = Pipeline(steps = 
                          [
                              ('ct', ColumnTransformer(
                                  transformers = [
                                      ('ohe', OneHotEncoder(), ['zipcode']),
                                      ('interactions',
                                       PolynomialFeatures(interaction_only = True,
                                                                         include_bias = False),
                                       ['sqft_living', 'condition'])
                                  ]
                              )
                              ),
                              ('lr', LinearRegression())
                          ]
                          )

lr_zip_int.fit(X_train, y_train)
223/25:
coefficients = pd.DataFrame({
    'variable': lr_zip_int['ct'].get_feature_names_out(),
    'coefficient': lr_zip_int['lr'].coef_
})

coefficients
223/26:
print(f'Training Set: {r2_score(y_train, lr_zip_int.predict(X_train))}')
print(f'Training Set: {r2_score(y_test, lr_zip_int.predict(X_test))}')
223/27:
lr_zip = Pipeline(steps = 
                          [
                              ('ct', ColumnTransformer(
                                  transformers = [
                                      ('ohe', OneHotEncoder(), ['zipcode']),
                                      ('pt', 'passthrough', ['sqft_living', 'condition'])
                                  ]
                              )
                              ),
                              ('lr', LinearRegression())
                          ]
                          )

lr_zip.fit(X_train, y_train)
223/28:
print(f'Training Set: {r2_score(y_train, lr_zip.predict(X_train))}')
print(f'Training Set: {r2_score(y_test, lr_zip.predict(X_test))}')
223/29:
int_cv_scores = cross_val_score(
    estimator = lr_zip_int,
    X = X_train,
    y = y_train,
    cv = kfold
)

print(int_cv_scores)
print(np.mean(int_cv_scores))
223/30:
from sklear import __version__
print(__version__)
223/31:
from sklearn import __version__
print(__version__)
221/26: from sklearn.linear_model import LinearRegression
223/32: pip install -U scikit-learn
224/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
224/2: kc = pd.read_csv('../data/kc_house_data.csv')
224/3:
X = kc[['sqft_living', 'condition']]
y = kc['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)

linreg_base = LinearRegression().fit(X_train, y_train)
224/4:
coefficients = pd.DataFrame({
    'variable': linreg_base.feature_names_in_,
    'coefficient': linreg_base.coef_
})

coefficients
224/5:
for condition in range(1, 6):
    prediction_df = pd.DataFrame({
      'sqft_living': np.linspace(
          start = kc['sqft_living'].quantile(0.05),
          stop = kc['sqft_living'].quantile(0.95),
      ),
        'condition': condition
    })

    prediction_df['prediction'] = linreg_base.predict(prediction_df)

    plt.plot(prediction_df['sqft_living'],
             prediction_df['prediction'],
             label = condition)
    
plt.xlabel('sqft_living')
plt.ylabel('prediction')

plt.legend();
224/6:
print(f'Training Set: {r2_score(y_train, linreg_base.predict(X_train))}')
print(f'Training Set: {r2_score(y_test, linreg_base.predict(X_test))}')
224/7:
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
224/8: PolynomialFeatures(interaction_only = True, include_bias = False).fit_transform(X_train)
224/9:
lr_interactions = Pipeline(steps = 
                          [
                              ('pf', PolynomialFeatures(interaction_only = True,
                                                        include_bias = False)),
                              ('lr', LinearRegression())
                          ]
                          )

lr_interactions.fit(X_train, y_train)
224/10:
coefficients = pd.DataFrame({
    'variable': lr_interactions['pf'].get_feature_names_out(),
    'coefficient': lr_interactions['lr'].coef_
})

coefficients
224/11:
for condition in range(1, 6):
    prediction_df = pd.DataFrame({
      'sqft_living': np.linspace(
          start = kc['sqft_living'].quantile(0.05),
          stop = kc['sqft_living'].quantile(0.95),
      ),
        'condition': condition
    })

    prediction_df['prediction'] = lr_interactions.predict(prediction_df)

    plt.plot(prediction_df['sqft_living'],
             prediction_df['prediction'],
             label = condition)
    
plt.xlabel('sqft_living')
plt.ylabel('prediction')

plt.legend();
224/12:
print(f'Training Set: {r2_score(y_train, lr_interactions.predict(X_train))}')
print(f'Training Set: {r2_score(y_test, lr_interactions.predict(X_test))}')
224/13: from sklearn.model_selection import cross_val_score, KFold
224/14: kfold = KFold(n_splits = 10, shuffle = True, random_state = 321)
224/15:
base_cv_scores = cross_val_score(
    estimator = linreg_base,
    X = X_train,
    y = y_train,
    cv = kfold
)

print(base_cv_scores)
print(np.mean(base_cv_scores))
224/16:
interactions_cv_scores = cross_val_score(
    estimator = lr_interactions,
    X = X_train,
    y = y_train,
    cv = kfold
)

print(interactions_cv_scores)
print(np.mean(interactions_cv_scores))
224/17: from sklearn.preprocessing import OneHotEncoder
224/18: OneHotEncoder(sparse = False).fit_transform(kc[['zipcode']])
224/19: from sklearn.compose import ColumnTransformer
224/20:
X = kc[['sqft_living', 'condition', 'zipcode']]
y = kc['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
224/21:
lr_zip_int = Pipeline(steps = 
                          [
                              ('ct', ColumnTransformer(
                                  transformers = [
                                      ('ohe', OneHotEncoder(), ['zipcode']),
                                      ('interactions',
                                       PolynomialFeatures(interaction_only = True,
                                                                         include_bias = False),
                                       ['sqft_living', 'condition'])
                                  ]
                              )
                              ),
                              ('lr', LinearRegression())
                          ]
                          )

lr_zip_int.fit(X_train, y_train)
224/22:
coefficients = pd.DataFrame({
    'variable': lr_zip_int['ct'].get_feature_names_out(),
    'coefficient': lr_zip_int['lr'].coef_
})

coefficients
224/23:
print(f'Training Set: {r2_score(y_train, lr_zip_int.predict(X_train))}')
print(f'Training Set: {r2_score(y_test, lr_zip_int.predict(X_test))}')
224/24:
lr_zip = Pipeline(steps = 
                          [
                              ('ct', ColumnTransformer(
                                  transformers = [
                                      ('ohe', OneHotEncoder(), ['zipcode']),
                                      ('pt', 'passthrough', ['sqft_living', 'condition'])
                                  ]
                              )
                              ),
                              ('lr', LinearRegression())
                          ]
                          )

lr_zip.fit(X_train, y_train)
224/25:
print(f'Training Set: {r2_score(y_train, lr_zip.predict(X_train))}')
print(f'Training Set: {r2_score(y_test, lr_zip.predict(X_test))}')
224/26:
int_cv_scores = cross_val_score(
    estimator = lr_zip_int,
    X = X_train,
    y = y_train,
    cv = kfold
)

print(int_cv_scores)
print(np.mean(int_cv_scores))
224/27:
no_int_cv_scores = cross_val_score(
    estimator = lr_zip,
    X = X_train,
    y = y_train,
    cv = kfold
)

print(no_int_cv_scores)
print(np.mean(no_int_cv_scores))
224/28:
from sklearn import __version__
print(__version__)
222/12: X_train.head(20)
222/13: X_train.head(10)
222/14: y_train.head(10)
222/15: X_train.head(10)
222/16: pd.read_json('../data/champion.json')
222/17: champion = pd.read_json('../data/champion.json')
222/18:
champion = pd.read_json('../data/champion.json')
champion
222/19:
champion_data = pd.DataFrame.from_dict(champions['data'].values.tolist())   

champion_data
222/20:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data
222/21:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.columns
222/22:
champion_data_info = pd.DataFrame.from_dict(champion['info'].values.tolist())   

champion_data
222/23:
champion_data_info = pd.DataFrame.from_dict(champion_data['info'].values.tolist())   

champion_data_info
222/24:
champion_data_stats = pd.DataFrame.from_dict(champion_data['stats'].values.tolist())   

champion_data_stats
222/25:
champion_data_stats = pd.DataFrame.from_dict(champion_data['stats'].values.tolist())   

champion_data_stats.columns
222/26:
champion_data_info = pd.DataFrame.from_dict(champion_data['info'].values.tolist())   

champion_data_info.columns
222/27:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head()
222/28:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
222/29:
champion_data_stats = pd.DataFrame.from_dict(champion_data['stats'].values.tolist())   

champion_data_stats.head(5)
222/30: pd.read_csv('../data/champion_mastery.csv')
222/31:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data.loc[['key']==51]
222/32:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data.loc['key']==51
222/33:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']==51].count()
222/34:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']==51]
222/35:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']==160]
222/36:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
# champion_data[champion_data['key']==160]
222/37:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']==166]
222/38:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data['key']==166
222/39:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
[champion_data['key']==166]
222/40:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']==166].sort_values()
222/41:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']==166]
222/42:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']==166]['name']
222/43:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']==166]['name'].count()
222/44:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
# champion_data[champion_data['key']==166]['name'].count()
222/45:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
# champion_data[champion_data['key']==166]['title'].count()
222/46:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']==166]['title'].count()
222/47:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']==166]['title']
222/48:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
# champion_data[champion_data['key']==166]['title']
222/49:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data.loc[champion_data['key']==166]['title']
222/50:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data.loc[champion_data['key']==166]
222/51:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data.loc[[champion_data['key']==166]]
222/52:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data.loc[champion_data['key']==166]
222/53:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats

champ.groupby('championId')['championPoints '].sum()
222/54:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats

champ.groupby('championId')['championPoints'].sum()
222/55:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats

[champ.groupby('championId')['championPoints']].sum()
222/56:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats

champ.groupby('championId')['championPoints'].sum().to_DataFrame()
222/57:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats

champ.groupby('championId')['championPoints'].sum().to_frame()
222/58:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats

champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame()
222/59: champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame()
222/60:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats
222/61: champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame().tail(20)
222/62: champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame().iloc[20:50]
222/63: champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame().iloc[0:20]
222/64: champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame().iloc[0:50]
222/65:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
222/66:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['name']== "Akshan"]
222/67:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['key']== 166]
222/68:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['name']== "Akshan"]
222/69: champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame().iloc[40:50]
222/70:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats
champ.columns
222/71: champ_grp = champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame().iloc[40:50]
222/72: champ_grp = champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame()
222/73:
champ_grp = champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame()
champ_grp
222/74: pd.merge(X_train, champ_grp, left_on="championId", right_on="championId", how=left)
222/75: pd.merge(X_train, champ_grp, left_on="championId", right_on="championId", how="left")
222/76: pd.merge(X_train, champ_grp, left_on="championId", right_on="championId", how="left").shape
222/77: pd.merge(X_train, champ_grp, left_on="championId", right_on="championId", how="left")
222/78: X_train_added = pd.merge(X_train, champ_grp, left_on="championId", right_on="championId", how="left")
222/79:
X_train_added = pd.merge(X_train, champ_grp, left_on="championId", right_on="championId", how="left")
X_train_added.head(5)
222/80:
# **using the championPoints for prediction**
train_predictions = ( 
    X_train_added
    .sort_values(['matchId', 'summonerLevel', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions
222/81:
train_predictions = ( 
    X_train
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions
222/82:
train_predictions = ( 
    X_train
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions.shape
222/83:
train_predictions = ( 
    X_train
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions
222/84:
# **using the championPoints for prediction**
train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'summonerLevel', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/85:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId',  'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/86:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId',  'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/87:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)
222/88:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#Score is reduced adding the championpoints reduces from 0.503 to 0.489
222/89: **Will add both the smmerlevel and the chapionpoints for prediction**
222/90:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'summonerLevel', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/91:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'summonerLevel', 'championPoints'], ascending = [True, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/92:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#Score is reduced adding the championpoints reduces from 0.503 to 0.489
222/93:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/94:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#Score is reduced adding the championpoints reduces from 0.503 to 0.489
222/95:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'summonerLevel', 'championPoints'], ascending = [True, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/96:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)
222/97:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#it is back to 0.50375, lookslike championpoint has no additive effect.
222/98:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champion_data.join(champion_data_stats)
222/99:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champion_data.join(champion_data_stats).columns
222/100:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)
222/101:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
222/102:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape()
222/103:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape
222/104:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape #162, 31
champ_spraed.columns
222/105:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape #162, 31
champ_spraed.columns()
222/106:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape #162, 31
champ_spread.columns()
222/107:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape #162, 31
champ_spread.columns
222/108:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape #162, 31
champ_spread.columns.to_list()
222/109:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape #162, 31
champ_spread.columns.to_list()
222/110:
champ_spread.drop['version',
 'id',
 'key',
 'name',
 'title',
 'blurb',
 'info',
 'image',
 'tags',
 'partype',
 'stats',]
222/111:
champ_spread.drop(columns = ['version',
 'id',
 'key',
 'name',
 'title',
 'blurb',
 'info',
 'image',
 'tags',
 'partype',
 'stats'])
222/112:
champ_spread.drop(columns = ['version',
  'title',
 'blurb',
 'info',
 'image',
 'tags',
 'partype',
 'stats'])
222/113:
champ_spread.drop(columns = ['version',
 'id',
 
 'title',
 'blurb',
 'info',
 'image',
 'tags',
 'partype',
 'stats'])
222/114:
champ_spread.drop(columns = ['version',
 'id',
 
 'title',
 'blurb',
 'info',
 'image',
 'tags',
 'partype',
 'stats']).rename(columns = {'key': 'championId',
                             'name': 'championName'})
222/115:
champ_spread1 = champ_spread.drop(columns = ['version',
 'id',
 
 'title',
 'blurb',
 'info',
 'image',
 'tags',
 'partype',
 'stats']).rename(columns = {'key': 'championId',
                             'name': 'championName'})
222/116:
X_train_added1 = pd.merge(X_train_added, champ_spread1, on["championId", "championName"], how="left")
X_train_added1.head(5)
222/117:
X_train_added1 = pd.merge(X_train_added, champ_spread1, on=["championId", "championName"], how="left")
X_train_added1.head(5)
222/118:
X_train_added1 = pd.merge(X_train_added, champ_spread1, on=("championId", "championName"), how="left")
X_train_added1.head(5)
222/119:
X_train_added1 = pd.merge(X_train_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          rightt_on=["championId", "championName"], 
                          how="left")
X_train_added1.head(5)
222/120:
X_train_added1 = pd.merge(X_train_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_train_added1.head(5)
222/121: X_train.info()
222/122: X_train_added.info()
222/123: X_train.info()
222/124: X_train_added.info()
222/125: champ_spread1.info()
222/126:
champ_spread1.info()

champ_spread1['championId'] = champ_spread1['championId'].astype('int')
222/127:
# champ_spread1.info()

champ_spread1['championId'] = champ_spread1['championId'].astype('int')
222/128:


champ_spread1['championId'] = champ_spread1['championId'].astype('int')
champ_spread1.info()
222/129:


champ_spread1['championId'] = champ_spread1['championId'].astype('int64')
champ_spread1.info()
222/130:
X_train_added1 = pd.merge(X_train_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_train_added1.head(5)
222/131:
X_train_added1 = pd.merge(X_train_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_train_added1.head(5)
X_train_added1.info
222/132:
X_train_added1 = pd.merge(X_train_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_train_added1.head(5)
X_train_added1.column.to_list()
222/133:
X_train_added1 = pd.merge(X_train_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_train_added1.head(5)
X_train_added1.columns.to_list()
222/134: pd.read_csv('../data/teamPositions.csv')
222/135: teamposition = pd.read_csv('../data/teamPositions.csv') #matchId    participantId   teamPositio
222/136:
teamposition = pd.read_csv('../data/teamPositions.csv') #matchId    participantId   teamPositio

X_train_added2 = pd.merge(X_train_added1, teamposition, 
                          left_on=["matchId", "participantId"], 
                          right_on=["matchId", "participantId"], 
                          how="left")
X_train_added2.head(5)
X_train_added2.columns.to_list()
222/137:
train_predictions_added1 = ( 
    X_train_added1
    .sort_values(['matchId', 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/138:
train_predictions_added1 = ( 
    X_train_added1
    .sort_values(['matchId', 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed'], ascending = [True, False, False, False, False, 
                              False, False, False, False, False,
                             False, False, False, False, False,
                             False, False, False, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/139:
train_predictions_added1 = ( 
    X_train_added1
    .sort_values(['matchId', 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed'], ascending = [True, False, False, False, False, 
                              False, False, False, False, False,
                             False, False, False, False, False,
                             False, False, False, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/140:
train_predictions_added1 = ( 
    X_train_added1
    .sort_values(['matchId', 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed'], ascending = [True, False, False, False, False, 
                              False, False, False, False, False,
                             False, False, False, False, False,
                             False, False, False, False, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
222/141:
train_predictions_added1 = ( 
    X_train_added1
    .sort_values(['matchId', 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed'], ascending = [True, False, False, False, False, 
                              False, False, False, False, False,
                             False, False, False, False, False,
                             False, False, False, False, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added1
222/142:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added1['teamId']
)
226/1: import pandas as pd
226/2:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
submission = pd.read_csv('../data/sample_submission.csv')
226/3: X_train.head(10)
226/4:
train_predictions = ( 
    X_train
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions
226/5: from sklearn.metrics import accuracy_score
226/6:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions['teamId']
)
226/7:
y_pred = ( 
    X_test
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    ['teamId']
    .reset_index(drop = True)
)

y_pred
226/8:
submission['winner'] = y_pred
submission.head()
226/9: submission.to_csv('../data/first_submission.csv', index = False)
226/10:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats
champ.columns
226/11:
champ_grp = champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame()
champ_grp
226/12:
X_train_added = pd.merge(X_train, champ_grp, left_on="championId", right_on="championId", how="left")
X_train_added.head(5)
226/13:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
226/14:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#Score is reduced adding the championpoints reduces from 0.504 to 0.489
226/15:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'summonerLevel', 'championPoints'], ascending = [True, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
226/16:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#it is back to 0.50375, lookslike championpoint has no additive effect.
226/17:
champion = pd.read_json('../data/champion.json')
champion
226/18:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['name']== "Akshan"]
226/19:
champion_data_info = pd.DataFrame.from_dict(champion_data['info'].values.tolist())   

champion_data_info.columns
226/20:
champion_data_stats = pd.DataFrame.from_dict(champion_data['stats'].values.tolist())   

champion_data_stats.head(5)
226/21:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape #162, 31
champ_spread.columns.to_list()
226/22:
champ_spread1 = champ_spread.drop(columns = ['version',
 'id',
 
 'title',
 'blurb',
 'info',
 'image',
 'tags',
 'partype',
 'stats']).rename(columns = {'key': 'championId',
                             'name': 'championName'})
226/23: X_train_added.info()
226/24:


champ_spread1['championId'] = champ_spread1['championId'].astype('int64')
champ_spread1.info()
226/25:
X_train_added1 = pd.merge(X_train_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_train_added1.head(5)
X_train_added1.columns.to_list()
226/26:
teamposition = pd.read_csv('../data/teamPositions.csv') #matchId    participantId   teamPositio

X_train_added2 = pd.merge(X_train_added1, teamposition, 
                          left_on=["matchId", "participantId"], 
                          right_on=["matchId", "participantId"], 
                          how="left")
X_train_added2.head(5)
X_train_added2.columns.to_list()
226/27:
train_predictions_added1 = ( 
    X_train_added1
    .sort_values(['matchId', 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed'], ascending = [True, False, False, False, False, 
                              False, False, False, False, False,
                             False, False, False, False, False,
                             False, False, False, False, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added1
227/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from scipy.cluster.hierarchy import linkage, dendrogram

from sklearn.preprocessing import LabelEncoder

from scipy.spatial.distance import pdist, squareform
227/2:
votes = pd.read_csv('data/votes_117_2.csv').dropna()
votes.head()
227/3:
#pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])
votes1 = pd.melt(votes, id_vars=['senator'], value_vars=['vote_num_001'])
votes1.head()
227/4:
#pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])
votes = pd.melt(votes, id_vars=['senator'])
votes.head()
227/5: le = LabelEncoder().fit(votes['value'])
227/6: votes['value'] = le.transform(votes['value'])
227/7:
votes['value'] = le.transform(votes['value'])
votes
227/8: votes['value'] = le.transform(votes['value'])
227/9: le = LabelEncoder().fit(votes['value'])
227/10: votes['value'] = le.transform(votes['value'])
227/11:
votes['value'] = le.transform(votes['value'])
votes
227/12: votes = votes.pivot(index='senator', columns='variable', value='value')
227/13: votes = votes.pivot(index = 'senator', columns = 'variable', values = 'value')
227/14:
votes = votes.pivot(index = 'senator', 
                    columns = 'variable', 
                    values = 'value')
votes
227/15:
votes = votes.pivot(index = 'senator', columns = 'variable', values = 'value')
votes
227/16:
votes['value'] = le.transform(votes['value'])
votes
228/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from scipy.cluster.hierarchy import linkage, dendrogram

from sklearn.preprocessing import LabelEncoder

from scipy.spatial.distance import pdist, squareform
228/2:
votes = pd.read_csv('data/votes_117_2.csv').dropna()
votes.head()
228/3:
#thi scan be used for melting all the columnas as value except senator.
columns = votes.drop(columns = 'senator').columns
pd.melt(votes, id_vars=['senator'], value_vars=column)
228/4:
#thi scan be used for melting all the columnas as value except senator.
# columns = votes.drop(columns = 'senator').columns
# pd.melt(votes, id_vars=['senator'], value_vars=column)
228/5:
#pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])
votes = pd.melt(votes, id_vars=['senator'])
votes.head()
228/6: le = LabelEncoder().fit(votes['value'])
228/7:
votes['value'] = le.transform(votes['value'])
votes
228/8:
votes = votes.pivot(index = 'senator', columns = 'variable', values = 'value')
votes
229/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, Lasso
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
229/2:
kc = pd.read_csv('data/kc_house_data.csv')

X = kc[['date', 'bedrooms', 'bathrooms', 'sqft_living',
       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',
       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',
       'lat', 'long', 'sqft_living15', 'sqft_lot15']].copy()
y = kc['price']

X['date'] = pd.to_datetime(X['date'])
X['sales_year'] = X['date'].dt.year

X['age_at_sale'] = X['sales_year'] - X['yr_built']
X['years_since_renovation'] = X['sales_year'] - np.max(X[['yr_built', 'yr_renovated']], axis = 1)


X = X.drop(columns = ['date', 'sales_year', 'yr_built', 'yr_renovated'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
229/3:
linreg = Pipeline(steps = [
    ('ct', ColumnTransformer(transformers = [
        ('ohe', OneHotEncoder(), ['zipcode'])
    ],
                            remainder = 'passthrough',
                            verbose_feature_names_out = False)),
    ('lr', LinearRegression())
])

linreg.fit(X_train, y_train)
229/4: print(f'R2 Score: {r2_score(y_test, linreg.predict(X_test))}')
229/5:
house_num = 0
sample = X_test.iloc[[house_num]]
229/6: print(f'Prediction: {"${:,.2f}".format(linreg.predict(sample)[0])}')
229/7:
coefficients = pd.DataFrame({'feature': linreg['ct'].get_feature_names_out(),
                             'coefficient': linreg['lr'].coef_})

coefficients
229/8: from explainer import tell_me_why_pipe
229/9:
house_num = 0
tell_me_why_pipe(linreg, X_test.iloc[[house_num]], linreg['ct'].get_feature_names_out())
229/10:
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
229/11:
ridge = Pipeline(
    steps = [
    ('ct', ColumnTransformer(transformers = [
        ('ohe', OneHotEncoder(sparse = False), ['zipcode'])
    ],
                            remainder = 'passthrough',
                            verbose_feature_names_out = False)),
        ('scaler', StandardScaler()),
        ('linreg', RidgeCV())
    ]
).fit(X_train, y_train)


lasso = Pipeline(
    steps = [
    ('ct', ColumnTransformer(transformers = [
        ('ohe', OneHotEncoder(sparse = False), ['zipcode'])
    ],
                            remainder = 'passthrough',
                            verbose_feature_names_out = False)),
        ('scaler', StandardScaler()),
        ('linreg', LassoCV())
    ]
).fit(X_train, y_train)
229/12:
print(f'Ridge R2 Score: {r2_score(y_test, ridge.predict(X_test))}')
print(f'Lasso R2 Score: {r2_score(y_test, lasso.predict(X_test))}')
229/13:
house_num = 0
tell_me_why_pipe(ridge, X_test.iloc[[house_num]], ridge['ct'].get_feature_names_out())
229/14:
house_num = 0
tell_me_why_pipe(lasso, X_test.iloc[[house_num]], ridge['ct'].get_feature_names_out())
229/15:
lasso_manual = Pipeline(steps = 
                       [
                            ('ct', ColumnTransformer(transformers = [
                                ('ohe', OneHotEncoder(sparse = False), ['zipcode'])
                            ],
                                                    remainder = 'passthrough',
                                                    verbose_feature_names_out = False)),
                           ('scaler', StandardScaler()),
                           ('linreg', Lasso(alpha = 10000))
                       ]
                       ).fit(X_train, y_train)

print(f'Percent of nonzero Coefficients: {(lasso_manual[-1].coef_ != 0).mean()}')

print(f'Lasso R2 Score: {r2_score(y_test, lasso_manual.predict(X_test))}')
229/16:
house_num = 0
tell_me_why_pipe(lasso_manual, X_test.iloc[[house_num]], ridge['ct'].get_feature_names_out())
226/28: X_train_added2
226/29:
X_test_added = pd.merge(X_test, champ_grp, left_on="championId", right_on="championId", how="left")
X_test_added.head(5)
226/30: **testing the test data to create the submission.csv**
226/31:
test_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

test_predictions_added
226/32:
test_predictions_added = ( 
    X_test_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

test_predictions_added
226/33:
sm_submission['winner'] = test_predictions_added
sm_submission.head()
226/34:
submission['winner'] = test_predictions_added
submission.head()
226/35:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [teamId']
    .reset_index(drop = True)
)

train_predictions_added
226/36:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    ['teamId']
    .reset_index(drop = True)
)

train_predictions_added
226/37:
test_predictions_added = ( 
    X_test_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [ 'teamId']
    .reset_index(drop = True)
)

test_predictions_added
226/38:
submission['winner'] = test_predictions_added
submission.head()
226/39: submission.to_csv('../data/sm1_submission.csv', index = False)
226/40:
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

import seaborn as sns
import matplotlib.pyplot as plt
226/41:
X_train_added2

X_train, X_test, X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321, stratify = y) = train_test_split(X, y, random_state = 321, stratify = y)

#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321, stratify = y)
226/42:
X = X_train_added2
y = y_train

X_train_sm1, X_test_sm1, y_train_sm1, y_test_sm1 = train_test_split(X, y, random_state = 321)

linreg_base = LinearRegression().fit(X_train_sm1, y_train_sm1)
226/43:
X = X_train_added2
# y = y_train

# X_train_sm1, X_test_sm1, y_train_sm1, y_test_sm1 = train_test_split(X, y, random_state = 321)

# linreg_base = LinearRegression().fit(X_train_sm1, y_train_sm1)# 

X_train_added2.columns
226/44:
X = X_train_added2
# y = y_train

# X_train_sm1, X_test_sm1, y_train_sm1, y_test_sm1 = train_test_split(X, y, random_state = 321)

# linreg_base = LinearRegression().fit(X_train_sm1, y_train_sm1)# 

X_train_added2.columns
X_train_added2.head(10)
226/45: X_train_added2[X_train_added2['participantId']==1]
226/46:
pd.concat([X_train_added2[X_train_added2['participantId']==1], 
          X_train_added2[X_train_added2['participantId']==2]])

# X_train_added2[X_train_added2['participantId']==1]
226/47:
pd.concat([X_train_added2[X_train_added2['participantId']==1], 
          X_train_added2[X_train_added2['participantId']==2]], axis=1)

# X_train_added2[X_train_added2['participantId']==1]
226/48:
pd.concat([X_train_added2[X_train_added2['participantId']==1], 
          X_train_added2[X_train_added2['participantId']==2]], axis=1, ignore_index=True)

# X_train_added2[X_train_added2['participantId']==1]
226/49:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True), 
          X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True)], 
          axis=1, ignore_index=True)

# X_train_added2[X_train_added2['participantId']==1]
226/50:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True)], 
          axis=1)

# X_train_added2[X_train_added2['participantId']==1]
226/51:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1)

# X_train_added2[X_train_added2['participantId']==1]
226/52:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1).columns

# X_train_added2[X_train_added2['participantId']==1]
226/53:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1).columns.tolist()

# X_train_added2[X_train_added2['participantId']==1]
232/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression, RidgeCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
232/2:
kc = pd.read_csv('../data/kc_house_data.csv')

X = kc[['date', 'bedrooms', 'bathrooms', 'sqft_living',
       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',
       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',
       'lat', 'long', 'sqft_living15', 'sqft_lot15']].copy()
y = kc['price']

X['date'] = pd.to_datetime(X['date'])
X['sales_year'] = X['date'].dt.year

X['age_at_sale'] = X['sales_year'] - X['yr_built']
X['years_since_renovation'] = X['sales_year'] - np.max(X[['yr_built', 'yr_renovated']], axis = 1)


X = X.drop(columns = ['date', 'sales_year', 'yr_built', 'yr_renovated'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
232/3:
linreg = Pipeline(
    steps = [
        ('ct', ColumnTransformer(
            transformers = [
                ('ohe', OneHotEncoder(), ['zipcode'])
            ],
            remainder = 'passthrough'
        )
        ),
        ('lr', LinearRegression())
    ]
)

linreg.fit(X_train, y_train)

print(f'R2 Score: {r2_score(y_test, linreg.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, linreg.predict(X_test))}')
232/4:
pipe = Pipeline(
    steps = [
        ('ct', ColumnTransformer(
            transformers = [
                ('ohe', OneHotEncoder(sparse = False), ['zipcode'])
            ],
            remainder = 'passthrough'
        )),
        ('scale', StandardScaler()),
        ('linreg', RidgeCV())
    ]
)

pipe.fit(X_train, y_train)

print(f'R2 Score: {r2_score(y_test, pipe.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, pipe.predict(X_test))}')
232/5:
y_train.hist()
plt.title('Untransformed');
232/6:
np.log(y_train).hist()
plt.title('Transformed');
232/7: from sklearn.compose import TransformedTargetRegressor
232/8:
pipe = Pipeline(
    steps = [
        ('ct', ColumnTransformer(
            transformers = [
                ('ohe', OneHotEncoder(sparse = False), ['zipcode'])
            ],
            remainder = 'passthrough'
        )),
        ('scale', StandardScaler()),
        ('linreg', RidgeCV())
    ]
)

ttr = TransformedTargetRegressor(
    regressor = pipe,
    func = np.log,
    inverse_func = np.exp
)

ttr.fit(X_train, y_train)

print(f'R2 Score: {r2_score(y_test, ttr.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, ttr.predict(X_test))}')
232/9:
non_zips = [x for x in X_train.columns if 'zipcode' not in x]
non_zips

X_train[non_zips].skew().sort_values(ascending = False)
232/10: from sklearn.preprocessing import PowerTransformer
232/11:
X_train['sqft_living'].hist()
plt.title('Untransformed');
232/12:
pd.Series(PowerTransformer().fit_transform(X_train[['sqft_living']])[:,0]).hist()
plt.title('Transformed');
232/13:
transformed_columns = ['sqft_living', 'sqft_lot', 'bedrooms', 
                       'sqft_basement', 'sqft_above', 
                       'sqft_living', 'sqft_living15']
232/14:
pipe = Pipeline(
    steps = [
        ('ct', ColumnTransformer(
            transformers = [
                ('ohe', OneHotEncoder(sparse = False), ['zipcode']),
                ('power', PowerTransformer(), transformed_columns)
            ],
            remainder = 'passthrough'
        )),
        ('scaler', StandardScaler()),
        ('linear', RidgeCV())
    ]
)

ttr = TransformedTargetRegressor(
    regressor = pipe,
    func = np.log,
    inverse_func = np.exp
)

ttr.fit(X_train, y_train)

print(f'R2 Score: {r2_score(y_test, ttr.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, ttr.predict(X_test))}')
232/15: from sklearn.preprocessing import MinMaxScaler
232/16:
pipe = Pipeline(
    steps = [
        ('ct', ColumnTransformer(
            transformers = [
                ('ohe', OneHotEncoder(sparse = False), ['zipcode'])
            ],
            remainder = 'passthrough'
        )),
        ('scaler', MinMaxScaler()),
        ('mlr', MLPRegressor(verbose = True))
    ]
)
232/17: pipe.fit(X_train, y_train)
232/18:
print(f'R2: {r2_score(y_test, pipe.predict(X_test))}')

print(f'MAE: {mean_absolute_error(y_test, pipe.predict(X_test))}')
226/54:
data = X_train_added2['matchId',
 'teamId',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed',
 'teamPosition']
226/55:
data = X_train_added2[['matchId',
 'teamId',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed',
 'teamPosition']]
226/56:
data = X_train_added2[['matchId',
 'teamId',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed',
 'teamPosition']]
data.head(5)
226/57:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1)

# X_train_added2[X_train_added2['participantId']==1]
226/58:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1).columns>to_list()

# X_train_added2[X_train_added2['participantId']==1]
226/59:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1).columns.to_list()

# X_train_added2[X_train_added2['participantId']==1]
226/60:
data = X_train_added2[['matchId',
 'teamId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed',
 'teamPosition']]
data.head(5)
226/61:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True).rename(columns={'mp'='mp_1'}),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1).
226/62:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True).rename(columns={'mp':'mp_1'}),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1).
226/63:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True).rename(columns={'mp':'mp_1'}),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1)
226/64:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True).rename(columns={'mp':'mp_1'}),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1).columns.to_ist()
226/65:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True).rename(columns={'mp':'mp_1'}),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1).columns.to_list()
226/66:
pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True)
          data[data['participantId']==5].reset_index(drop=True)
          data[data['participantId']==6].reset_index(drop=True)
          data[data['participantId']==7].reset_index(drop=True)
          data[data['participantId']==8].reset_index(drop=True)
          data[data['participantId']==9].reset_index(drop=True)
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1).columns.to_list()
226/67:
pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1).columns.to_list()
226/68:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1)

# X_train_added2[X_train_added2['participantId']==1]
226/69:
data = X_train_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed',
 'teamPosition']]
data.head(5)
226/70:
pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1).columns.to_list()
226/71:
pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1)
226/72:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1)
226/73: y_train.head(5)
226/74:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1).merge(y_train, on='matchId', how='left')
226/75:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1).join(y_train)
226/76:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1)
226/77:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1)
data.head()
226/78:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1)
data.shape
226/79:
pd.concat([X_train_added2[X_train_added2['participantId']==1].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==2].reset_index(drop=True),
           X_train_added2[X_train_added2['participantId']==3].reset_index(drop=True),
          X_train_added2[X_train_added2['participantId']==4].reset_index(drop=True)], 
          axis=1)

# X_train_added2[X_train_added2['participantId']==1]
226/80:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1)
data_x.shape
226/81:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True),
           data[data['participantId']==3].reset_index(drop=True),
          data[data['participantId']==4].reset_index(drop=True),
          data[data['participantId']==5].reset_index(drop=True),
          data[data['participantId']==6].reset_index(drop=True),
          data[data['participantId']==7].reset_index(drop=True),
          data[data['participantId']==8].reset_index(drop=True),
          data[data['participantId']==9].reset_index(drop=True),
          data[data['participantId']==10].reset_index(drop=True)], 
          axis=1)
data_x.shape #8000, 270
data_x.columns.to_list()
226/82:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 270
data_x.columns.to_list()
226/83:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 270
#data_x.columns.to_list()
226/84:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 240
data_x.columns.to_list()
226/85:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 240
data_x.columns.to_list()
226/86:
X = data_x
y= y_train

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
226/87: logreg = LogisticRegression().fit(X_train, y_train)
226/88:
data = X_train_added2[['matchId',
 'teamId',
#'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed',
 'teamPosition']]
data.head(5)
226/89:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 240
data_x.columns.to_list()
226/90:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 240
data_x.columns.to_list()
226/91:
data = X_train_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed',
 #'teamPosition']]
data.head(5)
226/92:
data = X_train_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']
 #'teamPosition']
        
data.head(5)
226/93:
data = X_train_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
data.head(5)
226/94:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 240
data_x.columns.to_list()
226/95:
X = data_x
y= y_train

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
226/96:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 240
# data_x.columns.to_list()
226/97:
y_train.head(5)
y_trainshape
226/98:
y_train.head(5)
y_train.shape
226/99:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
submission = pd.read_csv('../data/sample_submission.csv')
226/100: y_train.shape
226/101:
y_train.head(5)
y_train.shape
226/102:
X = data_x
y= y_train

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
226/103: logreg = LogisticRegression().fit(X_train, y_train)
226/104: X_train.isna().sum()
226/105: logreg = LogisticRegression().fit(X_train.fillna(0), y_train)
226/106:
y_train.head(5)
y_train.shape
226/107:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
submission = pd.read_csv('../data/sample_submission.csv')
226/108: y_train.shape
226/109:
y_train.head(5)
y_train.shape
226/110:
X = data_x
y = y_train.drop(['matchId'])

X_train, X_test, y_train1, y_test = train_test_split(X, y, random_state = 321)
226/111:
X = data_x
y = y_train.drop(columns = ['matchId'])

X_train, X_test, y_train1, y_test = train_test_split(X, y, random_state = 321)
226/112: logreg = LogisticRegression().fit(X_train.fillna(0), y_train)
226/113:
X = data_x
y = y_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
226/114: logreg = LogisticRegression().fit(X_train.fillna(0), y_train)
226/115: from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
226/116: logreg.predict(X_test)
226/117: logreg.predict(X_test.fillna(0))
226/118: accuracy_score(y_test, logreg.predict(X_test))
226/119: y_pred = logreg.predict(X_test.fillna(0))
226/120: accuracy_score(y_test, y_pred)
226/121:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 240
# data_x.columns.to_list()
data_x.head(10)
226/122: accuracy_score(y_test, y_pred)
226/123: logreg.intercept_
226/124: **Having the teamdata all the values we are still at 51>3% accuracy level we should add the team position and HotEncoding the positions**
226/125: # **Having the teamdata all the values we are still at 51>3% accuracy level we should add the team position and HotEncoding the positions**
226/126: data.columns
226/127: data_x.columns
226/128: read_csv('../data/last_frame_valuesALL.csv')
226/129: pd.read_csv('../data/last_frame_valuesALL.csv')
226/130: data.head(5)
226/131: data.shape
226/132:
data.shape
data.columns.to()
226/133:
data.shape
data.columns.to_list()
226/134: last_aj=pd.read_csv('../data/last_frame_valuesALL.csv')
226/135:
data.shape
data.columns.to_list()
pd.merge(data, last_aj, left_on=["matchID", "paarticipantID"], right_on=["matchID", "paarticipantID"], how="left")
226/136:
data.shape
data.columns.to_list()
pd.merge(data, last_aj, left_on=["matchId", "paarticipantID"], right_on=["matchId", "paarticipantID"], how="left")
226/137:
data.shape
data.columns.to_list()
pd.merge(data, last_aj, left_on=["matchId", "paarticipantId"], right_on=["matchId", "paarticipantId"], how="left")
226/138:
data.shape
data.columns.to_list()
pd.merge(data, last_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
226/139:
data.shape
data.columns.to_list()
data_aj = pd.merge(data, last_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
226/140:
data.shape
data.columns.to_list()
data_aj = pd.merge(data, last_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
data_aj.columns.to_list()
226/141:
data.shape
data.columns.to_list()
data_aj = pd.merge(data, last_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
data_aj.columns.to_list()
data_aj.shape
226/142:
data_aj=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
226/143:
data_aj_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
226/144:
data.shape
data.columns.to_list()
data_aj = pd.merge(data, last_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
data_aj.columns.to_list()
data_aj.shape
226/145:
data_aj_x=pd.concat([data_aj[data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data_aj[data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data_aj[data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data_aj[data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         data_aj[data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data_aj[data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data_aj[data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
         data_aj[data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data_aj[data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
         data_aj[data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
226/146:
data_aj_x=pd.concat([data_aj[data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data_aj[data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data_aj[data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data_aj[data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         data_aj[data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data_aj[data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data_aj[data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
         data_aj[data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data_aj[data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
         data_aj[data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_aj_x.shape
226/147: y_train.shape
226/148:
y_train.shape (6000, 1)
y_train = pd.read_csv('../data/train_winners.csv')
226/149:
y_train.shape #(6000, 1)
y_train = pd.read_csv('../data/train_winners.csv')
226/150:
y_train.shape #(6000, 1)
ydata_train = pd.read_csv('../data/train_winners.csv')

# y_train.shape
226/151:
y_train.shape #(6000, 1)
ydata_train = pd.read_csv('../data/train_winners.csv')

ydata_train.shape
226/152:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
226/153: logreg_aj = LogisticRegression().fit(X_train.fillna(0), y_train)
226/154: y_pred_aj = logreg_aj.predict(X_test.fillna(0))
226/155: accuracy_score(y_test, y_pred_aj)
226/156:
last_aj=pd.read_csv('../data/last_frame_valuesALL.csv')

last_aj.columns()
226/157:
last_aj=pd.read_csv('../data/last_frame_valuesALL.csv')

last_aj.columns
226/158:
last_aj=pd.read_csv('../data/last_frame_valuesALL.csv')

last_aj.head()
226/159:
last_aj=pd.read_csv('../data/last_frame_valuesALL.csv')

last_aj.shape()
226/160:
last_aj=pd.read_csv('../data/last_frame_valuesALL.csv')

last_aj.shape
226/161: X.shape
226/162:
X.shape
X_train.shape
226/163: logreg_aj = LogisticRegression().fit(X_train.fillna(0), y_train)
226/164: y_pred_aj = logreg_aj.predict(X_test.fillna(0))
226/165: accuracy_score(y_test, y_pred_aj)
226/166:
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
226/167: ?LogisticRegression()
226/168: ?LogisticRegression
226/169:
logReg_aj_scaler = Pipeline(
    steps = [
            ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.01))
    ]
).fit(X_train, y_train)
226/170:
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
226/171:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.01))
    ]
).fit(X_train, y_train)
226/172: scal_pred = logReg_aj_scaler.predict(X_test)
226/173: accuracy_score(y_test, scal_pred)
226/174:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.001))
    ]
).fit(X_train, y_train)
226/175: scal_pred = logReg_aj_scaler.predict(X_test)
226/176: accuracy_score(y_test, scal_pred)
226/177:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.0001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/178: scal_pred = logReg_aj_scaler.predict(X_test)
226/179: accuracy_score(y_test, scal_pred)
226/180:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.00001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/181: scal_pred = logReg_aj_scaler.predict(X_test)
226/182: accuracy_score(y_test, scal_pred)
226/183:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.0001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/184: scal_pred = logReg_aj_scaler.predict(X_test)
226/185: accuracy_score(y_test, scal_pred)
226/186:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.01)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/187: scal_pred = logReg_aj_scaler.predict(X_test)
226/188: accuracy_score(y_test, scal_pred)
226/189:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/190: scal_pred = logReg_aj_scaler.predict(X_test)
226/191: accuracy_score(y_test, scal_pred)
226/192:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.0001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/193: scal_pred = logReg_aj_scaler.predict(X_test)
226/194: accuracy_score(y_test, scal_pred)
226/195:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.00001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/196: scal_pred = logReg_aj_scaler.predict(X_test)
226/197: accuracy_score(y_test, scal_pred)
226/198:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.00015)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/199: scal_pred = logReg_aj_scaler.predict(X_test)
226/200: accuracy_score(y_test, scal_pred)
226/201:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.0001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/202: scal_pred = logReg_aj_scaler.predict(X_test)
226/203: accuracy_score(y_test, scal_pred)
226/204: ?train_test_split
226/205:
# look at coefficient

pd.DataFrame({'feature': logReg_aj_scaler.get_feature_names_out(),
                             'coefficient': logReg_aj_scaler['log_reg'].coef_})
226/206:
# look at coefficient

pd.DataFrame({'feature': logReg_aj_scaler['scaler'].get_feature_names_out(),
                             'coefficient': logReg_aj_scaler['log_reg'].coef_})
226/207: logReg_aj_scaler['scaler'].get_feature_names_out()
226/208:
# look at coefficient

pd.DataFrame({'feature': logReg_aj_scaler['scaler'].feature_names_in_,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_})
226/209:
# look at coefficient

pd.DataFrame({'feature': logReg_aj_scaler['scaler'].feature_names_in__,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_})
226/210:
logReg_aj_scaler['scaler'].get_feature_names_out()
logReg_aj_scaler['log_reg'].coef_
226/211:
logReg_aj_scaler['scaler'].get_feature_names_out()
logReg_aj_scaler['log_reg'].coef_.shape
226/212:
logReg_aj_scaler['scaler'].get_feature_names_out()
logReg_aj_scaler['log_reg'].coef_.flatten()
226/213:
# look at coefficient

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()})
226/214:
# look at coefficient

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values()
226/215:
# look at coefficient

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', ascending =False)
226/216:
# look at coefficient

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).head(20)
226/217:
# look at coefficient

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).head(50)
226/218:
# look at coefficient

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =True).head(50)
226/219:
# look at coefficient

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).head(50)
226/220:
# look at coefficient

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).iloc[; 51:100]
226/221:
# look at coefficient

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).iloc[51:100]
226/222:
# look at coefficient

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).iloc[0:20]
226/223:
data.shape
data.columns.to_list()
data_aj = pd.merge(data, last_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
data_aj.columns.to_list()
data_aj.shape
data_aj.head(5)
226/224:
data.shape
data.columns.to_list()
data_aj = pd.merge(data, last_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
data_aj.columns.to_list()
# data_aj.shape
# data_aj.head(5)
226/225: data_x.columns.to_list()
226/226:
# look at coefficient

logReg_aj_scaler_coefficeinct = pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False)
226/227:
y_train.head(5)
y_train.shape
226/228: y1_train = pd.read_csv('../data/train_winners.csv')
226/229:
X = data_x
y = y1_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
226/230:
X = data_x
y = y1_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)

X_train.shape
226/231:
X = data_x
y = y1_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)

X_test.shape
226/232:
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.0001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/233: accuracy_score(y_test, logReg_scaler.predict(X_test))
226/234:
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.00001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/235: logReg_scaler.predict(X_test)
226/236: accuracy_score(y_test, logReg_scaler.predict(X_test))
226/237:
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.01)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/238:
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.01)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)

accuracy_score(y_test, logReg_scaler.predict(X_test))
226/239:
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)

accuracy_score(y_test, logReg_scaler.predict(X_test))
226/240:
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.0001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)

accuracy_score(y_test, logReg_scaler.predict(X_test))
226/241:
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)

accuracy_score(y_test, logReg_scaler.predict(X_test))
226/242: accuracy_score(y_test, logreg1)
226/243:
# fit the model on X_test data
logreg1 = logReg_scaler.predict(X_test)
226/244:
#finding the accuracy score

accuracy_score(y_test, logreg1)
226/245:
#Which features are the best predictor

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False)
226/246:
#Which features are the best predictor

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False)
226/247:
#Which features are the best predictor

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).head[20]
226/248:
#Which features are the best predictor

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).head(20)
226/249: y_train.shape
226/250:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
submission = pd.read_csv('../data/sample_submission.csv')
226/251: y_train.shape
226/252:
champion_data_info = pd.DataFrame.from_dict(champion_data['info'].values.tolist())   

champion_data_info.columns
226/253:
X_test_added1 = pd.merge(X_test_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_test_added1.head(5)
X_test_added1.columns.to_list()
226/254:
teamposition = pd.read_csv('../data/teamPositions.csv') #matchId    participantId   teamPosition

X_train_added2 = pd.merge(X_train_added1, teamposition, 
                          left_on=["matchId", "participantId"], 
                          right_on=["matchId", "participantId"], 
                          how="left")
X_train_added2.head(5)
X_train_added2.columns.to_list()

X_test_added2 = pd.merge(X_test_added1, teamposition, 
                          left_on=["matchId", "participantId"], 
                          right_on=["matchId", "participantId"], 
                          how="left")
226/255:
#modifing test data

test_data = X_test_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
test_data.head(5)
226/256:
#modifing test data

test_data = X_test_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
test_data
226/257:
#modifing test data

test_data = X_test_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
test_data.shape 20000, 26
226/258:
#modifing test data

test_data = X_test_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
test_data.shape #20000, 26
226/259:
#Reading the time stamp files from aj for test data

last_test_aj=pd.read_csv('../data/test_last_frame_values_ALL.csv')

last_test_aj.shape #80000, 47
226/260:
#merging the test_aj to the test_data

test_data_aj = pd.merge(test_data, last_test_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
test_data_aj.shape
226/261:
test_data_aj_x=pd.concat([test_data_aj[test_data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           test_data_aj[test_data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
          test_data_aj[test_data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          test_data_aj[test_data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         test_data_aj[test_data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
         test_data_aj[test_data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          test_data_aj[test_data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
       test_data_aj[test_data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
         test_data_aj[test_data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
      test_data_aj[test_data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
test_data_aj_x.shape (8000, 681)
226/262:
test_data_aj_x=pd.concat([test_data_aj[test_data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           test_data_aj[test_data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
          test_data_aj[test_data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          test_data_aj[test_data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         test_data_aj[test_data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
         test_data_aj[test_data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          test_data_aj[test_data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
       test_data_aj[test_data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
         test_data_aj[test_data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
      test_data_aj[test_data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
test_data_aj_x.shape #(8000, 681)
226/263:
X_test = test_data_aj_x

y_pred = logReg_aj_scaler.predict(X_test)
226/264:
X_test = test_data_aj_x

y_pred = logReg_aj_scaler.predict(X_test)

y_pred.shape
226/265:
X_test = test_data_aj_x

y_pred = logReg_aj_scaler.predict(X_test)

y_pred
226/266:
submission['winner'] = y_pred
submission.head()
226/267: submission.to_csv('../data/sm2_submission.csv', index = False) #accuracy score on train set was 70.15%
226/268:
data_aj_x=pd.concat([data_aj[data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data_aj[data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data_aj[data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data_aj[data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         data_aj[data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data_aj[data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data_aj[data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
         data_aj[data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data_aj[data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
         data_aj[data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_aj_x.shape #(8000, 681)
data_aj_x.head(5)
226/269:
**trying the random forest**

from sklearn.ensemble import RandomForestClassifier

# instantiate the classifier 

rfc = RandomForestClassifier(random_state=0)
226/270:

from sklearn.ensemble import RandomForestClassifier

# instantiate the classifier 

rfc = RandomForestClassifier(random_state=0)
226/271: X_train.shape
226/272: y_train.shape
226/273: y_train.shape
226/274:
X.shape
X_train.shape
226/275:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
submission = pd.read_csv('../data/sample_submission.csv')
226/276: from sklearn.metrics import accuracy_score
226/277: import pandas as pd
226/278:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
submission = pd.read_csv('../data/sample_submission.csv')
226/279: y_train.shape
226/280: X_train.head(10)
226/281:
train_predictions = ( 
    X_train
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions
226/282: from sklearn.metrics import accuracy_score
226/283:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions['teamId']
)
226/284:
y_pred = ( 
    X_test
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    ['teamId']
    .reset_index(drop = True)
)

y_pred
226/285:
submission['winner'] = y_pred
submission.head()
226/286: submission.to_csv('../data/first_submission.csv', index = False)
226/287:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats
champ.columns
226/288:
champ_grp = champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame()
champ_grp
226/289:
X_train_added = pd.merge(X_train, champ_grp, left_on="championId", right_on="championId", how="left")
X_train_added.head(5)
226/290:
X_test_added = pd.merge(X_test, champ_grp, left_on="championId", right_on="championId", how="left")
X_test_added.head(5)
226/291:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    ['teamId']
    .reset_index(drop = True)
)

train_predictions_added
226/292:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#Score is reduced adding the championpoints reduces from 0.504 to 0.489
226/293: submission.to_csv('../data/sm1_submission.csv', index = False)
226/294:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'summonerLevel', 'championPoints'], ascending = [True, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
226/295:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#it is back to 0.50375, lookslike championpoint has no additive effect.
226/296:
champion = pd.read_json('../data/champion.json')
champion
226/297:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['name']== "Akshan"]
226/298:
champion_data_info = pd.DataFrame.from_dict(champion_data['info'].values.tolist())   

champion_data_info.columns
226/299:
champion_data_stats = pd.DataFrame.from_dict(champion_data['stats'].values.tolist())   

champion_data_stats.head(5)
226/300:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape #162, 31
champ_spread.columns.to_list()
226/301: X_train_added.info()
226/302:


champ_spread1['championId'] = champ_spread1['championId'].astype('int64')
champ_spread1.info()
226/303:
X_train_added1 = pd.merge(X_train_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_train_added1.head(5)
X_train_added1.columns.to_list()
226/304:
X_test_added1 = pd.merge(X_test_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_test_added1.head(5)
X_test_added1.columns.to_list()
226/305:
teamposition = pd.read_csv('../data/teamPositions.csv') #matchId    participantId   teamPosition

X_train_added2 = pd.merge(X_train_added1, teamposition, 
                          left_on=["matchId", "participantId"], 
                          right_on=["matchId", "participantId"], 
                          how="left")
X_train_added2.head(5)
X_train_added2.columns.to_list()

X_test_added2 = pd.merge(X_test_added1, teamposition, 
                          left_on=["matchId", "participantId"], 
                          right_on=["matchId", "participantId"], 
                          how="left")
226/306:
train_predictions_added1 = ( 
    X_train_added1
    .sort_values(['matchId', 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed'], ascending = [True, False, False, False, False, 
                              False, False, False, False, False,
                             False, False, False, False, False,
                             False, False, False, False, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added1
226/307:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added1['teamId']
)
226/308:
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

import seaborn as sns
import matplotlib.pyplot as plt
226/309:
X = X_train_added2
# y = y_train

# X_train_sm1, X_test_sm1, y_train_sm1, y_test_sm1 = train_test_split(X, y, random_state = 321)

# linreg_base = LinearRegression().fit(X_train_sm1, y_train_sm1)# 

X_train_added2.columns
X_train_added2.head(10)
226/310:
data = X_train_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
data.head(5)
226/311:
#modifing test data

test_data = X_test_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
test_data.shape #20000, 26
226/312:
y_train.head(5)
y_train.shape
226/313:
y_train.head(5)
y_train.shape
226/314:
X = data_x
y = y_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
226/315:
y_train.head(5)
y_train.shape
226/316: X_train.isna().sum()
226/317: logreg = LogisticRegression().fit(X_train.fillna(0), y_train)
226/318: from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
226/319: y_pred = logreg.predict(X_test.fillna(0))
226/320: accuracy_score(y_test, y_pred)
226/321: y1_train = pd.read_csv('../data/train_winners.csv')
226/322:
X = data_x
y = y1_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)

X_test.shape
226/323:
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
226/324:
# fit the model on X_test data
logreg1 = logReg_scaler.predict(X_test)
226/325:
#finding the accuracy score

accuracy_score(y_test, logreg1)
226/326:
#Which features are the best predictor

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).head(20)
226/327: data_x.columns.to_list()
226/328:
last_aj=pd.read_csv('../data/last_frame_valuesALL.csv')

last_aj.shape #80000, 47
226/329:
data.shape
data.columns.to_list()
data_aj = pd.merge(data, last_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
data_aj.columns.to_list()
# data_aj.shape
# data_aj.head(5)
226/330:
data_aj_x=pd.concat([data_aj[data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data_aj[data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data_aj[data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data_aj[data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         data_aj[data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data_aj[data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data_aj[data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
         data_aj[data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data_aj[data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
         data_aj[data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_aj_x.shape #(8000, 681)
data_aj_x.head(5)
226/331:
data_aj_x=pd.concat([data_aj[data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data_aj[data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data_aj[data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data_aj[data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         data_aj[data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data_aj[data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data_aj[data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
         data_aj[data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data_aj[data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
         data_aj[data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_aj_x.shape #(8000, 681)
# data_aj_x.head(5)
226/332:
# rereading the y_train data because it becomes 6000 from 8000 upon split in previous model, 
# so rereading and calling it ydata_train

# y_train.shape #(6000, 1)
ydata_train = pd.read_csv('../data/train_winners.csv')

ydata_train.shape
226/333:
# rereading the y_train data because it becomes 6000 from 8000 upon split in previous model, 
# so rereading and calling it ydata_train

# y_train.shape #(6000, 1)
ydata_train = pd.read_csv('../data/train_winners.csv')

ydata_train.shape
226/334:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.
226/335:
X.shape
X_train.shape
226/336: y_train.shape
226/337:
y_train.shape
X_train.shape
226/338:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/339:
y_train.shape
X_train.shape
X_test.shape
226/340:
y_train.shape
X_train.shape
X_test.shape
y_test.shape
226/341:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
226/342: **Slight improvement on the score it is now 70.5%**
226/343: print('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))
226/344:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=0.01)) # 
    ]
).fit(X_train, y_train)
226/345:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=10)) # 
    ]
).fit(X_train, y_train)
226/346:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=10)) # 
    ]
).fit(X_train, y_train)
226/347:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
226/348: print('Model accuracy score with rfc model with random state(0): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))
226/349: print('Model accuracy score with rfc model with random state(10): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))
226/350:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/351:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
226/352: print('Model accuracy score with rfc model with random state(0): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))
226/353: from sklearn.neural_network import MLPRegressor
226/354:
y_train.shape
X_train.shape
# X_test.shape
# y_test.shape
226/355:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPRegressor(verbose = True))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/356: from sklearn.preprocessing import MinMaxScaler
226/357:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPRegressor(verbose = True))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/358:
#fit the model on test data and get accuracy score

nn_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, nn_pred)
226/359: print('Model accuracy score with neural network model with verbose =True: {0:0.4f}'. format(accuracy_score(y_test, nn_pred)))
226/360:
print('Model accuracy score with neural network model with verbose =True: {0:0.4f}'. format(accuracy_score(y_test, nn_pred)))

print(f'R2 Score: {r2_score(y_test, nn_pred)}')
print(f'MAE: {mean_absolute_error(y_test, nn_pred)}')
226/361:
#fit the model on test data and get accuracy score

nn_pred = nn_aj_scaler.predict(X_test)
accuracy_score(y_test, nn_pred)
226/362:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPRegressor(verbose = True))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/363:
#fit the model on test data and get accuracy score

nn_pred = nn_aj_scaler.predict(X_test)
# accuracy_score(y_test, nn_pred)
226/364:
#fit the model on test data and get accuracy score

nn_pred = nn_aj_scaler.predict(X_test)
accuracy_score(y_test, nn_pred)
226/365: y_test
226/366: nn_pred
226/367: len(nn_pred)
226/368: rfc_pred
226/369:
y_train.shape
X_train.shape
# X_test.shape
y_test.shape
226/370: y_test
226/371: rfc_pred
226/372: from sklearn.metrics import classification_report
226/373: print(classification_report(y_test, nn_pred))
226/374: nn_pred
226/375: nn_pred.type
226/376: nn_pred.type()
226/377: nn_pred
226/378:
#fit the model on test data and get accuracy score

nn_pred = nn_aj_scaler.predict(X_test)
# accuracy_score(y_test, nn_pred)
226/379:
print(f'R2 Score: {r2_score(y_test, nn_aj_scaler.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, nn_aj_scaler.predict(X_test))}')
226/380: # print(classification_report(y_test, nn_pred))
226/381:
from sklearn.metrics import r2_score, mean_absolute_error

print(f'R2 Score: {r2_score(y_test, nn_aj_scaler.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, nn_aj_scaler.predict(X_test))}')
226/382: from sklearn.neural_network import MLPRegressor MLPClassifier
226/383: from sklearn.neural_network import MLPRegressor, MLPClassifier
226/384:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/385:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/386:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/387:
#fit the model on test data and get accuracy score

# nn_pred1 = nn_aj_scaler1.predict(X_test)
# accuracy_score(y_test, nn_pred)

X_test.shape
226/388:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
# accuracy_score(y_test, nn_pred)

# X_test.shape
226/389:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred)

# X_test.shape
226/390:
print(f'R2 Score: {r2_score(y_test, nn_aj_scaler1.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, nn_aj_scaler1.predict(X_test))}')
226/391:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
226/392:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/393:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
226/394:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 1))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/395:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
226/396:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 10))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/397:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
226/398:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 0.1))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/399:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
226/400:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 100)) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/401:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
#alpha: 65.9(0.01) 70(1)
226/402:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 5)) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/403:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
#alpha: 65.9(0.01) 70(1) 51.2(100)
226/404:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 2)) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/405:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
#alpha: 65.9(0.01) 70(1) 51.2(100) 70.2(5)
226/406:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 4)) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/407:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
#alpha: 65.9(0.01) 70(1) 51.2(100) 70.2(5) 70.05(2)
226/408:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 4,
                            hidden_layer_sizes = (100, 100, 100)) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/409:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 4,
                             hidden_layer_sizes = (100, 100, 100))) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/410:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
#alpha: 65.9(0.01) 70(1) 51.2(100) 70.2(5) 70.05(2) 71.05(4)
226/411:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
#alpha: 65.9(0.01) 70(1) 51.2(100) 70.2(5) 70.05(2) 71.05(4)

#adding the hidden layer the score dropped from 71 to 70.75
226/412:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 4,
                             hidden_layer_sizes = (500, 500, 500))) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/413:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
#alpha: 65.9(0.01) 70(1) 51.2(100) 70.2(5) 70.05(2) 71.05(4)

#adding the hidden layer the score dropped from 71 to 70.75
226/414:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 4
                             )) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/415:
nn_pred1 = nn_aj_scaler.predict(X_test)
accuracy_score(y_test, nn_pred1)
226/416:
#apply the rfc model on the 
X_test = test_data_aj_x

y_pred = rfc_aj_scaler.predict(X_test)

y_pred
226/417:
submission['winner'] = y_pred
submission.head()
226/418: submission.to_csv('../data/sm_rfc_submission.csv', index = False) #accuracy score on train set was 70.5%
226/419:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 1
                             )) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/420:
nn_pred1 = nn_aj_scaler.predict(X_test)
accuracy_score(y_test, nn_pred1)
226/421:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 5
                             )) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
226/422:
nn_pred1 = nn_aj_scaler.predict(X_test)
accuracy_score(y_test, nn_pred1)
226/423:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.
226/424:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.

X_train.shape
226/425:
# import SVC classifier
from sklearn.svm import SVC


# import metrics to compute accuracy
from sklearn.metrics import accuracy_score


# instantiate classifier with default hyperparameters
svc=SVC() 


# fit classifier to training set
svc.fit(X_train,y_train)


# make predictions on test set
y_pred=svc.predict(X_test)


# compute and print accuracy score
print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
226/426:
# import SVC classifier
from sklearn.svm import SVC


# import metrics to compute accuracy
from sklearn.metrics import accuracy_score

nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('svc', SVC())) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)

# instantiate classifier with default hyperparameters
# svc=SVC() 


# fit classifier to training set
svc.fit(X_train,y_train)


# make predictions on test set
y_pred=svc.predict(X_test)


# compute and print accuracy score
print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
226/427:
# import SVC classifier
from sklearn.svm import SVC


# import metrics to compute accuracy
from sklearn.metrics import accuracy_score

nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('svc', SVC()) 
    ]
).fit(X_train, y_train)

# instantiate classifier with default hyperparameters
# svc=SVC() 


# fit classifier to training set
# svc.fit(X_train,y_train)


# make predictions on test set
# y_pred=svc.predict(X_test)


# compute and print accuracy score
print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
226/428:
# import SVC classifier
from sklearn.svm import SVC


# import metrics to compute accuracy
from sklearn.metrics import accuracy_score

nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('svc', SVC()) 
    ]
).fit(X_train, y_train)

# instantiate classifier with default hyperparameters
# svc=SVC() 


# fit classifier to training set
# svc.fit(X_train,y_train)


# make predictions on test set
y_pred=svc.predict(X_test)


# compute and print accuracy score
print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
233/1: import pandas as pd
233/2: import pandas as pd
233/3: pd.read_csv("../data/J1939Faults.csv")
233/4: faults = pd.read_csv("../data/J1939Faults.csv")
233/5: pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
233/6: diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
233/7: pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
233/8:
diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
diagnostic.shape
226/429:
# import SVC classifier
from sklearn.svm import SVC


# import metrics to compute accuracy
from sklearn.metrics import accuracy_score

svm_model = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('svc', SVC()) 
    ]
).fit(X_train, y_train)

# instantiate classifier with default hyperparameters
# svc=SVC() 


# fit classifier to training set
# svc.fit(X_train,y_train)


# make predictions on test set
# y_pred=svc.predict(X_test)


# compute and print accuracy score
# print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
233/9: import pandas as pd
233/10:
faults = pd.read_csv("../data/J1939Faults.csv")
faults.shape
233/11:
faults = pd.read_csv("../data/J1939Faults.csv")
faults.shape #(1187335, 20)
faults.info()
233/12:
diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
diagnostic.shape
diagnostic.info
238/1: diagnostics.head(5)
238/2: diagnostic.head(5)
238/3: diagnostic.head(5)
238/4:
diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
diagnostic.shape
diagnostic.info
238/5:
diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
diagnostic.shape
diagnostic.info
238/6:
diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
# diagnostic.shape
# diagnostic.info
239/1: import pandas as pd
239/2:
faults = pd.read_csv("../data/J1939Faults.csv")
faults.shape #(1187335, 20)
faults.info()
239/3:
diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
# diagnostic.shape
# diagnostic.info
239/4: diagnostic.head(5)
239/5: pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
239/6: service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
239/7:
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
service.shape
239/8: service.head(5)
239/9: diagnostic.info
239/10: faults.isna().sum()
239/11: diagnostics.isna().sum()
239/12: diagnostic.isna().sum()
239/13: diagnostic.dtype()
239/14: diagnostic.dtype
239/15: diagnostic.dtype
239/16: diagnostic.dtypes
239/17: diagnostic['value'].value_counts()
239/18: diagnostic['Value'].value_counts()
239/19: diagnostic['Value'].value_counts().to_DataFrame()
239/20: diagnostic['Value'].value_counts().to_frame()
239/21: diagnostic['Value'].value_counts().to_frame().reset_index()
239/22: diagnostic['Value'].value_counts().to_frame().reset_index().head(50)
239/23: diagnostic['Value'].value_counts().to_frame().reset_index().head(50)
239/24: diagnostic['Value'].value_counts().to_frame().reset_index().tail(50)
239/25: diagnostic['Value'].value_counts().to_frame().reset_index()
239/26:
diagnostic['Value'].value_counts().to_frame().reset_index()
12821626 - 1009465
239/27:
diagnostic['Value'].value_counts().to_frame().reset_index()
# 12821626 - 1009465
239/28:
diagnostic['Value'].value_counts().to_frame().reset_index().head(50)
# 12821626 - 1009465
239/29:
diagnostic['Value'].value_counts().to_frame().reset_index().tail(50)
# 12821626 - 1009465
239/30:
diagnostic['Value'].value_counts().to_frame().reset_index().head(50)
# 12821626 - 1009465
239/31:
import pandas as pd
from datetime import datetime
239/32: faults.head(5)
239/33: diagnostic['FaultId'].value_counts().to_frame().reset_index().head(50)
239/34: diagnostic['FaultId'].value_counts().to_frame().reset_index()
239/35: diagnostic['Name'].value_counts().to_frame().reset_index()
239/36:
# faults = pd.read_csv("../data/J1939Faults.csv")

faults = pd.read_csv("../data/J1939Faults.csv", 
                    parse_dates=['EventTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
239/37: faults.head(5)
239/38:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults = failts.drop['actionDescription' 'faultValue']
239/39:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults.drop['actionDescription' 'faultValue']
239/40:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults.drop[['actionDescription' 'faultValue']]
239/41:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults.drop[['actionDescription', 'faultValue']]
239/42:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults.drop['actionDescription', 'faultValue']
239/43:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults.drop(['actionDescription', 'faultValue'])
239/44:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults.drop(columns = ['actionDescription', 'faultValue'])
239/45:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults = faults.drop(columns = ['actionDescription', 'faultValue'])
faults
239/46: faults.loc[faults['spn'] == 5246]
239/47:
#cleaning the faults data to hav equipmentID less than 5

fautls_equipid = faults[faults['EquipmentID'].str.len() <= 5]
239/48:
#cleaning the faults data to hav equipmentID less than 5

faults_equipid = faults[faults['EquipmentID'].str.len() <= 5]
faults_equipid.shape
239/49:
#cleaning the faults data to hav equipmentID less than 5
#faults = 1187335

faults_equipid = faults[faults['EquipmentID'].str.len() <= 5]
faults_equipid.shape
239/50:
# faults = pd.read_csv("../data/J1939Faults.csv")

faults = pd.read_csv("../data/J1939Faults.csv", 
                    parse_dates=['EventTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
239/51:
# faults = pd.read_csv("../data/J1939Faults.csv")

faults = pd.read_csv("../data/J1939Faults.csv", 
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
239/52: faults.isna().sum()
239/53:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults = faults.drop(columns = ['actionDescription', 'faultValue'])
faults
239/54:
#cleaning the faults data to hav equipmentID less than 5
#faults = 1187335

# faults_equipid = faults[faults['EquipmentID'].str.len() <= 5]
faults_equipid.shape #65202,18

faults[faults['EquipmentID'].str.len() <= 5]
239/55:
# faults = pd.read_csv("../data/J1939Faults.csv")

faults = pd.read_csv("../data/J1939Faults.csv", 
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
239/56: faults[faults['EquipmentID'].str.len() <= 5]
239/57: faults[faults['EquipmentID'].str.len() > 5]
239/58: faults[faults['EquipmentID'].str.len() <=5]
239/59: faults[faults['EquipmentID'].str.len() <=5].tail(20)
239/60: faults[faults['EquipmentID'].str.len() ==5]
240/1:
import pandas as pd
from datetime import datetime
240/2:
# faults = pd.read_csv("../data/J1939Faults.csv")

faults = pd.read_csv("../data/J1939Faults.csv", 
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
240/3: faults[faults['EquipmentID'].str.len() ==5]
240/4: faults[faults['EquipmentID'].str.len() <=5]
240/5:
# faults = pd.read_csv("../data/J1939Faults.csv")

faults = pd.read_csv("../data/J1939Faults.csv", low_memory=False,
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
240/6: faults[faults['EquipmentID'].str.len() <=5]
240/7: faults.head(5)
240/8: faults.isna().sum()
240/9:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults = faults.drop(columns = ['actionDescription', 'faultValue'])
faults
240/10: faults[faults['EquipmentID'].str.len() <=5]
240/11: faults.loc[faults['spn'] == 5246]
240/12: faults = faults[faults['EquipmentID'].str.len() <=5]
240/13:
faults = faults[faults['EquipmentID'].str.len() <=5]
faults.shape
240/14:
diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
# diagnostic.shape
# diagnostic.info
240/15: diagnostic.head(5)
240/16: diagnostic.isna().sum()
240/17: diagnostic.info
240/18: diagnostic.dtypes
240/19:
diagnostic['Value'].value_counts().to_frame().reset_index().head(50)
# 12821626 - 1009465
240/20: diagnostic['FaultId'].value_counts().to_frame().reset_index()
240/21: diagnostic['Name'].value_counts().to_frame().reset_index()
240/22:
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
service.shape #(7124, 14)
240/23: service.head(5)
240/24: fault[faults['spn']==1526]
240/25: faults[faults['spn']==1526]
240/26: faults.loc[faults['spn']==1526]
240/27: faults[faults['spn']==5426]
240/28: faults.loc[faults['spn'] == 1569]
240/29: faults.loc[faults['spn'] == 1569 & faults['fmi'] == 31]
240/30: faults.loc[(faults['spn'] == 1569) & (faults['fmi'] == 31)]
240/31: faults[(faults['spn'] == 1569) & (faults['fmi'] == 31)] #10717 with and without fmi=31
240/32: faults[(faults['spn'].isin([1569, 5426]))]
240/33: faults[(faults['spn'].isin([1569, 5426]))]
240/34: faults[(faults['spn']==[1569, 5426])]
240/35: faults[(faults['spn'].isin([1569, 5426]))]
240/36: faults.loc[~(faults['spn'] .isin([1569, 5426]))]
240/37:
faults[(faults['spn'].isin([1569 & 5426]))]

same number of rows as for 1569
240/38:
faults[(faults['spn']==1569) & (faults['spn']==5246)]

# same number of rows as for 1569 which is 10717 rows × 18 columns
240/39:
faults[(faults['spn']==5246)]

# same number of rows as for 1569 which is 10717 rows × 18 columns
240/40: 10717+1174449 +1189
240/41:
faults[(faults['spn']==5246)] #1189
faults[(faults['spn']==5246)]['EquipmentID'].nunique()

# same number of rows as for 1569 which is 10717 rows × 18 columns
240/42:
faults.loc[~(faults['spn'] .isin([1569, 5426]))] #1174449
faults.loc[~(faults['spn'] .isin([1569, 5426]))]['EquipmentID'].nunique()
240/43:
faults.loc[~(faults['spn'] .isin([1569, 5426]))] #1174449
faults.loc[~(faults['spn'] .isin([1569, 5426]))]['EquipmentID'].nunique() #1045
240/44:
faults.loc[~(faults['spn'] .isin([1569, 5426]))] #1174449
# faults.loc[~(faults['spn'] .isin([1569, 5426]))]['EquipmentID'].nunique() #1045
240/45:
faults[(faults['spn']==5246)] #1189
# faults[(faults['spn']==5246)]['EquipmentID'].nunique() #211

# same number of rows as for 1569 which is 10717 rows × 18 columns
240/46:
faults[(faults['spn'] == 1569) & (faults['fmi'] == 31)] #10717 with and without fmi=31

faults[(faults['spn'] == 1569) & (faults['fmi'] == 31)]['EquipmentID'].nunique()
240/47: faults['EquipmentID'].nunique()
240/48:
faults.loc[~(faults['spn'].isin([1569, 5426]))] #1174449
# faults.loc[~(faults['spn'] .isin([1569, 5426]))]['EquipmentID'].nunique() #1045
240/49: diagnostic.head(5)
240/50: diagnostic.pivot(index='Id', columns='FaultId', values='Value')
240/51: diagnostic.pivot(columns='FaultId', values='Value')
240/52: # diagnostic.pivot()
240/53: diagnostic.pivot(columns='FaultId', index='Id', values=['Name', 'Value'])
240/54:
diagnostic['FaultId'].value_counts().to_frame().reset_index()

diagnostic['FaultId'].nunique()
240/55: faults['Latitude']-36.0666667
240/56: [faults['Latitude']-36.0666667]
240/57: faults['latA_diff'] = [faults['Latitude']-36.0666667]
240/58: faults['latA_diff'] = faults['Latitude']-36.0666667
240/59:
faults['latA_diff'] = faults['Latitude']-36.0666667
faults.head(5)
240/60:
faults['latA_diff'] = faults['Latitude']-36.0666667
faults['latB_diff'] = faults['Latitude']-35.5883333
faults['latC_diff'] = faults['Latitude']-36.1950
faults['longA_diff'] = faults['Longitude']-(-86.4347222)
faults['longB_diff'] = faults['Longitude']-(-86.4438888)
faults['longC_diff'] = faults['Longitude']-(-83.174722)

faults.head(5)
240/61:
faults['latA_diff'] = faults['Latitude']-36.0666667
faults['latB_diff'] = faults['Latitude']-35.5883333
faults['latC_diff'] = faults['Latitude']-36.1950
faults['longA_diff'] = faults['Longitude']-(-86.4347222)
faults['longB_diff'] = faults['Longitude']-(-86.4438888)
faults['longC_diff'] = faults['Longitude']-(-83.174722)

faults
240/62:
# diagnostic.pivot(columns='FaultId', index='Id', values=['Name', 'Value'])

diagnostic.pivot(columns='Name', index='FaultId', values= 'Value')
240/63:
diagnostic['Name'].value_counts().to_frame().reset_index()
diagnostic['Name'].nunique()
240/64:
diagnostic['Name'].value_counts().to_frame().reset_index()
# diagnostic['Name'].nunique()
241/1: from sklearn.ensembl import GradientBoostingClassifier
241/2: from sklearn.ensemble import GradientBoostingClassifier
241/3:
X = data_aj_x
# y = ydata_train.drop(columns = ['matchId'])

# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.

# X_train.shape
241/4: import pandas as pd
241/5:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
# submission = pd.read_csv('../data/sample_submission.csv')
241/6:
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

import seaborn as sns
import matplotlib.pyplot as plt
242/1: import pandas as pd
242/2:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
# submission = pd.read_csv('../data/sample_submission.csv')
242/3: y_train.shape
242/4: X_train.head(10)
242/5:
train_predictions = ( 
    X_train
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions
242/6: from sklearn.metrics import accuracy_score
242/7:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions['teamId']
)
242/8:
y_pred = ( 
    X_test
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    ['teamId']
    .reset_index(drop = True)
)

y_pred
242/9:
submission['winner'] = y_pred
submission.head()
242/10:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
# submission = pd.read_csv('../data/sample_submission.csv')
242/11:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
submission = pd.read_csv('../data/sample_submission.csv')
242/12:
submission['winner'] = y_pred
submission.head()
242/13:
submission['winner'] = y_pred
submission.head()
242/14: submission.to_csv('../data/first_submission.csv', index = False)
242/15:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats
champ.columns
242/16:
champ_grp = champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame()
champ_grp
242/17:
X_train_added = pd.merge(X_train, champ_grp, left_on="championId", right_on="championId", how="left")
X_train_added.head(5)
242/18:
X_test_added = pd.merge(X_test, champ_grp, left_on="championId", right_on="championId", how="left")
X_test_added.head(5)
242/19:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    ['teamId']
    .reset_index(drop = True)
)

train_predictions_added
242/20:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#Score is reduced adding the championpoints reduces from 0.504 to 0.489
242/21:
test_predictions_added = ( 
    X_test_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [ 'teamId']
    .reset_index(drop = True)
)

test_predictions_added
242/22:
submission['winner'] = test_predictions_added
submission.head()
242/23: submission.to_csv('../data/sm1_submission.csv', index = False)
242/24:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'summonerLevel', 'championPoints'], ascending = [True, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
242/25:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#it is back to 0.50375, lookslike championpoint has no additive effect.
242/26:
champion = pd.read_json('../data/champion.json')
champion
242/27:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['name']== "Akshan"]
242/28:
champion_data_info = pd.DataFrame.from_dict(champion_data['info'].values.tolist())   

champion_data_info.columns
242/29:
champion_data_stats = pd.DataFrame.from_dict(champion_data['stats'].values.tolist())   

champion_data_stats.head(5)
242/30:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape #162, 31
champ_spread.columns.to_list()
242/31:
champ_spread1 = champ_spread.drop(columns = ['version',
 'id',
 
 'title',
 'blurb',
 'info',
 'image',
 'tags',
 'partype',
 'stats']).rename(columns = {'key': 'championId',
                             'name': 'championName'})
242/32: X_train_added.info()
242/33:


champ_spread1['championId'] = champ_spread1['championId'].astype('int64')
champ_spread1.info()
242/34:
X_train_added1 = pd.merge(X_train_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_train_added1.head(5)
X_train_added1.columns.to_list()
242/35:
X_test_added1 = pd.merge(X_test_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_test_added1.head(5)
X_test_added1.columns.to_list()
242/36:
teamposition = pd.read_csv('../data/teamPositions.csv') #matchId    participantId   teamPosition

X_train_added2 = pd.merge(X_train_added1, teamposition, 
                          left_on=["matchId", "participantId"], 
                          right_on=["matchId", "participantId"], 
                          how="left")
X_train_added2.head(5)
X_train_added2.columns.to_list()

X_test_added2 = pd.merge(X_test_added1, teamposition, 
                          left_on=["matchId", "participantId"], 
                          right_on=["matchId", "participantId"], 
                          how="left")
242/37:
train_predictions_added1 = ( 
    X_train_added1
    .sort_values(['matchId', 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed'], ascending = [True, False, False, False, False, 
                              False, False, False, False, False,
                             False, False, False, False, False,
                             False, False, False, False, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added1
242/38:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added1['teamId']
)
242/39:
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

import seaborn as sns
import matplotlib.pyplot as plt
242/40:
X = X_train_added2
# y = y_train

# X_train_sm1, X_test_sm1, y_train_sm1, y_test_sm1 = train_test_split(X, y, random_state = 321)

# linreg_base = LinearRegression().fit(X_train_sm1, y_train_sm1)# 

X_train_added2.columns
X_train_added2.head(10)
242/41:
data = X_train_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
data.head(5)
242/42:
#modifing test data

test_data = X_test_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
test_data.shape #20000, 26
242/43:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 240
# data_x.columns.to_list()
data_x.head(10)
242/44:
X = data_x
y = y_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
242/45:
y_train.head(5)
y_train.shape
242/46: X_train.isna().sum()
242/47: logreg = LogisticRegression().fit(X_train.fillna(0), y_train)
242/48: from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
242/49: y_pred = logreg.predict(X_test.fillna(0))
242/50: accuracy_score(y_test, y_pred)
242/51: y1_train = pd.read_csv('../data/train_winners.csv')
242/52:
X = data_x
y = y1_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)

X_test.shape
242/53:
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
242/54:
from sklearn.impute Import SimpleImputer
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
242/55:
from sklearn.impute import SimpleImputer
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
242/56:
from sklearn.impute import SimpleImputer
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
242/57:
# fit the model on X_test data
logreg1 = logReg_scaler.predict(X_test)
242/58:
#finding the accuracy score

accuracy_score(y_test, logreg1)
242/59:
#Which features are the best predictor

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).head(20)
242/60: data_x.columns.to_list()
242/61:
last_aj=pd.read_csv('../data/last_frame_valuesALL.csv')

last_aj.shape #80000, 47
242/62:
data.shape
data.columns.to_list()
data_aj = pd.merge(data, last_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
data_aj.columns.to_list()
# data_aj.shape
# data_aj.head(5)
242/63:
data_aj_x=pd.concat([data_aj[data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data_aj[data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data_aj[data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data_aj[data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         data_aj[data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data_aj[data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data_aj[data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
         data_aj[data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data_aj[data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
         data_aj[data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_aj_x.shape #(8000, 681)
# data_aj_x.head(5)
242/64:
# rereading the y_train data because it becomes 6000 from 8000 upon split in previous model, 
# so rereading and calling it ydata_train

# y_train.shape #(6000, 1)
ydata_train = pd.read_csv('../data/train_winners.csv')

ydata_train.shape
242/65:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.
242/66: ?train_test_split
242/67:
X.shape
X_train.shape
242/68: logreg_aj = LogisticRegression().fit(X_train.fillna(0), y_train)
242/69: y_pred_aj = logreg_aj.predict(X_test.fillna(0))
242/70: accuracy_score(y_test, y_pred_aj)
242/71:
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer #fills average of the column if there are NaN missing values
242/72: ?LogisticRegression #to find the variable that can be adjusted
242/73:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.0001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
242/74: scal_pred = logReg_aj_scaler.predict(X_test)
242/75: accuracy_score(y_test, scal_pred)
242/76:
logReg_aj_scaler['scaler'].get_feature_names_out()
logReg_aj_scaler['log_reg'].coef_.flatten() #turns into one array
242/77:
# look at coefficient

logReg_aj_scaler_coefficeinct = pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False)
242/78:
#Reading the time stamp files from aj for test data

last_test_aj=pd.read_csv('../data/test_last_frame_values_ALL.csv')

last_test_aj.shape #20000, 47
242/79:
#merging the test_aj to the test_data

test_data_aj = pd.merge(test_data, last_test_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
test_data_aj.shape #20000, 71
242/80:
test_data_aj_x=pd.concat([test_data_aj[test_data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           test_data_aj[test_data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
          test_data_aj[test_data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          test_data_aj[test_data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         test_data_aj[test_data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
         test_data_aj[test_data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          test_data_aj[test_data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
       test_data_aj[test_data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
         test_data_aj[test_data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
      test_data_aj[test_data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
test_data_aj_x.shape #(w000, 681)
242/81:
X_test = test_data_aj_x

y_pred = logReg_aj_scaler.predict(X_test)

y_pred
242/82:
submission['winner'] = y_pred
submission.head()
242/83: submission.to_csv('../data/sm2_submission.csv', index = False) #accuracy score on train set was 70.15%
242/84:

from sklearn.ensemble import RandomForestClassifier

# instantiate the classifier 

# rfc = RandomForestClassifier(random_state=0)
242/85:
y_train.shape
X_train.shape
X_test.shape
y_test.shape
242/86:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
242/87:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
242/88: y_test
242/89: rfc_pred
242/90:
submission['winner'] = y_pred
submission.head()
242/91: submission.to_csv('../data/sm_rfc_submission.csv', index = False) #accuracy score on train set was 70.5%
242/92: print('Model accuracy score with rfc model with random state(0): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))
242/93: print('Model accuracy score with rfc model with random state(10): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))
242/94:
#apply the rfc model on the 
X_test = test_data_aj_x

y_pred = rfc_aj_scaler.predict(X_test)

y_pred
242/95: from sklearn.neural_network import MLPRegressor, MLPClassifier
242/96:
y_train.shape
X_train.shape
# X_test.shape
y_test.shape
242/97: from sklearn.preprocessing import MinMaxScaler
242/98:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPRegressor(verbose = True))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
242/99: nn_pred
242/100:
y_train.shape
# X_train.shape
# X_test.shape
# y_test.shape
242/101:
y_train.shape
X_train.shape
# X_test.shape
# y_test.shape
242/102:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
242/103:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
242/104:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.
242/105:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
242/106:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=0.01)) # 
    ]
).fit(X_train, y_train)
242/107:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=0.10)) # 
    ]
).fit(X_train, y_train)
242/108:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=10)) # 
    ]
).fit(X_train, y_train)
242/109:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
242/110: y_test
242/111: rfc_pred
242/112:
submission['winner'] = y_pred
submission.head()
242/113: submission.to_csv('../data/sm_rfc_submission.csv', index = False) #accuracy score on train set was 70.5%
242/114: print('Model accuracy score with rfc model with random state(0): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))
242/115: print('Model accuracy score with rfc model with random state(10): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))
242/116:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=100)) # 
    ]
).fit(X_train, y_train)
242/117:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
242/118: y_test
242/119: rfc_pred
242/120:
submission['winner'] = y_pred
submission.head()
242/121:
print('Model accuracy score with rfc model with random state(0): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))

# random state(0) =48.8
# random state(10) =69.0
# random state(100) = 69.8
242/122: print('Model accuracy score with rfc model with random state(10): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))
242/123:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=1000)) # 
    ]
).fit(X_train, y_train)
242/124:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
242/125: y_test
242/126:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=10000)) # 
    ]
).fit(X_train, y_train)
242/127:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
242/128:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=1000)) # 
    ]
).fit(X_train, y_train)
242/129:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
242/130: y_test
242/131: rfc_pred
242/132:
submission['winner'] = y_pred
submission.head()
242/133: submission.to_csv('../data/sm_rfc_submission.csv', index = False) #accuracy score on train set was 70.8%
242/134:
print('Model accuracy score with rfc model with random state(1000): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))

# random state(0) =48.8
# random state(10) =69.0
# random state(100) = 69.8
#random state(1000) =70.8
#random state(1000) =68.2
242/135:
#apply the rfc model on the 
X_test = test_data_aj_x

y_pred = rfc_aj_scaler.predict(X_test)

y_pred
242/136: from sklearn.neural_network import MLPRegressor, MLPClassifier
242/137:
y_train.shape
X_train.shape
# X_test.shape
y_test.shape
242/138: from sklearn.preprocessing import MinMaxScaler
242/139:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPRegressor(verbose = True))
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
242/140:
from sklearn.metrics import r2_score, mean_absolute_error

print(f'R2 Score: {r2_score(y_test, nn_aj_scaler.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, nn_aj_scaler.predict(X_test))}')
242/141: from sklearn.metrics import classification_report
242/142: # print(classification_report(y_test, nn_pred))
242/143:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 4,
                             hidden_layer_sizes = (500, 500, 500))) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
240/65:
faults['latA_diff'] = abs(faults['Latitude']-36.0666667)
faults['latB_diff'] = abs(faults['Latitude']-35.5883333)
faults['latC_diff'] = abs(faults['Latitude']-36.1950)
faults['longA_diff'] = abs(faults['Longitude']-(-86.4347222))
faults['longB_diff'] = abs(faults['Longitude']-(-86.4438888))
faults['longC_diff'] = abs(faults['Longitude']-(-83.174722)))

faults
240/66:
faults['latA_diff'] = abs(faults['Latitude']-36.0666667)
faults['latB_diff'] = abs(faults['Latitude']-35.5883333)
faults['latC_diff'] = abs(faults['Latitude']-36.1950)
faults['longA_diff'] = abs(faults['Longitude']-(-86.4347222))
faults['longB_diff'] = abs(faults['Longitude']-(-86.4438888))
faults['longC_diff'] = abs(faults['Longitude']-(-83.174722))

faults
240/67: faults['EquipmentID '].value_counts()
240/68: faults['EquipmentID'].value_counts()
240/69: faults['EquipmentID'].value_counts().sort_values(ascending=False)
240/70: faults['EquipmentID'].value_counts().sort_values(ascending=False).to_frame()
240/71: faults['EquipmentID'].value_counts().sort_values(ascending=False).to_frame().head(10)
242/144:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
#alpha: 65.9(0.01) 70(1) 51.2(100) 70.2(5) 70.05(2) 71.05(4)

#adding the hidden layer the score dropped from 71 to 70.75
242/145:
print(f'R2 Score: {r2_score(y_test, nn_aj_scaler1.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, nn_aj_scaler1.predict(X_test))}')
242/146:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 5
                             )) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
242/147:
nn_pred1 = nn_aj_scaler.predict(X_test)
accuracy_score(y_test, nn_pred1)
242/148:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.

X_train.shape
242/149:
# import SVC classifier
from sklearn.svm import SVC


# import metrics to compute accuracy
from sklearn.metrics import accuracy_score

svm_model = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('svc', SVC()) 
    ]
).fit(X_train, y_train)

# instantiate classifier with default hyperparameters
# svc=SVC() 


# fit classifier to training set
# svc.fit(X_train,y_train)


# make predictions on test set
# y_pred=svc.predict(X_test)


# compute and print accuracy score
# print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
242/150:
svm_pred = svm_model.predict(X_test)
accuracy_score(y_test, svm_pred1)
242/151:
svm_pred = svm_model.predict(X_test)
accuracy_score(y_test, svm_pred)
242/152: from sklearn.ensemble import GradientBoostingClassifier
242/153:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.

# X_train.shape
242/154:
gdc_model = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('gdc', GradientBoostingClassifier()) 
    ]
).fit(X_train, y_train)
242/155:
gdc_pred = gdc_model.predict(X_test)
accuracy_score(y_test, gdc_pred)
242/156:
gdc_model = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('gdc', GradientBoostingClassifier(n_estimator = 1000, learning_rate=0.01)) 
    ]
).fit(X_train, y_train)
242/157:
gdc_model = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('gdc', GradientBoostingClassifier(nestimator = 1000, learning_rate=0.01)) 
    ]
).fit(X_train, y_train)
242/158:
gdc_model = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('gdc', GradientBoostingClassifier(n_estimator = 1000, learning_rate=0.01)) 
    ]
).fit(X_train, y_train)
242/159:
gdc_model = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('gdc', GradientBoostingClassifier(n_estimators = 1000, learning_rate=0.01)) 
    ]
).fit(X_train, y_train)
242/160:
gdc_pred = gdc_model.predict(X_test)
accuracy_score(y_test, gdc_pred)
242/161: data_aj_x
242/162: data_aj_x.columns.to_list()
242/163: print(classification_report(y_test, logReg_aj_scaler(X_test)))
242/164: print(classification_report(y_test, logReg_aj_scaler.predict(X_test)))
242/165: print(confusion_matrix(y_test, logReg_aj_scaler.predict(X_test)))
242/166:
#look at confusin matrix differently

from cm import plot_confusion_matrix
242/167:
#look at confusin matrix differently

from sklearn.metrics import plot_confusion_matrix
242/168:
#look at confusin matrix differently

from sklearn.metrics import plot_confusion_matrix

from sklearn.metrics import plot_confusion_matrix
242/169:
#look at confusin matrix differently

from sklearn.metrics import plot_confusion_matrix

from sklearn.metrics import plot_confusion_matrix
242/170:
#look at confusin matrix differently

# from sklearn.metrics import plot_confusion_matrix

from sklearn.metrics import plot_confusion_matrix
242/171:
#look at confusin matrix differently

# from sklearn.metrics import plot_confusion_matrix

# from sklearn.metrics import plot_confusion_matrix
from cm import plot_confusion_matrix_multiclass
242/172: pip install cm
242/173:
#look at confusin matrix differently

# from sklearn.metrics import plot_confusion_matrix

# from sklearn.metrics import plot_confusion_matrix
from cm import plot_confusion_matrix_multiclass
242/174:
#look at confusin matrix differently

from sklearn.metrics import plot_confusion_matrix

# from sklearn.metrics import plot_confusion_matrix
# from cm import plot_confusion_matrix_multiclass
242/175:
#look at confusin matrix differently

from cm import plot_confusion_matrix

# from sklearn.metrics import plot_confusion_matrix
# from cm import plot_confusion_matrix_multiclass
242/176: pip install --upgrade scikit-learn
242/177:
from sklearn import __version__
print(__version__)
242/178:
from sklearn import __version__
print(__version__)
242/179:
#look at confusin matrix differently

# from cm import plot_confusion_matrix

from sklearn.metrics import plot_confusion_matrix
# from cm import plot_confusion_matrix_multiclass
246/1: import pandas as pd
246/2:
X_train = pd.read_csv('../data/participants_train.csv')
X_test = pd.read_csv('../data/participants_test.csv')
y_train = pd.read_csv('../data/train_winners.csv')
submission = pd.read_csv('../data/sample_submission.csv')
246/3: y_train.shape
246/4: X_train.head(10)
246/5:
train_predictions = ( 
    X_train
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions
246/6: from sklearn.metrics import accuracy_score
246/7:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions['teamId']
)
246/8:
y_pred = ( 
    X_test
    .sort_values(['matchId', 'summonerLevel'], ascending = [True, False])
    .drop_duplicates('matchId')
    ['teamId']
    .reset_index(drop = True)
)

y_pred
246/9:
submission['winner'] = y_pred
submission.head()
246/10: submission.to_csv('../data/first_submission.csv', index = False)
246/11:
champ = pd.read_csv('../data/champion_mastery.csv') 
#question how to connect this with the X_train data where we have the win and loose stats
champ.columns
246/12:
champ_grp = champ.groupby('championId')['championPoints'].sum().sort_values(ascending =False).to_frame()
champ_grp
246/13:
X_train_added = pd.merge(X_train, champ_grp, left_on="championId", right_on="championId", how="left")
X_train_added.head(5)
246/14:
X_test_added = pd.merge(X_test, champ_grp, left_on="championId", right_on="championId", how="left")
X_test_added.head(5)
246/15:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    ['teamId']
    .reset_index(drop = True)
)

train_predictions_added
246/16:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#Score is reduced adding the championpoints reduces from 0.504 to 0.489
246/17:
test_predictions_added = ( 
    X_test_added
    .sort_values(['matchId', 'championPoints'], ascending = [True, False])
    .drop_duplicates('matchId')
    [ 'teamId']
    .reset_index(drop = True)
)

test_predictions_added
246/18:
submission['winner'] = test_predictions_added
submission.head()
246/19: submission.to_csv('../data/sm1_submission.csv', index = False)
246/20:
# **using the championPoints for prediction**

train_predictions_added = ( 
    X_train_added
    .sort_values(['matchId', 'summonerLevel', 'championPoints'], ascending = [True, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added
246/21:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added['teamId']
)

#it is back to 0.50375, lookslike championpoint has no additive effect.
246/22:
champion = pd.read_json('../data/champion.json')
champion
246/23:
champion_data = pd.DataFrame.from_dict(champion['data'].values.tolist())   

champion_data.head(5)
champion_data[champion_data['name']== "Akshan"]
246/24:
champion_data_info = pd.DataFrame.from_dict(champion_data['info'].values.tolist())   

champion_data_info.columns
246/25:
champion_data_stats = pd.DataFrame.from_dict(champion_data['stats'].values.tolist())   

champion_data_stats.head(5)
246/26:
#merging two df using index when dont have same columns df1.join(df2) or pd.concat([df1, df2], axis=1)

champ_spread = champion_data.join(champion_data_stats)

champ_spread.describe()
champ_spread.shape #162, 31
champ_spread.columns.to_list()
246/27:
champ_spread1 = champ_spread.drop(columns = ['version',
 'id',
 
 'title',
 'blurb',
 'info',
 'image',
 'tags',
 'partype',
 'stats']).rename(columns = {'key': 'championId',
                             'name': 'championName'})
246/28: X_train_added.info()
246/29:


champ_spread1['championId'] = champ_spread1['championId'].astype('int64')
champ_spread1.info()
246/30:
X_train_added1 = pd.merge(X_train_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_train_added1.head(5)
X_train_added1.columns.to_list()
246/31:
X_test_added1 = pd.merge(X_test_added, champ_spread1, 
                          left_on=["championId", "championName"], 
                          right_on=["championId", "championName"], 
                          how="left")
X_test_added1.head(5)
X_test_added1.columns.to_list()
246/32:
teamposition = pd.read_csv('../data/teamPositions.csv') #matchId    participantId   teamPosition

X_train_added2 = pd.merge(X_train_added1, teamposition, 
                          left_on=["matchId", "participantId"], 
                          right_on=["matchId", "participantId"], 
                          how="left")
X_train_added2.head(5)
X_train_added2.columns.to_list()

X_test_added2 = pd.merge(X_test_added1, teamposition, 
                          left_on=["matchId", "participantId"], 
                          right_on=["matchId", "participantId"], 
                          how="left")
246/33:
train_predictions_added1 = ( 
    X_train_added1
    .sort_values(['matchId', 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed'], ascending = [True, False, False, False, False, 
                              False, False, False, False, False,
                             False, False, False, False, False,
                             False, False, False, False, False, False])
    .drop_duplicates('matchId')
    [['matchId', 'teamId']]
    .reset_index(drop = True)
)

train_predictions_added1
246/34:
accuracy_score(
    y_true = y_train['winner'],
    y_pred = train_predictions_added1['teamId']
)
246/35:
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

import seaborn as sns
import matplotlib.pyplot as plt
246/36:
X = X_train_added2
# y = y_train

# X_train_sm1, X_test_sm1, y_train_sm1, y_test_sm1 = train_test_split(X, y, random_state = 321)

# linreg_base = LinearRegression().fit(X_train_sm1, y_train_sm1)# 

X_train_added2.columns
X_train_added2.head(10)
246/37:
data = X_train_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
data.head(5)
246/38:
#modifing test data

test_data = X_test_added2[['matchId',
 'teamId',
'participantId',
    'summonerLevel',
  'championId',
 'championPoints',
 'hp',
 'hpperlevel',
 'mp',
 'mpperlevel',
 'movespeed',
 'armor',
 'armorperlevel',
 'spellblock',
 'spellblockperlevel',
 'attackrange',
 'hpregen',
 'hpregenperlevel',
 'mpregen',
 'mpregenperlevel',
 'crit',
 'critperlevel',
 'attackdamage',
 'attackdamageperlevel',
 'attackspeedperlevel',
 'attackspeed']]
 #'teamPosition']
        
test_data.shape #20000, 26
246/39:
data_x=pd.concat([data[data['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data[data['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data[data['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data[data['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
          data[data['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data[data['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data[data['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
          data[data['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data[data['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
          data[data['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_x.shape #8000, 240
# data_x.columns.to_list()
data_x.head(10)
246/40:
X = data_x
y = y_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)
246/41:
y_train.head(5)
y_train.shape
246/42: X_train.isna().sum()
246/43: logreg = LogisticRegression().fit(X_train.fillna(0), y_train)
246/44: from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
246/45: y_pred = logreg.predict(X_test.fillna(0))
246/46: accuracy_score(y_test, y_pred)
246/47: y1_train = pd.read_csv('../data/train_winners.csv')
246/48:
X = data_x
y = y1_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321)

X_test.shape
246/49:
from sklearn.impute import SimpleImputer
logReg_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
246/50:
# fit the model on X_test data
logreg1 = logReg_scaler.predict(X_test)
246/51:
#finding the accuracy score

accuracy_score(y_test, logreg1)
246/52:
#Which features are the best predictor

pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False).head(20)
246/53: data_x.columns.to_list()
246/54:
last_aj=pd.read_csv('../data/last_frame_valuesALL.csv')

last_aj.shape #80000, 47
246/55:
data.shape
data.columns.to_list()
data_aj = pd.merge(data, last_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
data_aj.columns.to_list()
# data_aj.shape
# data_aj.head(5)
246/56:
data_aj_x=pd.concat([data_aj[data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           data_aj[data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
           data_aj[data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          data_aj[data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         data_aj[data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
          data_aj[data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          data_aj[data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
         data_aj[data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
          data_aj[data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
         data_aj[data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
data_aj_x.shape #(8000, 681)
# data_aj_x.head(5)
246/57:
# rereading the y_train data because it becomes 6000 from 8000 upon split in previous model, 
# so rereading and calling it ydata_train

# y_train.shape #(6000, 1)
ydata_train = pd.read_csv('../data/train_winners.csv')

ydata_train.shape
246/58:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.
246/59: ?train_test_split
246/60:
X.shape
X_train.shape
246/61: logreg_aj = LogisticRegression().fit(X_train.fillna(0), y_train)
246/62: y_pred_aj = logreg_aj.predict(X_test.fillna(0))
246/63: accuracy_score(y_test, y_pred_aj)
246/64:
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer #fills average of the column if there are NaN missing values
246/65: ?LogisticRegression #to find the variable that can be adjusted
246/66:
logReg_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C = 0.0001)) #started from 0.01, 0,001, 
    ]
).fit(X_train, y_train)
246/67: scal_pred = logReg_aj_scaler.predict(X_test)
246/68: accuracy_score(y_test, scal_pred)
246/69:
logReg_aj_scaler['scaler'].get_feature_names_out()
logReg_aj_scaler['log_reg'].coef_.flatten() #turns into one array
246/70:
# look at coefficient

logReg_aj_scaler_coefficeinct = pd.DataFrame({'feature': X_train.columns,
                             'coefficient': logReg_aj_scaler['log_reg'].coef_.flatten()}).sort_values('coefficient', 
                                                                                                      ascending =False)
246/71: print(classification_report(y_test, logReg_aj_scaler.predict(X_test)))
246/72: print(confusion_matrix(y_test, logReg_aj_scaler.predict(X_test)))
246/73: pip install cm
246/74:
from sklearn import __version__
print(__version__)
246/75: pip install --upgrade scikit-learn
246/76:
from sklearn import __version__
print(__version__)
246/77:
#look at confusin matrix differently

# from cm import plot_confusion_matrix

from sklearn.metrics import plot_confusion_matrix
# from cm import plot_confusion_matrix_multiclass
246/78:
#look at confusin matrix differently

# from cm import plot_confusion_matrix

from sklearn.metrics import plot_confusion_matrix
# from cm import plot_confusion_matrix_multiclass
246/79:
submission['winner'] = y_pred
submission.head()
246/80: submission.to_csv('../data/sm_rfc_submission.csv', index = False) #accuracy score on train set was 70.8%
246/81:
print('Model accuracy score with rfc model with random state(1000): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))

# random state(0) =48.8
# random state(10) =69.0
# random state(100) = 69.8
#random state(1000) =70.8
#random state(1000) =68.2
246/82:
from sklearn.metrics import r2_score, mean_absolute_error

print(f'R2 Score: {r2_score(y_test, nn_aj_scaler.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, nn_aj_scaler.predict(X_test))}')
246/83:
#Reading the time stamp files from aj for test data

last_test_aj=pd.read_csv('../data/test_last_frame_values_ALL.csv')

last_test_aj.shape #20000, 47
246/84:
#merging the test_aj to the test_data

test_data_aj = pd.merge(test_data, last_test_aj, left_on=["matchId", "participantId"], right_on=["matchId", "participantId"], how="left")
test_data_aj.shape #20000, 71
246/85:
test_data_aj_x=pd.concat([test_data_aj[test_data_aj['participantId']==1].reset_index(drop=True).drop(columns = ['teamId', 'participantId']).rename(columns=lambda X: X+"_1"),
           test_data_aj[test_data_aj['participantId']==2].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_2"),
          test_data_aj[test_data_aj['participantId']==3].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_3"),
          test_data_aj[test_data_aj['participantId']==4].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_4"),
         test_data_aj[test_data_aj['participantId']==5].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_5"),
         test_data_aj[test_data_aj['participantId']==6].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_6"),
          test_data_aj[test_data_aj['participantId']==7].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_7"),
       test_data_aj[test_data_aj['participantId']==8].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_8"),
         test_data_aj[test_data_aj['participantId']==9].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_9"),
      test_data_aj[test_data_aj['participantId']==10].reset_index(drop=True).drop(columns = ['matchId',
                                                                             'teamId', 'participantId']).rename(columns=lambda X: X+"_10")], 
          axis=1)
test_data_aj_x.shape #(w000, 681)
246/86:
X_test = test_data_aj_x

y_pred = logReg_aj_scaler.predict(X_test)

y_pred
246/87:
submission['winner'] = y_pred
submission.head()
246/88: submission.to_csv('../data/sm2_submission.csv', index = False) #accuracy score on train set was 70.15%
246/89:

from sklearn.ensemble import RandomForestClassifier

# instantiate the classifier 

# rfc = RandomForestClassifier(random_state=0)
246/90:
# y_train.shape
# X_train.shape
# # X_test.shape
# # y_test.shape
246/91:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.
246/92:
rfc_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('rfc', RandomForestClassifier(random_state=1000)) # 
    ]
).fit(X_train, y_train)
246/93:
#fit the model on test data and get accuracy score

rfc_pred = rfc_aj_scaler.predict(X_test)
accuracy_score(y_test, rfc_pred)
246/94: y_test
246/95: rfc_pred
246/96:
submission['winner'] = y_pred
submission.head()
246/97: submission.to_csv('../data/sm_rfc_submission.csv', index = False) #accuracy score on train set was 70.8%
246/98:
print('Model accuracy score with rfc model with random state(1000): {0:0.4f}'. format(accuracy_score(y_test, rfc_pred)))

# random state(0) =48.8
# random state(10) =69.0
# random state(100) = 69.8
#random state(1000) =70.8
#random state(1000) =68.2
246/99:
#apply the rfc model on the 
X_test = test_data_aj_x

y_pred = rfc_aj_scaler.predict(X_test)

y_pred
246/100: from sklearn.neural_network import MLPRegressor, MLPClassifier
246/101:
y_train.shape
X_train.shape
# X_test.shape
y_test.shape
246/102: from sklearn.preprocessing import MinMaxScaler
246/103:
from sklearn.metrics import r2_score, mean_absolute_error

print(f'R2 Score: {r2_score(y_test, nn_aj_scaler.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, nn_aj_scaler.predict(X_test))}')
246/104: from sklearn.metrics import classification_report
246/105: # print(classification_report(y_test, nn_pred))
246/106: print(classification_report(y_test, nn_pred))
246/107:
nn_aj_scaler1 = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 4,
                             hidden_layer_sizes = (500, 500, 500))) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
246/108:
#fit the model on test data and get accuracy score

nn_pred1 = nn_aj_scaler1.predict(X_test)
accuracy_score(y_test, nn_pred1)

# X_test.shape
#alpha: 65.9(0.01) 70(1) 51.2(100) 70.2(5) 70.05(2) 71.05(4)

#adding the hidden layer the score dropped from 71 to 70.75
246/109:
print(f'R2 Score: {r2_score(y_test, nn_aj_scaler1.predict(X_test))}')
print(f'MAE: {mean_absolute_error(y_test, nn_aj_scaler1.predict(X_test))}')
246/110:
nn_aj_scaler = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', MinMaxScaler()),
        ('mlr', MLPClassifier(verbose = True,
                             max_iter = 1000,
                             alpha = 5
                             )) #started with 1  and then 10 score came down, default set is 0.001
#         ('rfc', RandomForestClassifier(random_state=0)) # 
    ]
).fit(X_train, y_train)
246/111:
nn_pred1 = nn_aj_scaler.predict(X_test)
accuracy_score(y_test, nn_pred1)
246/112:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.

X_train.shape
246/113:
# import SVC classifier
from sklearn.svm import SVC


# import metrics to compute accuracy
from sklearn.metrics import accuracy_score

svm_model = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('svc', SVC()) 
    ]
).fit(X_train, y_train)

# instantiate classifier with default hyperparameters
# svc=SVC() 


# fit classifier to training set
# svc.fit(X_train,y_train)


# make predictions on test set
# y_pred=svc.predict(X_test)


# compute and print accuracy score
# print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
246/114:
svm_pred = svm_model.predict(X_test)
accuracy_score(y_test, svm_pred)
246/115: from sklearn.ensemble import GradientBoostingClassifier
246/116:
X = data_aj_x
y = ydata_train.drop(columns = ['matchId'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321) #split the training data into test and train set.

# X_train.shape
246/117:
gdc_model = Pipeline(
    steps = [
        ('impute', SimpleImputer()), #fills average of the column if there are NaN missing values
        ('scaler', StandardScaler()),
        ('gdc', GradientBoostingClassifier(n_estimators = 1000, learning_rate=0.01)) 
    ]
).fit(X_train, y_train)
246/118:
gdc_pred = gdc_model.predict(X_test)
accuracy_score(y_test, gdc_pred)
246/119: At this point we need to reduce the features to get better predictions.
246/120: data_aj_x.columns.to_list()
246/121:
data_aj_x['summonerLevel_team1'] = data_aj_x.loc[:,['summonerLevel_1',
                                                    'summonerLevel_2',
                                                    'summonerLevel_3',
                                                    'summonerLevel_4',
                                                    'summonerLevel_5',]].sum(axis=1)
data_aj_x.head(5)
246/122:
data_aj_x['summonerLevel_team1'] = data_aj_x.loc[:,['summonerLevel_1',
                                                    'summonerLevel_2',
                                                    'summonerLevel_3',
                                                    'summonerLevel_4',
                                                    'summonerLevel_5',]].sum(axis=1)
data_aj_x['summonerLevel_team2'] = data_aj_x.loc[:,['summonerLevel_6',
                                                    'summonerLevel_7',
                                                    'summonerLevel_8',
                                                    'summonerLevel_9',
                                                    'summonerLevel_10',]].sum(axis=1)
data_aj_x.head(5)
246/123:
data_aj_x['summonerLevel_team1'] = data_aj_x.loc[:,['summonerLevel_1',
                                                    'summonerLevel_2',
                                                    'summonerLevel_3',
                                                    'summonerLevel_4',
                                                    'summonerLevel_5',]].sum(axis=1)
data_aj_x['summonerLevel_team2'] = data_aj_x.loc[:,['summonerLevel_6',
                                                    'summonerLevel_7',
                                                    'summonerLevel_8',
                                                    'summonerLevel_9',
                                                    'summonerLevel_10',]].sum(axis=1)
data_aj_x['championPoints_team1'] = data_aj_x.loc[:,['championPoints_1',
                                                    'championPoints_2',
                                                    'championPoints_3',
                                                    'championPoints_3',
                                                    'championPoints_5',]].sum(axis=1)
data_aj_x.head(5)
246/124:
data_aj_x['summonerLevel_team1'] = data_aj_x.loc[:,['summonerLevel_1',
                                                    'summonerLevel_2',
                                                    'summonerLevel_3',
                                                    'summonerLevel_4',
                                                    'summonerLevel_5',]].sum(axis=1)
data_aj_x['summonerLevel_team2'] = data_aj_x.loc[:,['summonerLevel_6',
                                                    'summonerLevel_7',
                                                    'summonerLevel_8',
                                                    'summonerLevel_9',
                                                    'summonerLevel_10',]].sum(axis=1)
data_aj_x['championPoints_team1'] = data_aj_x.loc[:,['championPoints_1',
                                                    'championPoints_2',
                                                    'championPoints_3',
                                                    'championPoints_3',
                                                    'championPoints_5',]].sum(axis=1)
data_aj_x['championPoints_team2'] = data_aj_x.loc[:,['championPoints_6',
                                                    'championPoints_7',
                                                    'championPoints_8',
                                                    'championPoints_9',
                                                    'championPoints_10']].sum(axis=1)
data_aj_x.head(5)
246/125:
data_aj_x['summonerLevel_team1'] = data_aj_x.loc[:,['summonerLevel_1',
                                                    'summonerLevel_2',
                                                    'summonerLevel_3',
                                                    'summonerLevel_4',
                                                    'summonerLevel_5']].sum(axis=1)
data_aj_x['summonerLevel_team2'] = data_aj_x.loc[:,['summonerLevel_6',
                                                    'summonerLevel_7',
                                                    'summonerLevel_8',
                                                    'summonerLevel_9',
                                                    'summonerLevel_10']].sum(axis=1)
data_aj_x['championPoints_team1'] = data_aj_x.loc[:,['championPoints_1',
                                                    'championPoints_2',
                                                    'championPoints_3',
                                                    'championPoints_3',
                                                    'championPoints_5']].sum(axis=1)
data_aj_x['championPoints_team2'] = data_aj_x.loc[:,['championPoints_6',
                                                    'championPoints_7',
                                                    'championPoints_8',
                                                    'championPoints_9',
                                                    'championPoints_10']].sum(axis=1)
data_aj_x.head(5)
246/126:
data_aj_x.columns.to_list()
data_aj_x
246/127:
data_aj_x.columns.to_list()
data_aj_x_a=data_aj_x
246/128:
data_aj_x['summonerLevel_team1'] = data_aj_x.loc[:,['summonerLevel_1',
                                                    'summonerLevel_2',
                                                    'summonerLevel_3',
                                                    'summonerLevel_4',
                                                    'summonerLevel_5']].sum(axis=1)
data_aj_x['summonerLevel_team2'] = data_aj_x.loc[:,['summonerLevel_6',
                                                    'summonerLevel_7',
                                                    'summonerLevel_8',
                                                    'summonerLevel_9',
                                                    'summonerLevel_10']].sum(axis=1)
data_aj_x['championPoints_team1'] = data_aj_x.loc[:,['championPoints_1',
                                                    'championPoints_2',
                                                    'championPoints_3',
                                                    'championPoints_3',
                                                    'championPoints_5']].sum(axis=1)
data_aj_x['championPoints_team2'] = data_aj_x.loc[:,['championPoints_6',
                                                    'championPoints_7',
                                                    'championPoints_8',
                                                    'championPoints_9',
                                                    'championPoints_10']].sum(axis=1)
data_aj_x.head(5)
247/1:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

%matplotlib inline
247/2: penguins = pd.read_csv('../data/penguins.csv').dropna().reset_index(drop = True)
247/3: penguins
247/4:
numeric_vars = ['bill_length_mm', 'bill_depth_mm',
       'flipper_length_mm', 'body_mass_g']
sns.pairplot(penguins[numeric_vars]);
247/5: pca_penguins = PCA().fit(penguins[numeric_vars])
247/6:
component_num = 0

components = pd.DataFrame({
    'variable': pca_penguins.feature_names_in_,
    'coordinate': pca_penguins.components_[component_num]
})

components
247/7: penguins[numeric_vars].var()
247/8:
# Fill in the code to answer the above questions.
pca_penguins.transform(penguins[numeric_vars].var())
247/9:
# Fill in the code to answer the above questions.
pca_penguins.transform(penguins[numeric_vars])
247/10: penguins_projection = pca_penguins.transform(penguins[numeric_vars])
247/11:
# Fill in the code to answer the above questions.
penguins_projection=pca_penguins.transform(penguins[numeric_vars])
247/12: penguins_projection.shape
247/13: penguins_projection[:,0].var(ddof=1)
247/14: pca_penguins.explained_variance_
247/15:
pca_pipe = Pipeline(steps = [
    ('scale', StandardScaler()),
    ('pca', PCA())
])

pca_pipe.fit(penguins[numeric_vars])
247/16:
component_num = 0

components = pd.DataFrame({
    'variable': pca_pipe.feature_names_in_,
    'coefficient': pca_pipe['pca'].components_[component_num]
})

components
247/17:
component_num = 0

components = pd.DataFrame({
    'variable': pca_pipe.feature_names_in_,
    'coefficient': pca_pipe['pca'].components_[component_num]
})

components
247/18: penguins_projection = pca_pipe.transform(penguins[numeric_vars])
247/19:
plt.figure(figsize = (12,8))

sns.scatterplot(x = penguins_projection[:,0], 
                y = penguins_projection[:,1], 
                hue = penguins['species'])

plt.xlabel('PCA1')
plt.ylabel('PCA2');
247/20: pca_pipe[-1].explained_variance_ratio_
248/1:
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import plotly.express as px
from ipywidgets import interact, IntSlider
from sklearn.datasets import fetch_openml

import warnings
warnings.filterwarnings('ignore')
248/2:
votes = pd.read_csv('../data/votes_117_2.csv').dropna()
votes['party'] = votes['senator'].str.extract('\((.)')
votes = votes.set_index(['senator', 'party'])
votes.head(2)
248/3:
pca_senators = Pipeline(
    steps = [
        ('ohe', OneHotEncoder(sparse = False)),
        ('pca', PCA(n_components = 2))
    ]
)
pca_senators.fit(votes)
248/4: pca_senators['pca'].explained_variance_ratio_
248/5:
senator_projection = pd.DataFrame(pca_senators.transform(votes),
                                  columns = ['PC1', 'PC2'], 
                                  index = votes.index).reset_index()

px.scatter(data_frame = senator_projection,
           x = 'PC1', y = 'PC2', color = 'party', hover_data = ['senator'])
248/6: X, y = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)
248/7:
X = X[:1000]
y = y[:1000]
248/8:
i = 0

print(f'Label: {y[i]}')
plt.imshow(X[i].reshape(28,28), cmap = plt.cm.gray_r);
248/9:
pca_pipe = Pipeline(steps = [
    ('scale', StandardScaler()),
    ('pca', PCA())
])

pca_pipe.fit(X)
248/10:
@interact(i = IntSlider(value = 0, min = 0, max = 28*28 - 1))
def show_component(i):
    plt.imshow((pca_pipe['pca'].components_[i]).reshape(28,28),
          cmap = plt.cm.gray_r, vmin = -0.1, vmax = 0.1)
248/11:
i = 0

@interact(dimensions = IntSlider(value = 1, min = 1, max = 28*28))
def show_component(dimensions):
    proj = np.matmul(np.matmul(pca_pipe['scale'].transform(X[i].reshape(1, -1)),
              np.transpose(pca_pipe['pca'].components_[:dimensions,:])),
              pca_pipe['pca'].components_[:dimensions,:])

    proj = pca_pipe['scale'].inverse_transform(proj.reshape(1, -1))

    fig, ax = plt.subplots(ncols = 2)
    ax[0].imshow(X[i].reshape(28, 28), cmap = plt.cm.gray_r, vmin = 0, vmax = 255)
    ax[0].set_title('Original Image')
    ax[1].imshow(proj.reshape(28,28), cmap = plt.cm.gray_r, vmin = 0, vmax = 255)
    ax[1].set_title(f'{dimensions} Components');
248/12:
component_num = 0

components = pd.DataFrame({
    'variable': pca_senators['ohe'].get_feature_names_ou(),
    'coefficient': pca_senators['pca'].components_[component_num]
})

components
248/13:
component_num = 0
components = pd.DataFrame({
    'variable': pca_senators['ohe'].get_feature_names_out(),
    'coefficient': pca_senators['pca'].components_[component_num]
})
components.sort_values('coefficient')
248/14:
i = 96

print(f'Label: {y[i]}')
plt.imshow(X[i].reshape(28,28), cmap = plt.cm.gray_r);
246/129:
#look at confusin matrix differently

from cm import plot_confusion_matrix

# from sklearn.metrics import plot_confusion_matrix
# from cm import plot_confusion_matrix_multiclass
246/130:
#look at confusin matrix differently

from cm import plot_confusion_matrix

from sklearn.metrics import plot_confusion_matrix
# from cm import plot_confusion_matrix_multiclass
246/131:
y_true = ["O", "A", "A"]
y_pred = ["A", "O", "O"]
cm=confusion_matrix(y_true, y_pred, labels=["O", "A"])
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()
246/132:
#look at confusin matrix differently

from cm import plot_confusion_matrix
240/72:

faults.loc[faults['spn'] == 5246]['fmi'].value_counts()
240/73: faults.loc[faults['spn'] == 1569]['fmi'].value_counts()
240/74: faults.loc[faults['fmi'] == 16]['spn'].value_counts()
240/75:
faults.loc[faults['fmi'] == 16]['spn'].value_counts()
faults.loc[faults['fmi'] == 16]['spn'].nunique()
240/76:
faults.loc[faults['fmi'] == 16]['spn'].value_counts()
faults.loc[faults['fmi'] == 16]['spn'].nunique() #51
240/77:
faults.loc[faults['fmi'] == 16]['spn'].value_counts()
# faults.loc[faults['fmi'] == 16]['spn'].nunique() #51
240/78:

faults.loc[faults['spn'] == 929]['fmi'].value_counts()
240/79:
# faults.loc[faults['spn'] == 5246]['fmi'].value_counts()
# faults.loc[faults['spn'] == 929]['fmi'].value_counts() #256516
faults.loc[faults['spn'] == 929]
240/80:
filter and 
faults[]

(faults.loc[(faults['latA_diff']) > 0.01) &
            (faults['latB_diff']) > 0.01) &
            (faults['latC_diff']) > 0.01) &
            (faults['longA_diff']) > 0.01) &
            (faults['longB_diff']) > 0.01) &
            (faults['longC_diff']) > 0.01)]
)
240/81:
# filter and 
# faults[]

(faults.loc[(faults['latA_diff']) > 0.01) &
            (faults['latB_diff']) > 0.01) &
            (faults['latC_diff']) > 0.01) &
            (faults['longA_diff']) > 0.01) &
            (faults['longB_diff']) > 0.01) &
            (faults['longC_diff']) > 0.01)]
)
240/82:
# filter and 
# faults[]

(faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
)
240/83:
# filter and 
# faults[]

(faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
240/84:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
240/85:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
faults.shape
240/86:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
240/87:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
(faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
240/88: faults.shape
240/89:
#find the rows close to service stations within 0.01 mile then droping them

faults['latA_diff'] = abs(faults['Latitude']-36.0666667)
faults['latB_diff'] = abs(faults['Latitude']-35.5883333)
faults['latC_diff'] = abs(faults['Latitude']-36.1950)
faults['longA_diff'] = abs(faults['Longitude']-(-86.4347222))
faults['longB_diff'] = abs(faults['Longitude']-(-86.4438888))
faults['longC_diff'] = abs(faults['Longitude']-(-83.174722))

faults

#abs() gives the result as absolute number
# now drop the rows which have values less than 0.01, meaning in proximity of the service station.
240/90:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
240/91: faults.shape
240/92:
# faults = pd.read_csv("../data/J1939Faults.csv") donot ignore warning low_memory=False as you loose lot of rows.

faults = pd.read_csv("../data/J1939Faults.csv", low_memory=False,
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
240/93:
#drop the illegitimate row with wrong  EventTimeStamp

faults.loc[(facults['EventTimeStamp']) > "2014-12-31"]
240/94:
#drop the illegitimate row with wrong  EventTimeStamp

faults.loc[(faults['EventTimeStamp']) > "2014-12-31"]
240/95:
#drop the illegitimate row with wrong  EventTimeStamp

faults.loc[(faults['EventTimeStamp']) > "2014-12-31"].shape
240/96:
import pandas as pd
from datetime import datetime
240/97:
# faults = pd.read_csv("../data/J1939Faults.csv") donot ignore warning low_memory=False as you loose lot of rows.

faults = pd.read_csv("../data/J1939Faults.csv", low_memory=False,
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
240/98: faults.head(5)
240/99: faults.isna().sum()
240/100:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults = faults.drop(columns = ['actionDescription', 'faultValue'])
faults
240/101:
#cleaning the faults data to hav equipmentID less than 5
#faults = 1187335


faults = faults[faults['EquipmentID'].str.len() <=5]
faults.shape #1185166, 18
240/102: faults.loc[faults['spn'] == 5246]
240/103: faults['EquipmentID'].nunique() #1045
240/104:
faults[(faults['spn'] == 1569) & (faults['fmi'] == 31)] #10717 with and without fmi=31

faults[(faults['spn'] == 1569) & (faults['fmi'] == 31)]['EquipmentID'].nunique() #498
240/105:
faults.loc[~(faults['spn'].isin([1569, 5426]))] #1174449
# faults.loc[~(faults['spn'] .isin([1569, 5426]))]['EquipmentID'].nunique() #1045
240/106:
faults[(faults['spn']==5246)] #1189
# faults[(faults['spn']==5246)]['EquipmentID'].nunique() #211

# same number of rows as for 1569 which is 10717 rows × 18 columns
240/107: (10717+1174449 +1189 )
240/108:
#find the rows close to service stations within 0.01 mile then droping them

faults['latA_diff'] = abs(faults['Latitude']-36.0666667)
faults['latB_diff'] = abs(faults['Latitude']-35.5883333)
faults['latC_diff'] = abs(faults['Latitude']-36.1950)
faults['longA_diff'] = abs(faults['Longitude']-(-86.4347222))
faults['longB_diff'] = abs(faults['Longitude']-(-86.4438888))
faults['longC_diff'] = abs(faults['Longitude']-(-83.174722))

faults

#abs() gives the result as absolute number
# now drop the rows which have values less than 0.01, meaning in proximity of the service station.
240/109:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
240/110: faults.shape
240/111:
#drop the illegitimate row with wrong  EventTimeStamp

faults.loc[(faults['EventTimeStamp']) > "2014-12-31"].shape
240/112:
#drop the illegitimate row with wrong  EventTimeStamp

faults = faults.loc[(faults['EventTimeStamp']) > "2014-12-31"].shape
240/113:
import pandas as pd
from datetime import datetime
240/114:
# faults = pd.read_csv("../data/J1939Faults.csv") donot ignore warning low_memory=False as you loose lot of rows.

faults = pd.read_csv("../data/J1939Faults.csv", low_memory=False,
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
240/115: faults.head(5)
240/116: faults.isna().sum()
240/117:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults = faults.drop(columns = ['actionDescription', 'faultValue'])
faults
240/118:
#cleaning the faults data to hav equipmentID less than 5
#faults = 1187335


faults = faults[faults['EquipmentID'].str.len() <=5]
faults.shape #1185166, 18
240/119: faults.loc[faults['spn'] == 5246]
240/120: faults['EquipmentID'].nunique() #1045
240/121:
faults[(faults['spn'] == 1569) & (faults['fmi'] == 31)] #10717 with and without fmi=31

faults[(faults['spn'] == 1569) & (faults['fmi'] == 31)]['EquipmentID'].nunique() #498
240/122:
faults.loc[~(faults['spn'].isin([1569, 5426]))] #1174449
# faults.loc[~(faults['spn'] .isin([1569, 5426]))]['EquipmentID'].nunique() #1045
240/123:
faults[(faults['spn']==5246)] #1189
# faults[(faults['spn']==5246)]['EquipmentID'].nunique() #211

# same number of rows as for 1569 which is 10717 rows × 18 columns
240/124: (10717+1174449 +1189 )
240/125:
#find the rows close to service stations within 0.01 mile then droping them

faults['latA_diff'] = abs(faults['Latitude']-36.0666667)
faults['latB_diff'] = abs(faults['Latitude']-35.5883333)
faults['latC_diff'] = abs(faults['Latitude']-36.1950)
faults['longA_diff'] = abs(faults['Longitude']-(-86.4347222))
faults['longB_diff'] = abs(faults['Longitude']-(-86.4438888))
faults['longC_diff'] = abs(faults['Longitude']-(-83.174722))

faults

#abs() gives the result as absolute number
# now drop the rows which have values less than 0.01, meaning in proximity of the service station.
240/126:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_prep = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
240/127: faults.shape
240/128:
# faults.shap/e #(1185166, 24)
faults_prep.shape
240/129:
#drop the illegitimate row with wrong  EventTimeStamp

faults_prep = faults.loc[(faults['EventTimeStamp']) > "2014-12-31"].shape
240/130:
# faults.loc[faults['spn'] == 5246]['fmi'].value_counts()
# faults.loc[faults['spn'] == 929]['fmi'].value_counts() #256516
faults_prep.loc[faults['spn'] == 929]
240/131:
#drop the illegitimate row with wrong  EventTimeStamp

faults_prep = faults.loc[(faults['EventTimeStamp']) > "2014-12-31"]
240/132:
# faults.loc[faults['spn'] == 5246]['fmi'].value_counts()
# faults.loc[faults['spn'] == 929]['fmi'].value_counts() #256516
faults_prep.loc[faults['spn'] == 929]
240/133: diagnostic.head(5)
250/1:
import pandas as pd
from datetime import datetime
250/2:
# faults = pd.read_csv("../data/J1939Faults.csv") donot ignore warning low_memory=False as you loose lot of rows.

faults = pd.read_csv("../data/J1939Faults.csv", low_memory=False,
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
250/3: faults.head(5)
250/4: faults.isna().sum()
250/5:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults = faults.drop(columns = ['actionDescription', 'faultValue'])
faults
250/6:
#cleaning the faults data to hav equipmentID less than 5
#faults = 1187335


faults = faults[faults['EquipmentID'].str.len() <=5]
faults.shape #1185166, 18
250/7: faults.loc[faults['spn'] == 5246]
250/8: faults['EquipmentID'].nunique() #1045
250/9:
faults[(faults['spn'] == 1569) & (faults['fmi'] == 31)] #10717 with and without fmi=31

faults[(faults['spn'] == 1569) & (faults['fmi'] == 31)]['EquipmentID'].nunique() #498
250/10:
faults.loc[~(faults['spn'].isin([1569, 5426]))] #1174449
# faults.loc[~(faults['spn'] .isin([1569, 5426]))]['EquipmentID'].nunique() #1045
250/11:
faults[(faults['spn']==5246)] #1189
# faults[(faults['spn']==5246)]['EquipmentID'].nunique() #211

# same number of rows as for 1569 which is 10717 rows × 18 columns
250/12: (10717+1174449 +1189 )
250/13:
#find the rows close to service stations within 0.01 mile then droping them

faults['latA_diff'] = abs(faults['Latitude']-36.0666667)
faults['latB_diff'] = abs(faults['Latitude']-35.5883333)
faults['latC_diff'] = abs(faults['Latitude']-36.1950)
faults['longA_diff'] = abs(faults['Longitude']-(-86.4347222))
faults['longB_diff'] = abs(faults['Longitude']-(-86.4438888))
faults['longC_diff'] = abs(faults['Longitude']-(-83.174722))

faults

#abs() gives the result as absolute number
# now drop the rows which have values less than 0.01, meaning in proximity of the service station.
250/14:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_prep = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
250/15:
# faults.shap/e #(1185166, 24)
faults_prep.shape #
250/16:
#drop the illegitimate row with wrong  EventTimeStamp

faults_prep = faults.loc[(faults['EventTimeStamp']) > "2014-12-31"]
250/17:
# faults.loc[faults['spn'] == 5246]['fmi'].value_counts()
# faults.loc[faults['spn'] == 929]['fmi'].value_counts() #256516
faults_prep.loc[faults['spn'] == 929]
250/18: faults.loc[faults['spn'] == 1569]['fmi'].value_counts()
250/19:
faults.loc[faults['fmi'] == 16]['spn'].value_counts()
# faults.loc[faults['fmi'] == 16]['spn'].nunique() #51
250/20: faults['EquipmentID'].value_counts().sort_values(ascending=False).to_frame().head(10)
250/21:
diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
# diagnostic.shape
# diagnostic.info
250/22: diagnostic.head(5)
250/23: diagnostic.isna().sum()
250/24: diagnostic.info
250/25: diagnostic.dtypes
250/26:
diagnostic['Value'].value_counts().to_frame().reset_index().head(50)
# 12821626 - 1009465
250/27:
diagnostic['FaultId'].value_counts().to_frame().reset_index()

diagnostic['FaultId'].nunique()
250/28:
diagnostic['Name'].value_counts().to_frame().reset_index()
# diagnostic['Name'].nunique()
250/29:
# diagnostic.pivot(columns='FaultId', index='Id', values=['Name', 'Value'])

diagnostic.pivot(columns='Name', index='FaultId', values= 'Value')
250/30: diagnostic.head(5)
250/31:
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
service.shape #(7124, 14)
250/32: service.head(5)
250/33:
# diagnostic.pivot(columns='FaultId', index='Id', values=['Name', 'Value'])

diagnostic_pivot = diagnostic.pivot(columns='Name', index='FaultId', values= 'Value')
250/34: diagnostic_pivot.describe()
250/35: diagnostic_pivot.info
250/36: faults[faults['spn'] == 5246]
250/37: faults[faults['spn'] == 5246].nunique()
250/38: faults[faults['spn'] == 5246].value_counts()
250/39: faults[faults['spn'] == 5246]
253/1:
import numpy as np
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.neural_network import MLPRegressor
from sklearn.decomposition import PCA
253/2: digits = load_digits(as_frame = True)
253/3:
mms = MinMaxScaler()
X = mms.fit_transform(digits['data'])
253/4:
pca_pipe = Pipeline(steps = [
    ('scale', StandardScaler()),
    ('pca', PCA(n_components=2))
])

pca_pipe.fit(X)

pca_projection = pca_pipe.transform(X)

plt.figure(figsize = (10,6))
sns.scatterplot(x = pca_projection[:,0],
                y = pca_projection[:,1],
                hue = digits['target'].astype('category'))
plt.legend(bbox_to_anchor = (1,1));
253/5:
autoencoder = MLPRegressor(hidden_layer_sizes = (2), max_iter = 10000)

autoencoder.fit(X, X)
253/6:
i = 5

result = autoencoder.predict(X[i].reshape(1,-1))

print(f'Label: {digits["target"][i]}')

fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))
ax[0].imshow(digits['images'][i], cmap = plt.cm.gray_r)
ax[0].set_title('Original Image')
ax[1].imshow(result.reshape(8,8), cmap = plt.cm.gray_r)
ax[1].set_title('Reconstruction');
253/7:
autoencoder = MLPRegressor(hidden_layer_sizes = (32,2,32), max_iter = 10000)

autoencoder.fit(X, X)
253/8:
i = 8

result = autoencoder.predict(X[i].reshape(1,-1))

print(f'Label: {digits["target"][i]}')

fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))
ax[0].imshow(digits['images'][i], cmap = plt.cm.gray_r)
ax[0].set_title('Original Image')
ax[1].imshow(result.reshape(8,8), cmap = plt.cm.gray_r)
ax[1].set_title('Reconstruction');
253/9:
encoder = MLPRegressor()
encoder.coefs_ = autoencoder.coefs_[:2]
encoder.intercepts_ = autoencoder.intercepts_[:2]
encoder.n_layers_ = 3
encoder.out_activation_ = 'relu'

projection = encoder.predict(X)

plt.figure(figsize = (10,6))
sns.scatterplot(projection[:,0], projection[:,1], hue = digits['target'].astype('category'))
plt.legend(bbox_to_anchor = (1,1));
253/10:
i = 0

result = autoencoder.predict(X[i].reshape(1,-1))

print(f'Label: {digits["target"][i]}')

fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))
ax[0].imshow(digits['images'][i], cmap = plt.cm.gray_r)
ax[0].set_title('Original Image')
ax[1].imshow(result.reshape(8,8), cmap = plt.cm.gray_r)
ax[1].set_title('Reconstruction');
253/11:
encoder = MLPRegressor()
encoder.coefs_ = autoencoder.coefs_[:2]
encoder.intercepts_ = autoencoder.intercepts_[:2]
encoder.n_layers_ = 3
encoder.out_activation_ = 'relu'

projection = encoder.predict(X)

plt.figure(figsize = (10,6))
sns.scatterplot(x=projection[:,0], y=projection[:,1], hue = digits['target'].astype('category'))
plt.legend(bbox_to_anchor = (1,1));
253/12:
autoencoder = MLPRegressor(hidden_layer_sizes = (64,48,32,2,32,48,64), 
                           max_iter = 10000, 
                           random_state = 123)

autoencoder.fit(X, X)
253/13:
i = 0

result = autoencoder.predict(X[i].reshape(1,-1))

print(f'Label: {digits["target"][i]}')

fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))
ax[0].imshow(digits['images'][i], cmap = plt.cm.gray_r)
ax[0].set_title('Original Image')
ax[1].imshow(result.reshape(8,8), cmap = plt.cm.gray_r)
ax[1].set_title('Reconstruction');
253/14:
encoder = MLPRegressor()
encoder.coefs_ = autoencoder.coefs_[:4]
encoder.intercepts_ = autoencoder.intercepts_[:4]
encoder.n_layers_ = 5
encoder.out_activation_ = 'relu'

projection = encoder.predict(X)

plt.figure(figsize = (10,6))
sns.scatterplot(x=projection[:,0], y=projection[:,1], hue = digits['target'].astype('category'))
plt.legend(bbox_to_anchor = (1,1));
253/15:
encoder = MLPRegressor()
encoder.coefs_ = autoencoder.coefs_[:4]
encoder.intercepts_ = autoencoder.intercepts_[:4]
encoder.n_layers_ = 5
encoder.out_activation_ = 'relu'

projection = encoder.predict(X)

plt.figure(figsize = (10,6))
sns.scatterplot(x=projection[:,0], y=projection[:,1], hue = digits['target'].astype('category'))
plt.legend(bbox_to_anchor = (1,1));
253/16:
decoder = MLPRegressor()
decoder.coefs_ = autoencoder.coefs_[4:]
decoder.intercepts_ = autoencoder.intercepts_[4:]
decoder.n_layers_ = 5
decoder.out_activation_ = 'identity'
253/17:
decode_x = 1.5
decode_y = 1.5

plt.imshow(decoder.predict(np.array([decode_x,decode_y]).reshape(1,-1)).reshape(8,8),cmap = plt.cm.gray_r);
253/18:
decode_x = 0
decode_y = 1

plt.imshow(decoder.predict(np.array([decode_x,decode_y]).reshape(1,-1)).reshape(8,8),cmap = plt.cm.gray_r);
254/1:
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import numpy as np
import umap

import warnings
warnings.filterwarnings('ignore')
254/2: conda install -c conda-forge umap-learn
254/3:
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import numpy as np
import umap

import warnings
warnings.filterwarnings('ignore')
254/4: X, y = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)
254/5:
X = X[:5000]
y = y[:5000]
254/6:
pca_pipe = Pipeline(steps = [
    ('scale', StandardScaler()),
    ('pca', PCA(n_components = 2))
])

pca_pipe.fit(X)
254/7: pca_projection = pca_pipe.transform(X)
254/8:
plt.figure(figsize = (10,6))
sns.scatterplot(pca_projection[:,0], pca_projection[:,1], 
                hue = pd.Series(y[:]).astype('category'),
               alpha = 0.7);
254/9:
plt.figure(figsize = (10,6))
sns.scatterplot(x=pca_projection[:,0], y=pca_projection[:,1], 
                hue = pd.Series(y[:]).astype('category'),
               alpha = 0.7);
251/1:
import json
import pandas as pd
251/2:
with open('../data/BR1_2665363619_timeline.json') as fi:
    timeline = json.load(fi)

events = []

for frame in timeline['info']['frames']:
    events.extend(frame['events'])
251/3:
events = pd.DataFrame(events)[['participantId', 'timestamp', 'type']].dropna()
events.head()
251/4:
events = pd.DataFrame(events)[['participantId', 'timestamp', 'type']].dropna()
events
250/40:
diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
diagnostic.shape
 diagnostic.info
250/41:
diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
diagnostic.shape
diagnostic.info
250/42: faults.columns.to_list()
250/43:
# faults = pd.read_csv("../data/J1939Faults.csv") 
# this call gave warning 
# donot ignore warning low_memory=False as you loose lot of rows.
# Also it has date time so read using datetime we may need it later.

faults = pd.read_csv("../data/J1939Faults.csv", low_memory=False,
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
250/44: faults.columns.to_list()
250/45: faults.isna().sum()
250/46:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults = faults.drop(columns = ['actionDescription', 'faultValue'])
faults
250/47:
# faults = pd.read_csv("../data/J1939Faults.csv") 
# this call gave warning 
# donot ignore warning low_memory=False as you loose lot of rows.
# Also it has date time so read using datetime we may need it later.

faults = pd.read_csv("../data/J1939Faults.csv", low_memory=False,
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
250/48:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults = faults.drop(columns = ['actionDescription', 'faultValue'])
faults
250/49: faults[faults['EquipmentID'].str.len() <=5].len()
250/50: len(faults[faults['EquipmentID'].str.len() <=5])
250/51: len(faults[faults['EquipmentID'].str.len() >5])
250/52:
#cleaning the faults data to hav equipmentID less than 5
#faults = 1187335


faults = faults[faults['EquipmentID'].str.len() <=5]
faults.shape #1185166, 18
250/53: faults['EquipmentID'].value_counts()
250/54: faults['EquipmentID'].value_counts().tail(20)
250/55: faults['EquipmentID'].value_counts().tail(50)
250/56: faults['EquipmentID'].value_counts().plot()
250/57: faults['EquipmentID']==1872
250/58: faults[faults['EquipmentID']==1872]
250/59: faults[faults['EquipmentID']=='1872']
250/60: faults[faults['EquipmentID']=='1872']['spn'].value_counts()
250/61: faults['EquipmentID'].value_counts.to_frame()
250/62: faults['EquipmentID'].value_counts().to_frame()
250/63:
#cleaning the faults data to hav equipmentID less than or equall to 5
#faults = 1187335


faults = faults[faults['EquipmentID'].str.len() <5]
faults.shape #1185166, 18
250/64:
#find the rows close to service stations within 0.01 mile then droping them

faults['latA_diff'] = abs(faults['Latitude']-36.0666667)
faults['latB_diff'] = abs(faults['Latitude']-35.5883333)
faults['latC_diff'] = abs(faults['Latitude']-36.1950)
faults['longA_diff'] = abs(faults['Longitude']-(-86.4347222))
faults['longB_diff'] = abs(faults['Longitude']-(-86.4438888))
faults['longC_diff'] = abs(faults['Longitude']-(-83.174722))

faults

#abs() gives the result as absolute number
# now drop the rows which have values less than 0.01, meaning in proximity of the service station.
250/65: faults['EquipmentID'].value_counts().to_frame()
250/66:
#cleaning the faults data to hav equipmentID less than or equall to 5
#faults = 1187335


faults = faults[faults['EquipmentID'].str.len() <=5]
faults.shape #1185166, 18
250/67:
#find the rows close to service stations within 0.01 mile then droping them

faults['latA_diff'] = abs(faults['Latitude']-36.0666667)
faults['latB_diff'] = abs(faults['Latitude']-35.5883333)
faults['latC_diff'] = abs(faults['Latitude']-36.1950)
faults['longA_diff'] = abs(faults['Longitude']-(-86.4347222))
faults['longB_diff'] = abs(faults['Longitude']-(-86.4438888))
faults['longC_diff'] = abs(faults['Longitude']-(-83.174722))

faults

#abs() gives the result as absolute number
# now drop the rows which have values less than 0.01, meaning in proximity of the service station.
250/68: faults['EquipmentID'].value_counts().to_frame() #1042
250/69: faults['EquipmentID'].nunique() #1045
250/70:
#find the rows close to service stations within 0.01 mile then droping them

faults['latA_diff'] = abs(faults['Latitude']-36.0666667)
faults['latB_diff'] = abs(faults['Latitude']-35.5883333)
faults['latC_diff'] = abs(faults['Latitude']-36.1950)
faults['longA_diff'] = abs(faults['Longitude']-(-86.4347222))
faults['longB_diff'] = abs(faults['Longitude']-(-86.4438888))
faults['longC_diff'] = abs(faults['Longitude']-(-83.174722))

faults

#abs() gives the result as absolute number
# now drop the rows which have values less than 0.01, meaning in proximity of the service station.
250/71: faults['EquipmentID'].value_counts().to_frame() #1042
250/72: faults.loc[faults['spn'] == 5246]
250/73: faults['EquipmentID'].nunique() #1045
250/74:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_prep = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
250/75:
# faults.shap/e #(1185166, 24)
faults_prep.shape #
250/76:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_prep = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
250/77:
# faults.shap/e #(1185166, 24)
faults_prep.shape #
250/78:
#drop the illegitimate row with wrong  EventTimeStamp

faults_prep = faults.loc[(faults['EventTimeStamp']) > "2014-12-31"]
250/79:
# faults.loc[faults['spn'] == 5246]['fmi'].value_counts()
# faults.loc[faults['spn'] == 929]['fmi'].value_counts() #256516
faults_prep.loc[faults['spn'] == 929]
250/80:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_prep = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
250/81:
# faults.shap/e #(1185166, 24)
faults_prep.shape #
250/82:
#drop the illegitimate row with wrong  EventTimeStamp

faults_prep = faults_prep.loc[(faults['EventTimeStamp']) > "2014-12-31"]
250/83:
# faults.loc[faults['spn'] == 5246]['fmi'].value_counts()
# faults.loc[faults['spn'] == 929]['fmi'].value_counts() #256516
faults_prep.loc[faults['spn'] == 929]
250/84:
#droping the active status which is False
faults_prep = faults_prep.loc[(faults['active']) = 'False']
250/85:
#droping the active status which is False
faults_prep = faults_prep.loc[(faults['active']) == 'False']
250/86:
#droping the active status which is False
faults_prep = faults_prep.loc[(faults['active']) == 'False']
faults_prep.shape
250/87:
#droping the active status which is False
faults_prep = faults_prep.loc[(faults['active']) != 'False']
faults_prep.shape
250/88:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_prep = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
250/89:
# faults.loc[faults['spn'] == 5246]['fmi'].value_counts()
# faults.loc[faults['spn'] == 929]['fmi'].value_counts() #256516
faults_prep.loc[faults['spn'] == 929]
250/90:
#droping the active status which is False
# faults_prep = faults_prep.loc[(faults['active']) != 'False']
faults_prep.shape
250/91:
#drop the illegitimate row with wrong  EventTimeStamp

faults_prep = faults_prep.loc[(faults_prep['EventTimeStamp']) > "2014-12-31"]
250/92:
# faults.loc[faults['spn'] == 5246]['fmi'].value_counts()
# faults.loc[faults['spn'] == 929]['fmi'].value_counts() #256516
faults_prep.loc[faults['spn'] == 929]
250/93:
#droping the active status which is False

faults_prep.shape

# faults_prep.loc[(faults['active']) != 'False']
250/94: faults_prep.loc[(faults_prep['active']) != "False"]
250/95: faults_prep.loc[(faults_prep['active']) != "False"].shape
250/96:
#drop the illegitimate row with wrong  EventTimeStamp

faults_prep = faults_prep.loc[(faults_prep['EventTimeStamp']) > "2014-12-31"]
250/97:
# faults.shap/e #(1185166, 24)
faults_prep.shape #
250/98: faults_prep.loc[(faults_prep['active']) != "False"].shape #(1030183, 18)
250/99: faults_prep.loc[(faults_prep['active']) != "False"] #(1030183, 18)
250/100: faults_prep['active'].value_counts()
250/101: faults_prep.loc[(faults_prep['active']) == "True"] #(1030183, 18)
250/102: faults_prep.loc[(faults_prep['active']) != False] #(1030183, 18)
250/103: faults_clean = faults_prep.loc[(faults_prep['active']) != False] #533192 rows × 18 columns
250/104: faults_clean.loc[faults['spn'] == 1569]['fmi'].value_counts()
250/105:
# faults.loc[faults['spn'] == 5246]['fmi'].value_counts()
# faults.loc[faults['spn'] == 929]['fmi'].value_counts() #256516
faults_clean.loc[faults['spn'] == 929]
250/106:
faults_clean.loc[faults['fmi'] == 16]['spn'].value_counts()
# faults.loc[faults['fmi'] == 16]['spn'].nunique() #51
250/107: faults_clean['EquipmentID'].value_counts().sort_values(ascending=False).to_frame().head(10)
250/108: faults_clean['EquipmentID'].value_counts().sort_values(ascending=False).to_frame()
250/109:
# faults = pd.read_csv("../data/J1939Faults.csv") 
# this call gave warning 
# donot ignore warning low_memory=False as you loose lot of rows.
# Also it has date time so read using datetime we may need it later.

faults = pd.read_csv("../data/J1939Faults.csv", low_memory=False,
                    parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
faults.shape #(1187335, 20)
faults.info()
250/110:
# as the actionDescription and faultValue  are all NaN values will drop them.

faults = faults.drop(columns = ['actionDescription', 'faultValue'])
faults
250/111:
#total number of rows more than 5
len(faults[faults['EquipmentID'].str.len() >5])
250/112:
#cleaning the faults data to hav equipmentID less than or equall to 5
#faults = 1187335
# faults = faults[faults['EquipmentID'].str.len() <= 5]
faults = faults[faults['EquipmentID'].str.len() < 5]
faults.shape #1185166, 18

#there are 1042 trucks which have ID less than 5 and 
# 3 that are equal to 5 with alpha numeric chaarcters.
250/113:
#find the rows close to service stations within 0.01 mile then droping them

faults['latA_diff'] = abs(faults['Latitude']-36.0666667)
faults['latB_diff'] = abs(faults['Latitude']-35.5883333)
faults['latC_diff'] = abs(faults['Latitude']-36.1950)
faults['longA_diff'] = abs(faults['Longitude']-(-86.4347222))
faults['longB_diff'] = abs(faults['Longitude']-(-86.4438888))
faults['longC_diff'] = abs(faults['Longitude']-(-83.174722))

faults

#abs() gives the result as absolute number
# now drop the rows which have values less than 0.01, meaning in proximity of the service station.
250/114:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_prep = (faults.loc[((faults['latA_diff']) > 0.01) &
            ((faults['latB_diff']) > 0.01) &
            ((faults['latC_diff']) > 0.01) &
            ((faults['longA_diff']) > 0.01) &
            ((faults['longB_diff']) > 0.01) &
            ((faults['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
250/115:
#drop the illegitimate row with wrong  EventTimeStamp
faults_prep.loc[faults['EventTimeStamp'].dt.year > 2011].shape
# faults_prep = faults_prep.loc[(faults_prep['EventTimeStamp']) > "2014-12-31"] going to use date and time after 2011.
250/116:
#drop the illegitimate row with wrong  EventTimeStamp
# faults_prep.loc[faults['EventTimeStamp'].dt.year > 2011].shape
faults_prep = faults_prep.loc[(faults_prep['EventTimeStamp']) > "2014-12-31"] going to use date and time after 2011.
250/117:
#drop the illegitimate row with wrong  EventTimeStamp
# faults_prep.loc[faults['EventTimeStamp'].dt.year > 2011].shape
faults_prep = faults_prep.loc[(faults_prep['EventTimeStamp']) > "2014-12-31"] #going to use date and time after 2011.
250/118:
# faults.shap/e #(1185166, 24)
faults_prep.shape #
250/119: faults_prep['active'].value_counts()
250/120: faults_clean = faults_prep.loc[(faults_prep['active']) != False] #533192 rows × 18 columns
250/121: faults_clean['EquipmentID'].value_counts().sort_values(ascending=False).to_frame() #there are a total of 1038 equipment
250/122: faults_clean[faults['spn'] == 5246] #1189 rows
250/123: faults_celan.shape
250/124: faults_clean.shape
250/125:
#total number of rows more than 5
len(faults[faults['EquipmentID'].str.len() =5])
250/126:
#total number of rows more than 5
len(faults[faults['EquipmentID'].str.len() == 5])
250/127:
#total number of rows more than 5
len(faults[faults['EquipmentID'].str.len() > 5])
250/128:
#total number of rows more than 5
len(faults[faults['EquipmentID'].str.len() < 5])
250/129: faults_clean[(faults_clean['EquipmentID'] == '302') & (faults['spn'] == 5246)]
250/130: faults_clean[(faults_clean['EquipmentID'] == '1499') & (faults['spn'] == 5246)]
256/1:
#Libraries need for work

import pandas as pd
from datetime import datetime



from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from pycaret.classification import *
from pycaret.regression import *
from pycaret.time_series import *

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
256/2:
#Libraries need for work

import pandas as pd
from datetime import datetime



from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

#we need to create a conda environment for pycaret and then pip install the packages after activating and rerun the notebook.
# from pycaret.classification import *
# from pycaret.regression import *
# from pycaret.time_series import *

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
256/3:
Reading the data files

faults = pd.read_csv("../data/J1939Faults.csv",low_memory=False,
                     parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
    #initial reading we saw it has date time object so we will keep them as date time. 
    #Also we saw the waning for memmory because of which we were loosing observations, so we will have low_memmory =False

diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
256/4:
#Reading the data files

faults = pd.read_csv("../data/J1939Faults.csv",low_memory=False,
                     parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
    #initial reading we saw it has date time object so we will keep them as date time. 
    #Also we saw the waning for memmory because of which we were loosing observations, so we will have low_memmory =False

diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
256/5: faults.shape
256/6:
faults.shape #(1187335, 20)
diagnostic.shape
256/7:
faults.shape #(1187335, 20)
diagnostic.shape #(12821626, 4)
service.shape
256/8: faults.head(2)
256/9: faults.info()
256/10: faults.describe()
256/11:
faults.describe()
faults.isna.sum()
256/12:
faults.describe()
faults.isna().sum()
256/13: faults.columns.to_list()
256/14: faults.columns
256/15:
faults.columns
faults['EventTimeStamp'].info()
256/16:
faults.columns
faults['EventTimeStamp'].describe()
256/17:
faults.columns
faults['EventTimeStamp'].value_counts()
256/18:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].sort_values()
256/19:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].sort_values(ascending = False)
256/20:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].year.sort_values(ascending = False)
256/21:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].dt.year.sort_values(ascending = False)
256/22:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].dt.year.value_counts().sort_values(ascending = False) #date time has year dt.year extracts year from it
256/23:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].dt.year.value_counts().sort_values(ascending = False).plot()
#date time has year dt.year extracts year from it
256/24:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].dt.year.value_counts().sort_values(ascending = False).plt.bar()
#date time has year dt.year extracts year from it
256/25:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].dt.year.value_counts().sort_values(ascending = False).plot('bar')
#date time has year dt.year extracts year from it
256/26:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].dt.year.value_counts().sort_values(ascending = False).plot()
#date time has year dt.year extracts year from it
256/27:
faults.columns
faults['EventTimeStamp'].value_counts()
plt.bar(faults['EventTimeStamp'].dt.year.value_counts().sort_values(ascending = False))
#date time has year dt.year extracts year from it
256/28:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].dt.year.value_counts().sort_values(ascending = False).plot(kind="bar", 
                                                                                    title = 'Observations_per_year', 
                                                                                    xlabel='', ylabel='Number of Observations')
#date time has year dt.year extracts year from it
256/29:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].dt.year.value_counts().plot(kind="bar", 
                                                                                    title = 'Observations_per_year', 
                                                                                    xlabel='', ylabel='Number of Observations')
#date time has year dt.year extracts year from it
256/30:
faults.columns
faults['EventTimeStamp'].value_counts()
faults['EventTimeStamp'].dt.year.value_counts().sort_values(ascending = False).plot(kind="bar", 
                                                                                    title = 'Observations_per_year', 
                                                                                    xlabel='', ylabel='Number of Observations')
#date time has year dt.year extracts year from it
256/31: faults['EventTimeStamp'].dt.year.unique()
256/32:
faults['EventTimeStamp'].dt.year.unique()
faults['EventTimeStamp'].dt.year.value_counts()
256/33:
faults.columns
faults['EventTimeStamp'].value_counts()
(faults['EventTimeStamp']
 .dt.year
 .value_counts()
 .sort_values(ascending = False)
 .plot(kind="bar", 
       title = 'Observations_per_year',
       xlabel='', ylabel='Number of Observations'))
#date time has year dt.year extracts year from it
256/34:
(faults['LocationTimeStamp']
 .dt.year
 .value_counts()
 .sort_values(ascending = False)
 .plot(kind="bar", 
       title = 'Observations_per_year',
       xlabel='', ylabel='Number of Observations'))
#date time has year dt.year extracts year from it
256/35:
faults['EventTimeStamp'].dt.year.unique()
faults['EventTimeStamp'].dt.year.value_counts()
faults['LocationTimeStamp'].dt.year.value_counts()
256/36: faults['EquipmentID'].nunique()
256/37:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts()
256/38:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts().sort_values.plot(kind='bar')
256/39:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts().sort_values().plot(kind='bar')
256/40:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts().sort_values(ascending=False).plot(kind='bar')
256/41:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts().sort_values(ascending=False).plot(kind='bar',
                                                                      fontsize=6)
256/42:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts().sort_values(ascending=False).plot(kind='bar',
                                                                      xlab = fontsize=6)
256/43:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts().sort_values(ascending=False).plot(kind='bar',
                                                                      xlab(fontsize=6))
256/44:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts().sort_values(ascending=False).plot(kind='bar',
                                                                      xlab(size=6))
256/45:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts().sort_values(ascending=False).plot(kind='bar',xlab(size=6))
256/46:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts().sort_values(ascending=False).plot(kind='bar',xlabel(size=6))
256/47:
faults['EquipmentID'].nunique() #1122
faults['EquipmentID'].value_counts().sort_values()
256/48:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel='',
       ylabel='Number of Observations'))
256/49:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= ('', rotation ="vertical"),
       ylabel='Number of Observations'))
256/50:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= (''),
       ylabel='Number of Observations'))
256/51:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= ('', rot=90),
       ylabel='Number of Observations'))
256/52:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= ('', ='vertical'),
       ylabel='Number of Observations'))
256/53:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= ('', rotation='vertical'),
       ylabel='Number of Observations'))
256/54:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= ('', rot=0),
       ylabel='Number of Observations'))
256/55:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= (''),
       ylabel='Number of Observations'))
256/56: faults['EquipmentID'].str.len().value_counts()
256/57:
#the EuipmentId is a string character and has length varrying from character lnrth 3 4, 5, and 9 and 10

faults['EquipmentID'].str.len().value_counts().plot(kind="bar", 
                                                    xlabel = "Number of characters in string",
                                                   ylabel="#of obsevations")
256/58:
#the EuipmentId is a string character and has length varrying from character lnrth 3 4, 5, and 9 and 10

faults['EquipmentID'].str.len().value_counts()
256/59:
#the EuipmentId is a string character and has length varrying from character lnrth 3 4, 5, and 9 and 10

faults['EquipmentID'].str.len().value_counts().sort_values(ascending=False).to_frame()
256/60:
#the EuipmentId is a string character and has length varrying from character lnrth 3 4, 5, and 9 and 10

faults['EquipmentID'].str.len().value_counts().sort_values(ascending=False).to_frame()
faults['EquipmentID'].str.len().nunique()
256/61:
#the EuipmentId is a string character and has length varrying from character lnrth 3 4, 5, and 9 and 10
faults['EquipmentID'].str.len().nunique() #5
faults['EquipmentID'].str.len().value_counts().sort_values(ascending=False).to_frame()
256/62: ** cleaning the faults the df**
256/63: faults['active'].value_counts()
256/64: faults['spn'].value_counts()
256/65: faults['spn'].nunique()
256/66:
faults['spn'].nunique() #450
faults['fmi'].nunique()
256/67:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults(faults['spn']==1569)
256/68:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)]
256/69:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape
256/70:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5426)].shape
256/71:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape
256/72:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults.groupby('EquipmentId')[(faults['spn'] == 1569)]
256/73:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults.groupby('EquipmentID').faults[(faults['spn'] == 1569)]
256/74:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults.groupby('EquipmentID')[(faults['spn'] == 1569)]
256/75:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults.groupby('EquipmentID')[['spn'] == 1569]
256/76:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults[(faults['spn'] == 5246)]['EquipmentID'].nunique()
256/77:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults[(faults['spn'] == 5246)]['EquipmentID'].nunique() #215 trucks out of 1122 have experienced full derate
faults[(faults['spn'] == 1569)]['EquipmentID'].nunique()
256/78:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults[(faults['spn'] == 5246)]['EquipmentID'].nunique() #215 trucks out of 1122 have experienced full derate
faults[(faults['spn'] == 1569)]['EquipmentID'].nunique() #506 trucks out of 1122 have experienced 75% derate

faults[(faults['spn'] == 1569)]['fmi'].nunique()
256/79:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults[(faults['spn'] == 5246)]['EquipmentID'].nunique() #215 trucks out of 1122 have experienced full derate
faults[(faults['spn'] == 1569)]['EquipmentID'].nunique() #506 trucks out of 1122 have experienced 75% derate

faults[(faults['spn'] == 1569)]['fmi'].nunique() 
faults[(faults['spn'] == 1569)]['fmi'].unique() #only one fmi code
256/80:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults[(faults['spn'] == 5246)]['EquipmentID'].nunique() #215 trucks out of 1122 have experienced full derate
faults[(faults['spn'] == 1569)]['EquipmentID'].nunique() #506 trucks out of 1122 have experienced 75% derate

faults[(faults['spn'] == 1569)]['fmi'].nunique() 
faults[(faults['spn'] == 1569)]['fmi'].unique() #only one fmi code which is 31 with spn 1569

faults[(faults['spn'] == 5246)]['fmi'].nunique()
256/81:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults[(faults['spn'] == 5246)]['EquipmentID'].nunique() #215 trucks out of 1122 have experienced full derate
faults[(faults['spn'] == 1569)]['EquipmentID'].nunique() #506 trucks out of 1122 have experienced 75% derate

faults[(faults['spn'] == 1569)]['fmi'].nunique() 
faults[(faults['spn'] == 1569)]['fmi'].unique() #only one fmi code which is 31 with spn 1569

faults[(faults['spn'] == 5246)]['fmi'].nunique() 
faults[(faults['spn'] == 5246)]['fmi'].unique() #only five fmi code which is 31 with spn 1569
256/82:
#finding the distance from the nearest service stations
faults_clean = faults
faults_clean['latA_diff'] = abs(faults_clean['Latitude']-36.0666667)
faults_clean['latB_diff'] = abs(faults_clean['Latitude']-35.5883333)
faults_clean['latC_diff'] = abs(faults_clean['Latitude']-36.1950)
faults_clean['longA_diff'] = abs(faults_clean['Longitude']-(-86.4347222))
faults_clean['longB_diff'] = abs(faults_clean['Longitude']-(-86.4438888))
faults_clean['longC_diff'] = abs(faults_clean['Longitude']-(-83.174722))

faults_clean
256/83:
#finding the distance from the nearest service stations
faults_clean = faults
faults_clean['latA_diff'] = abs(faults_clean['Latitude']-36.0666667)
faults_clean['latB_diff'] = abs(faults_clean['Latitude']-35.5883333)
faults_clean['latC_diff'] = abs(faults_clean['Latitude']-36.1950)
faults_clean['longA_diff'] = abs(faults_clean['Longitude']-(-86.4347222))
faults_clean['longB_diff'] = abs(faults_clean['Longitude']-(-86.4438888))
faults_clean['longC_diff'] = abs(faults_clean['Longitude']-(-83.174722))

faults_clean #1187335 rows × 26 columns
faults
256/84:
#Reading the data files

faults = pd.read_csv("../data/J1939Faults.csv",low_memory=False,
                     parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
    #initial reading we saw it has date time object so we will keep them as date time. 
    #Also we saw the waning for memmory because of which we were loosing observations, so we will have low_memmory =False

diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
256/85: faults.shape
256/86:
faullts_clean = faults
faults_clean.shape
256/87:
#Libraries need for work

import pandas as pd
from datetime import datetime



from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

#we need to create a conda environment for pycaret and then pip install the packages after activating and rerun the notebook.
# from pycaret.classification import *
# from pycaret.regression import *
# from pycaret.time_series import *

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
256/88:
#Reading the data files

faults = pd.read_csv("../data/J1939Faults.csv",low_memory=False,
                     parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
    #initial reading we saw it has date time object so we will keep them as date time. 
    #Also we saw the waning for memmory because of which we were loosing observations, so we will have low_memmory =False

diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
256/89:
faults.shape #(1187335, 20)
diagnostic.shape #(12821626, 4)
service.shape #service.shape #
256/90: faults.head(2)
256/91: faults.info()
256/92:
faults.describe()
faults.isna().sum()
256/93:
faults.columns
faults['EventTimeStamp'].value_counts()
(faults['EventTimeStamp']
 .dt.year
 .value_counts()
 .sort_values(ascending = False)
 .plot(kind="bar", 
       title = 'Observations_per_year',
       xlabel='', ylabel='Number of Observations'))
#date time has year dt.year extracts year from it
256/94:
(faults['LocationTimeStamp']
 .dt.year
 .value_counts()
 .sort_values(ascending = False)
 .plot(kind="bar", 
       title = 'Observations_per_year',
       xlabel='',
       ylabel='Number of Observations'))
#date time has year dt.year extracts year from it
256/95:
faults['EventTimeStamp'].dt.year.unique()
faults['EventTimeStamp'].dt.year.value_counts()
faults['LocationTimeStamp'].dt.year.value_counts()
256/96:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= (''),
       ylabel='Number of Observations'))
256/97:
#the EuipmentId is a string character and has length varrying from character lnrth 3 4, 5, and 9 and 10
faults['EquipmentID'].str.len().nunique() #5
faults['EquipmentID'].str.len().value_counts().sort_values(ascending=False).to_frame()
256/98: faults['active'].value_counts()
256/99:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults[(faults['spn'] == 5246)]['EquipmentID'].nunique() #215 trucks out of 1122 have experienced full derate
faults[(faults['spn'] == 1569)]['EquipmentID'].nunique() #506 trucks out of 1122 have experienced 75% derate

faults[(faults['spn'] == 1569)]['fmi'].nunique() 
faults[(faults['spn'] == 1569)]['fmi'].unique() #only one fmi code which is 31 with spn 1569

faults[(faults['spn'] == 5246)]['fmi'].nunique() 
faults[(faults['spn'] == 5246)]['fmi'].unique() #only five fmi code which is [ 0, 15, 16, 19, 14] with spn 5246
256/100: faults.shape #(1187335, 20)
256/101:
faullts_clean = faults
faults_clean.shape
256/102:
#finding the distance from the nearest service stations

faults_clean['latA_diff'] = abs(faults_clean['Latitude']-36.0666667)
faults_clean['latB_diff'] = abs(faults_clean['Latitude']-35.5883333)
faults_clean['latC_diff'] = abs(faults_clean['Latitude']-36.1950)
faults_clean['longA_diff'] = abs(faults_clean['Longitude']-(-86.4347222))
faults_clean['longB_diff'] = abs(faults_clean['Longitude']-(-86.4438888))
faults_clean['longC_diff'] = abs(faults_clean['Longitude']-(-83.174722))

faults_clean #1187335 rows × 26 columns
256/103:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults['latA_diff']) > 0.01) &
            ((faults_clean['latB_diff']) > 0.01) &
            ((faults_clean['latC_diff']) > 0.01) &
            ((faults_clean['longA_diff']) > 0.01) &
            ((faults_clean['longB_diff']) > 0.01) &
            ((faults_clean['longC_diff']) > 0.01)]
).drop(faults_clean['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
256/104:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults['latA_diff']) > 0.01) &
            ((faults_clean['latB_diff']) > 0.01) &
            ((faults_clean['latC_diff']) > 0.01) &
            ((faults_clean['longA_diff']) > 0.01) &
            ((faults_clean['longB_diff']) > 0.01) &
            ((faults_clean['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
256/105:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults['latA_diff']) > 0.01) &
            ((faults_clean['latB_diff']) > 0.01) &
            ((faults_clean['latC_diff']) > 0.01) &
            ((faults_clean['longA_diff']) > 0.01) &
            ((faults_clean['longB_diff']) > 0.01) &
            ((faults_clean['longC_diff']) > 0.01)]
)
256/106:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
(faults_clean.loc[((faults_clean['latA_diff']) > 0.01) &
            ((faults_clean['latB_diff']) > 0.01) &
            ((faults_clean['latC_diff']) > 0.01) &
            ((faults_clean['longA_diff']) > 0.01) &
            ((faults_clean['longB_diff']) > 0.01) &
            ((faults_clean['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
256/107:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults_clean['latA_diff']) > 0.01) &
            ((faults_clean['latB_diff']) > 0.01) &
            ((faults_clean['latC_diff']) > 0.01) &
            ((faults_clean['longA_diff']) > 0.01) &
            ((faults_clean['longB_diff']) > 0.01) &
            ((faults_clean['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
256/108:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults_clean['latA_diff']) > 0.01) &
            ((faults_clean['latB_diff']) > 0.01) &
            ((faults_clean['latC_diff']) > 0.01) &
            ((faults_clean['longA_diff']) > 0.01) &
            ((faults_clean['longB_diff']) > 0.01) &
            ((faults_clean['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
faults_clean.shape
256/109:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults_clean['latA_diff']) > 0.01) &
            ((faults_clean['latB_diff']) > 0.01) &
            ((faults_clean['latC_diff']) > 0.01) &
            ((faults_clean['longA_diff']) > 0.01) &
            ((faults_clean['longB_diff']) > 0.01) &
            ((faults_clean['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
256/110:
#finding the distance from the nearest service stations

faults_clean['latA_diff'] = abs(faults_clean['Latitude']-36.0666667)
faults_clean['latB_diff'] = abs(faults_clean['Latitude']-35.5883333)
faults_clean['latC_diff'] = abs(faults_clean['Latitude']-36.1950)
faults_clean['longA_diff'] = abs(faults_clean['Longitude']-(-86.4347222))
faults_clean['longB_diff'] = abs(faults_clean['Longitude']-(-86.4438888))
faults_clean['longC_diff'] = abs(faults_clean['Longitude']-(-83.174722))

faults_clean #1187335 rows × 26 columns
256/111:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
(faults_clean.loc[((faults_clean['latA_diff']) > 0.01) &
            ((faults_clean['latB_diff']) > 0.01) &
            ((faults_clean['latC_diff']) > 0.01) &
            ((faults_clean['longA_diff']) > 0.01) &
            ((faults_clean['longB_diff']) > 0.01) &
            ((faults_clean['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
256/112:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults_clean['latA_diff']) > 0.01) &
            ((faults_clean['latB_diff']) > 0.01) &
            ((faults_clean['latC_diff']) > 0.01) &
            ((faults_clean['longA_diff']) > 0.01) &
            ((faults_clean['longB_diff']) > 0.01) &
            ((faults_clean['longC_diff']) > 0.01)]
).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 'longB_diff', 'longC_diff'], axis=1)
256/113: faults_clean.shape
256/114:
#How many events happened close to service stations
1187335-1032324
256/115:
#How many events happened close to service stations == **155011**
1187335-1032324
256/116:
# dropping all the nan value columns actionDescription and faultValue
faults_clean.drop['actionDescription','faultValue']
256/117:
# dropping all the nan value columns actionDescription and faultValue
faults_clean.drop(['actionDescription','faultValue'])
256/118:
# dropping all the nan value columns actionDescription and faultValue
faults_clean.drop(['actionDescription','faultValue'], axis=1)
256/119:
# dropping all the nan value columns actionDescription and faultValue
faults_clean = faults_clean.drop(['actionDescription','faultValue'], axis=1)
256/120: faults_clean.shape
256/121: faults_clean.shape #it now 1032324 rows and 18 columns
256/122:
#now removing the event dates before 2011

faults_clean.loc[faults['EventTimeStamp'].dt.year > 2014]
256/123:
#now removing the event dates before 2011

faults_clean.loc[faults['EventTimeStamp'].dt.year > 2014] #31031930 rows × 18 columns
faults_clean.loc[faults['EventTimeStamp'].dt.year > 2011]
256/124:
#now removing the event dates before 2011

faults_clean.loc[faults['EventTimeStamp'].dt.year > 2014] #1031930 rows × 18 columns
faults_clean = faults_clean.loc[faults['EventTimeStamp'].dt.year > 2011] #1031930 rows × 18 columns

#no difference either with 2011 or 2014. we will keep 2014.
256/125:
#Howmay rows eliminated this way

1032324-1031930
256/126:
#Howmay rows eliminated this way: **394 observations eliminated**

1032324-1031930
256/127:
#Howmay rows eliminated this way: **394 observations eliminated**

1032324-1031930
256/128:
#removing the equipment string more than 5

faults_clean['EquipmentID'].str.len() <= 5
256/129:
#removing the equipment string more than 5

faults_clean[faults_clean['EquipmentID'].str.len() <= 5].shape
256/130:
#removing the equipment string more than 5

faults_clean[faults_clean['EquipmentID'].str.len() <= 5] #(1030202, 18)
256/131:
#removing the equipment string more than 5

faults_clean = faults_clean[faults_clean['EquipmentID'].str.len() <= 5] #(1030202, 18)
256/132:
#How many observations did we loose: 
1031930-1030202
256/133:
#removing the equipment string more than 5

faults_clean = faults_clean[faults_clean['EquipmentID'].str.len() <= 5]
faults_clean[faults_clean['EquipmentID'].str.len() < 5]#(1030202, 18)
256/134:
#removing the equipment string more than 5

faults_clean = faults_clean[faults_clean['EquipmentID'].str.len() <= 5] #(1030202, 18)
# faults_clean[faults_clean['EquipmentID'].str.len() < 5] 3#(1030202, 18)
256/135:
#How many observations did we loose: **1728**
1031930-1030202
256/136: faults_clean.loc[faults['active'] == True]
256/137: faults_celan = faults_clean.loc[faults['active'] == True] #533202 rows × 18 columns
256/138:
#How many rows lost by this filter:

1030202-533202
256/139:
#How many rows lost by this filter: **497000**

1030202-533202
256/140:
#How many rows lost by this filter: **497000**

1030202-533202

546674-533202
256/141:
#How many rows lost by this filter: **497000**

1030202-533202
256/142:
**NOTE**

**The ther members have 546674 rows but I have 533202 observations after clening up.**
256/143:
print(faults.shape) #(1187335, 20)
print(diagnostic.shape) #(12821626, 4)
print(service.shape) #service.shape #
256/144: diagnostic.head(5)
256/145: diagnostic.info()
256/146: diagnostic['FaultID'].nunique()
256/147: diagnostic['FaultId'].nunique()
256/148:
diagnostic['FaultId'].nunique() #1187335
diagnostic['Name'].nunique()
256/149:
diagnostic['FaultId'].nunique() #1187335
diagnostic['Name'].nunique() #24
diagnostic['Value'].nunique()
256/150:
# diagnostic.pivot(columns='FaultId', index='Id', values=['Name', 'Value'])

diagnostic_pivot = diagnostic.pivot(columns='Name', index='FaultId', values= 'Value')
256/151: diagnostic_pivot.shape
256/152:
diagnostic_pivot.shape #(1187335, 24)
diagnostic_pivot.head()
256/153:
diagnostic_pivot.shape #(1187335, 24)
diagnostic_pivot.head()
diagnostic_pivot.info()
256/154:
diagnostic_pivot.shape #(1187335, 24)
diagnostic_pivot.head()
diagnostic_pivot.info() all are object

diagnostic_pivot
256/155:
diagnostic_pivot.shape #(1187335, 24)
diagnostic_pivot.head()
diagnostic_pivot.info() #all are object

diagnostic_pivot
256/156:
diagnostic_pivot.shape #(1187335, 24)
diagnostic_pivot.head()
diagnostic_pivot.info() #all are object

diagnostic_pivot
diagnostic_pivot.describe()
256/157:
diagnostic_pivot.shape #(1187335, 24)
diagnostic_pivot.head()
diagnostic_pivot.info() #all are object

diagnostic_pivot
256/158: diagnostic_pivot.describe()
256/159: diagnostic_pivot.describe().to_frame()
256/160: diagnostic_pivot.describe()
256/161:
diagnostic_pivot.describe()
diagnostic_pivot.columns.to_list()
256/162:
diagnostic_pivot['AcceleratorPedal',
 'BarometricPressure',
#  'CruiseControlActive',
 'CruiseControlSetSpeed',
 'DistanceLtd',
 'EngineCoolantTemperature',
 'EngineLoad',
 'EngineOilPressure',
 'EngineOilTemperature',
 'EngineRpm',
 'EngineTimeLtd',
 'FuelLevel',
 'FuelLtd',
 'FuelRate',
 'FuelTemperature',
#  'IgnStatus',
 'IntakeManifoldTemperature',
 'LampStatus',
#  'ParkingBrake',
 'ServiceDistance',
 'Speed',
 'SwitchedBatteryVoltage',
 'Throttle',
 'TurboBoostPressure'].astype(str).astype(int)
256/163:
diagnostic_pivot['AcceleratorPedal'
#  'BarometricPressure',
# #  'CruiseControlActive',
#  'CruiseControlSetSpeed',
#  'DistanceLtd',
#  'EngineCoolantTemperature',
#  'EngineLoad',
#  'EngineOilPressure',
#  'EngineOilTemperature',
#  'EngineRpm',
#  'EngineTimeLtd',
#  'FuelLevel',
#  'FuelLtd',
#  'FuelRate',
#  'FuelTemperature',
# #  'IgnStatus',
#  'IntakeManifoldTemperature',
#  'LampStatus',
# #  'ParkingBrake',
#  'ServiceDistance',
#  'Speed',
#  'SwitchedBatteryVoltage',
#  'Throttle',
#  'TurboBoostPressure'
                ].astype(str).astype(int)
256/164: diagnostics_pivot['AcceleratorPedal'].str.contains(',')
256/165: diagnostic_pivot['AcceleratorPedal'].str.contains(',')
256/166: diagnostic_pivot['AcceleratorPedal'].str.contains(',').value_counts()
256/167: diagnostic_pivot['BarometricPressure'].str.contains(',').value_counts()
256/168:
diagnostic_pivot['BarometricPressure'].str.contains(',').value_counts()

diagnostic_pivot['AcceleratorPedal',
 'BarometricPressure',
#  'CruiseControlActive',
 'CruiseControlSetSpeed',
 'DistanceLtd',
 'EngineCoolantTemperature',
 'EngineLoad',
 'EngineOilPressure',
 'EngineOilTemperature',
 'EngineRpm',
 'EngineTimeLtd',
 'FuelLevel',
 'FuelLtd',
 'FuelRate',
 'FuelTemperature',
#  'IgnStatus',
 'IntakeManifoldTemperature',
 'LampStatus',
#  'ParkingBrake',
 'ServiceDistance',
 'Speed',
 'SwitchedBatteryVoltage',
 'Throttle',
 'TurboBoostPressure'].str.contains(',').value_counts()
256/169: diagnostic_pivot.reset_index()
256/170:

diagnostic_pivot.reset_index()
# diagnostic_pivot['BarometricPressure'].str.contains(',').value_counts()

diagnostic_pivot['AcceleratorPedal',
 'BarometricPressure',
#  'CruiseControlActive',
 'CruiseControlSetSpeed',
 'DistanceLtd',
 'EngineCoolantTemperature',
 'EngineLoad',
 'EngineOilPressure',
 'EngineOilTemperature',
 'EngineRpm',
 'EngineTimeLtd',
 'FuelLevel',
 'FuelLtd',
 'FuelRate',
 'FuelTemperature',
#  'IgnStatus',
 'IntakeManifoldTemperature',
 'LampStatus',
#  'ParkingBrake',
 'ServiceDistance',
 'Speed',
 'SwitchedBatteryVoltage',
 'Throttle',
 'TurboBoostPressure'].str.contains(',').value_counts()
256/171: diagnostic['Value'].str.contains(',').value_counts()
256/172: diagnostic.groupby('Name')['Value'].str.contains(',').value_counts()
256/173: diagnostic.groupby('Name')['Value'].value_counts()
256/174: diagnostic.groupby('Name')['Value'].value_counts().to_frame()
256/175: diagnostic.groupby('Name')['Value'].value_counts().to_frame().head(50)
256/176: diagnostic.groupby('Name')['Value'].value_counts().to_frame().iloc[;50:100]
256/177: diagnostic.groupby('Name')['Value'].value_counts().to_frame().iloc[,50:100]
256/178: diagnostic.groupby('Name')['Value'].value_counts().to_frame().iloc[50:100]
256/179: diagnostic.groupby('Name')['Value'].value_counts().to_frame().iloc[100:150]
256/180: diagnostic.groupby('Name')['Value'].value_counts().to_frame()
256/181:
# counting ',' in the value columns
diagnostic['Value'].str.contains(',').value_counts()
256/182:
# replacing the ',' 
 diagnostic['Value'].astype(str).str.replace(',', '')
256/183:
# replacing the ',' 
diagnostic['Value'].astype(str).str.replace(',', '')
256/184:
# replacing the ',' 
diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()
256/185:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic['Value'].astype(str).str.replace(',', '')
256/186:
# diagnostic.pivot(columns='FaultId', index='Id', values=['Name', 'Value'])

diagnostic_pivot = diagnostic_core.pivot(columns='Name', index='FaultId', values= 'Value')
256/187:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic['Value'].astype(str).str.replace(',', '')

diagnostic_core
256/188: diagnostic_core = diagnostic
256/189:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic_core['Value'].astype(str).str.replace(',', '').to_frame

diagnostic_core
256/190:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic_core['Value'].astype(str).str.replace(',', '')

diagnostic_core
256/191:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic_core['Value'].astype(str).str.replace(',', '')

diagnostic_core
256/192: diagnostic_core = diagnostic
256/193:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic_core['Value'].astype(str).str.replace(',', '')

diagnostic_core
256/194:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic_core['Value'].astype(str).str.replace(',', '')

diagnostic_core.type()
256/195:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic_core['Value'].astype(str).str.replace(',', '')

diagnostic_core.info()
256/196:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic_core['Value'].astype(str).str.replace(',', '')
256/197: diagnostic_core = diagnostic
256/198:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic_core['Value'].astype(str).str.replace(',', '')
256/199: diagnostic_core
256/200: diagnostic_core.type()
256/201: diagnostic_core.info
256/202:
# diagnostic.pivot(columns='FaultId', index='Id', values=['Name', 'Value'])

diagnostic_pivot = diagnostic_core.pivot(columns='Name', index='FaultId', values= 'Value')
256/203:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = [diagnostic_core['Value'].astype(str).str.replace(',', '')]
256/204:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core = diagnostic_core[diagnostic_core['Value'].astype(str).str.replace(',', '')]
256/205: diagnostic_core = diagnostic
256/206: diagnostic_core
256/207: diagnostic_core.type()
256/208: diagnostic_core.astype()
256/209: diagnostic_core.dtype()
256/210: diagnostic_core.info()
256/211:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core['Value'].astype(str).str.replace(',', '')
256/212:
# diagnostic.pivot(columns='FaultId', index='Id', values=['Name', 'Value'])

diagnostic_pivot = diagnostic_core.pivot(columns='Name', index='FaultId', values= 'Value')
256/213:
# Remove commas from all 25 columns
for col in diagnostics_pivot.columns[:25]:
   diagnostics_pivot[col] = diagnostics_pivot[col].astype(str).str.replace(',', '')

# Convert all columns to numeric
# Diagnostics = Diagnostics.apply(pd.to_numeric, errors='coerce')

diagnostics_pivot
256/214:
# Remove commas from all 25 columns
for col in diagnostic_pivot.columns[:25]:
   diagnostic_pivot[col] = diagnostic_pivot[col].astype(str).str.replace(',', '')

# Convert all columns to numeric
# Diagnostics = Diagnostics.apply(pd.to_numeric, errors='coerce')

diagnostic_pivot
256/215:

diagnostic_pivot.reset_index()
# diagnostic_pivot['BarometricPressure'].str.contains(',').value_counts()
256/216:
# Remove commas from all 25 columns
for col in diagnostic_pivot.columns[:25]:
   diagnostic_pivot[col] = diagnostic_pivot[col].astype(str).str.replace(',', '')

# Convert all columns to numeric
# Diagnostics = Diagnostics.apply(pd.to_numeric, errors='coerce')

diagnostic_pivot.rest_index()
256/217:
# Remove commas from all 25 columns
for col in diagnostic_pivot.columns[:25]:
   diagnostic_pivot[col] = diagnostic_pivot[col].astype(str).str.replace(',', '')

# Convert all columns to numeric
# Diagnostics = Diagnostics.apply(pd.to_numeric, errors='coerce')

diagnostic_pivot.reset_index()
256/218:
# Convert all columns to numeric
# diagnostic_pivot = diagnostic_pivot.apply(pd.to_numeric, errors='coerce')
256/219:
# Convert all columns to numeric
diagnostic_pivot = diagnostic_pivot.apply(pd.to_numeric, errors='coerce')
256/220:
faults_clean['spn_fmi'] = ['_'.join(i) for i in zip(faults_clean['spn'].astype(str), faults_clean['fmi'].astype(str))]
faults_clean
# faults_encoded = pd.get_dummies(faults, columns=['spn_fmi'], prefix='spn_fmi')

# faults_encoded = faults_encoded.sort_values(by='EventTimeStamp')
256/221: faults_clean = faults_clean.loc[faults['active'] == True] #533202 rows × 18 columns
256/222:
faults_clean['spn_fmi'] = ['_'.join(i) for i in zip(faults_clean['spn'].astype(str), faults_clean['fmi'].astype(str))]
faults_clean
# faults_encoded = pd.get_dummies(faults, columns=['spn_fmi'], prefix='spn_fmi')

# faults_encoded = faults_encoded.sort_values(by='EventTimeStamp')
256/223:
getting the onehot encoder using get dummies for the spn_fmi combo

faults_encoded = pd.get_dummies(faults_clean, columns=['spn_fmi'], prefix='spn_fmi')

# faults_encoded = faults_encoded.sort_values(by='EventTimeStamp')
256/224:
# getting the onehot encoder using pd.get_dummies for the spn_fmi combo

faults_encoded = pd.get_dummies(faults_clean, columns=['spn_fmi'], prefix='spn_fmi')

# faults_encoded = faults_encoded.sort_values(by='EventTimeStamp')
256/225: faults_encoded
256/226: faults_encoded.columns
256/227: faults_encoded.columns.to_list()
256/228: faults_encoded.columns.shape
256/229: faults_encoded.shape
256/230:
faults_encoded.shape #(533202, 993)

faults_encoded
256/231:
faults_encoded.shape #(533202, 993)

faults_encoded.sort_values(by='EventTimeStamp', ascending =False)
256/232:
faults_encoded.shape #(533202, 993)

faults_encoded.sort_values(by='EventTimeStamp')
256/233:
faults_encoded.shape #(533202, 993)

faults_encoded = faults_encoded.sort_values(by='EventTimeStamp')
256/234: faults_encoded([faults_encoded['spn']==5246)]['fmi']
256/235: faults_encoded[faults_encoded['spn']==5246]['fmi']
256/236: faults_encoded[faults_encoded['spn']==5246]['fmi'].value_counts()
256/237: faults_encoded[faults_encoded['fmi']==31]['spn'].value_counts()
256/238: faults_encoded[faults_encoded['fmi']==31]['spn'].value_counts().plot(kind="bar")
256/239: faults_encoded[faults_encoded['spn']==5246]['fmi'].value_counts().plot(kind='bar')
256/240: faults_encoded[faults_encoded['fmi']==0]['spn'].value_counts().plot(kind="bar")
256/241: faults_encoded[faults_encoded['fmi']==0]['spn'].value_counts()
256/242: faults_encoded[faults_encoded['fmi']==31]['spn'].value_counts()
256/243: faults_encoded[faults_encoded['spn']==5246]['fmi'].value_counts()
256/244: faults_encoded[faults_encoded['spn']==5246]['fmi'].value_counts().plot(kind='bar')
256/245:
# to obtain the one hot encoded columns since there are so many
spnfmi_cols = [col for col in faults_encoded.columns if 'spn_fmi' in col]
fixed_cols = ['RecordID', 'spn', 'fmi']
256/246: faults_encoded
256/247:
faults_encoded
diagnostic_pivot
256/248: # pd.merge(x=faults_encoded y=diagnostic_pivot, left_on="RecordID", right_on= "FaultId")
256/249: pd.merge(x=faults_encoded y=diagnostic_pivot, left_on="RecordID", right_on= "FaultId")
256/250: pd.merge(x=faults_encoded, y=diagnostic_pivot, left_on="RecordID", right_on= "FaultId")
256/251: pd.merge(left=faults_encoded, right=diagnostic_pivot, left_on="RecordID", right_on= "FaultId")
256/252: fault_diag = pd.merge(left=faults_encoded, right=diagnostic_pivot, left_on="RecordID", right_on= "FaultId")
256/253: fault_diag.shape
256/254:
fault_diag.shape #(533202, 1017)
fault_diag
256/255:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

(
    fault_diag
    .set_index('EventTimeStamp')
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/256:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

(
    fault_diag
    .groupby('EquipmentID')[['EventTimeStamp'] + spnfmi_cols]
    .reset_index('EventTimeStamp')
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/257:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

(
    fault_diag
    .groupby('EquipmentID')[['EventTimeStamp'] + spnfmi_cols]
    .set_index('EventTimeStamp')
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/258:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

(
    fault_diag
    .set_index('EventTimeStamp')
    .groupby('EquipmentID')[['EventTimeStamp'] + spnfmi_cols]
    
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/259:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

(
    fault_diag
    .set_index('EventTimeStamp')
#     .groupby('EquipmentID')[['EventTimeStamp'] + spnfmi_cols]
    
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/260:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

(
    fault_diag
    .set_index('EventTimeStamp')
    .groupby('EquipmentID')
    
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/261:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

(
    fault_diag
    .set_index('EventTimeStamp')
    .groupby('EquipmentID')[['EventTimeStamp'] + spnfmi_cols]
    
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/262:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

(
    fault_diag
    .set_index('EventTimeStamp')
    .groupby('EquipmentID')[spnfmi_cols]
    
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/263:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

(
    fault_diag
    .set_index('EventTimeStamp')
    .groupby('EquipmentID')
    
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/264:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

fault_diag_nan =(
    fault_diag
    .set_index('EventTimeStamp')
    .groupby('EquipmentID')
    
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/265: fault_diag_nan
256/266:
# pd.merge(left=faults_encoded, right=diagnostic_pivot, left_on="RecordID", right_on= "FaultId")

fault_encoded.shape
256/267:
# pd.merge(left=faults_encoded, right=diagnostic_pivot, left_on="RecordID", right_on= "FaultId")

faults_encoded.shape
256/268:
# pd.merge(left=faults_encoded, right=diagnostic_pivot, left_on="RecordID", right_on= "FaultId")

faults_encoded.shape #(533202, 993)
pd.merge(left=faults_encoded, right=fault_diag_nan, on=['EquipmentID', 'EventTimeStamp'])
256/269:
# pd.merge(left=faults_encoded, right=diagnostic_pivot, left_on="RecordID", right_on= "FaultId")

faults_encoded.shape #(533202, 993)
faults_diagnostic = pd.merge(right=faults_encoded, left=fault_diag_nan, on=['EquipmentID', 'EventTimeStamp'])
256/270: faults_diagnostic.shape
256/271:
faults_diagnostic.shape (648824, 1014)

faults_diagnostic
256/272:
faults_diagnostic.shape #(648824, 1014)

faults_diagnostic
256/273: pd.merge(left=faults_encoded, right=fault_diag_nan, on=['EquipmentID', 'EventTimeStamp'])
256/274: faults_diagnostic['spn'].value_counts()
256/275: faults_diagnostic['spn_fmi_96_3'].value_counts()
256/276: faults_diagnostic['spn_fmi_1569_31'].value_counts()
256/277: faults_diagnostic['spn_fmi_5264_16'].value_counts()
256/278: faults_diagnostic['spn_fmi_5246_16'].value_counts()
256/279: faults_diagnostic.isna().sum()
256/280: faults_diagnostic.isna().sum().to_frame()
256/281: faults_diagnostic.isna().sum().to_frame().head(50)
256/282: diagnostic_pivot.isna().sum()
256/283: fault_diag.isna().sum()
256/284: fault_diag.isna().sum().to_frame().head(50)
256/285: fault_diag.isna().sum().to_frame().tail(50)
256/286: fault_diag.isna().sum().to_frame().tail(24)
256/287: faults_diagnostic.isna().sum().to_frame().head(25)
256/288:
faults['EventTimeStamp'].dt.year.unique()
faults['EventTimeStamp'].dt.year.value_counts()
# faults['LocationTimeStamp'].dt.year.value_counts()
256/289:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults_clean['latA_diff'] > 0.01) |(faults_clean['longA_diff'] > 0.01))&
                                 ((faults_clean['latB_diff'] > 0.01) |(faults_clean['longB_diff'] > 0.01))&
                                 ((faults_clean['latC_diff'] > 0.01) |(faults_clean['longC_diff'] > 0.01))]
            ).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 
                    'longB_diff', 'longC_diff'], axis=1)
256/290:
#finding the distance from the nearest service stations

faults_clean['latA_diff'] = abs(faults_clean['Latitude']-36.0666667)
faults_clean['latB_diff'] = abs(faults_clean['Latitude']-35.5883333)
faults_clean['latC_diff'] = abs(faults_clean['Latitude']-36.1950)
faults_clean['longA_diff'] = abs(faults_clean['Longitude']-(-86.4347222))
faults_clean['longB_diff'] = abs(faults_clean['Longitude']-(-86.4438888))
faults_clean['longC_diff'] = abs(faults_clean['Longitude']-(-83.174722))

faults_clean #1187335 rows × 26 columns
256/291:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults_clean['latA_diff'] > 0.01) |(faults_clean['longA_diff'] > 0.01))&
                                 ((faults_clean['latB_diff'] > 0.01) |(faults_clean['longB_diff'] > 0.01))&
                                 ((faults_clean['latC_diff'] > 0.01) |(faults_clean['longC_diff'] > 0.01))]
            ).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 
                    'longB_diff', 'longC_diff'], axis=1)
256/292: faults_clean.shape #Row left =(1032324, 20)
256/293:
#Libraries need for work

import pandas as pd
from datetime import datetime



from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

#we need to create a conda environment for pycaret and then pip install the packages after activating and rerun the notebook.
# from pycaret.classification import *
# from pycaret.regression import *
# from pycaret.time_series import *

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
256/294:
#Reading the data files

faults = pd.read_csv("../data/J1939Faults.csv",low_memory=False,
                     parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
    #initial reading we saw it has date time object so we will keep them as date time. 
    #Also we saw the waning for memmory because of which we were loosing observations, so we will have low_memmory =False

diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
256/295:
print(faults.shape) #(1187335, 20)
print(diagnostic.shape) #(12821626, 4)
print(service.shape) #service.shape #
256/296: faults.head(2)
256/297: faults.info()
256/298:
faults.describe()
faults.isna().sum()
256/299:
faults.columns
faults['EventTimeStamp'].value_counts()
(faults['EventTimeStamp']
 .dt.year
 .value_counts()
 .sort_values(ascending = False)
 .plot(kind="bar", 
       title = 'Observations_per_year',
       xlabel='', ylabel='Number of Observations'))
#date time has year dt.year extracts year from it
256/300:
(faults['LocationTimeStamp']
 .dt.year
 .value_counts()
 .sort_values(ascending = False)
 .plot(kind="bar", 
       title = 'Observations_per_year',
       xlabel='',
       ylabel='Number of Observations'))
#date time has year dt.year extracts year from it
256/301:
faults['EventTimeStamp'].dt.year.unique()
faults['EventTimeStamp'].dt.year.value_counts()
# faults['LocationTimeStamp'].dt.year.value_counts()
256/302:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= (''),
       ylabel='Number of Observations'))
256/303:
#the EuipmentId is a string character and has length varrying from character lnrth 3 4, 5, and 9 and 10
faults['EquipmentID'].str.len().nunique() #5
faults['EquipmentID'].str.len().value_counts().sort_values(ascending=False).to_frame()
256/304: faults['active'].value_counts()
256/305:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults[(faults['spn'] == 5246)]['EquipmentID'].nunique() #215 trucks out of 1122 have experienced full derate
faults[(faults['spn'] == 1569)]['EquipmentID'].nunique() #506 trucks out of 1122 have experienced 75% derate

faults[(faults['spn'] == 1569)]['fmi'].nunique() 
faults[(faults['spn'] == 1569)]['fmi'].unique() #only one fmi code which is 31 with spn 1569

faults[(faults['spn'] == 5246)]['fmi'].nunique() 
faults[(faults['spn'] == 5246)]['fmi'].unique() #only five fmi code which is [ 0, 15, 16, 19, 14] with spn 5246
256/306: faults.shape #(1187335, 20)
256/307:
faullts_clean = faults
faults_clean.shape
256/308:
#finding the distance from the nearest service stations

faults_clean['latA_diff'] = abs(faults_clean['Latitude']-36.0666667)
faults_clean['latB_diff'] = abs(faults_clean['Latitude']-35.5883333)
faults_clean['latC_diff'] = abs(faults_clean['Latitude']-36.1950)
faults_clean['longA_diff'] = abs(faults_clean['Longitude']-(-86.4347222))
faults_clean['longB_diff'] = abs(faults_clean['Longitude']-(-86.4438888))
faults_clean['longC_diff'] = abs(faults_clean['Longitude']-(-83.174722))

faults_clean #1187335 rows × 26 columns
256/309:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults_clean['latA_diff'] > 0.01) |(faults_clean['longA_diff'] > 0.01))&
                                 ((faults_clean['latB_diff'] > 0.01) |(faults_clean['longB_diff'] > 0.01))&
                                 ((faults_clean['latC_diff'] > 0.01) |(faults_clean['longC_diff'] > 0.01))]
            ).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 
                    'longB_diff', 'longC_diff'], axis=1)
256/310: faults_clean.shape #Row left =(1032324, 20)
256/311:
#How many events happened close to service stations == **155011** events are eliminated
1187335-1032324
1187335-533202
256/312:
#Libraries need for work

import pandas as pd
from datetime import datetime



from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

#we need to create a conda environment for pycaret and then pip install the packages after activating and rerun the notebook.
# from pycaret.classification import *
# from pycaret.regression import *
# from pycaret.time_series import *

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
256/313:
#Reading the data files

faults = pd.read_csv("../data/J1939Faults.csv",low_memory=False,
                     parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
    #initial reading we saw it has date time object so we will keep them as date time. 
    #Also we saw the waning for memmory because of which we were loosing observations, so we will have low_memmory =False

diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
256/314:
print(faults.shape) #(1187335, 20)
print(diagnostic.shape) #(12821626, 4)
print(service.shape) #service.shape #
256/315: faults.head(2)
256/316: faults.info()
256/317:
faults.describe()
faults.isna().sum()
256/318:
faults.columns
faults['EventTimeStamp'].value_counts()
(faults['EventTimeStamp']
 .dt.year
 .value_counts()
 .sort_values(ascending = False)
 .plot(kind="bar", 
       title = 'Observations_per_year',
       xlabel='', ylabel='Number of Observations'))
#date time has year dt.year extracts year from it
256/319:
(faults['LocationTimeStamp']
 .dt.year
 .value_counts()
 .sort_values(ascending = False)
 .plot(kind="bar", 
       title = 'Observations_per_year',
       xlabel='',
       ylabel='Number of Observations'))
#date time has year dt.year extracts year from it
256/320:
faults['EventTimeStamp'].dt.year.unique()
faults['EventTimeStamp'].dt.year.value_counts()
# faults['LocationTimeStamp'].dt.year.value_counts()
256/321:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= (''),
       ylabel='Number of Observations'))
256/322:
#the EuipmentId is a string character and has length varrying from character lnrth 3 4, 5, and 9 and 10
faults['EquipmentID'].str.len().nunique() #5
faults['EquipmentID'].str.len().value_counts().sort_values(ascending=False).to_frame()
256/323: faults['active'].value_counts()
256/324:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults[(faults['spn'] == 5246)]['EquipmentID'].nunique() #215 trucks out of 1122 have experienced full derate
faults[(faults['spn'] == 1569)]['EquipmentID'].nunique() #506 trucks out of 1122 have experienced 75% derate

faults[(faults['spn'] == 1569)]['fmi'].nunique() 
faults[(faults['spn'] == 1569)]['fmi'].unique() #only one fmi code which is 31 with spn 1569

faults[(faults['spn'] == 5246)]['fmi'].nunique() 
faults[(faults['spn'] == 5246)]['fmi'].unique() #only five fmi code which is [ 0, 15, 16, 19, 14] with spn 5246
256/325: faults.shape #(1187335, 20)
256/326:
faullts_clean = faults
faults_clean.shape
256/327:
#finding the distance from the nearest service stations

faults_clean['latA_diff'] = abs(faults_clean['Latitude']-36.0666667)
faults_clean['latB_diff'] = abs(faults_clean['Latitude']-35.5883333)
faults_clean['latC_diff'] = abs(faults_clean['Latitude']-36.1950)
faults_clean['longA_diff'] = abs(faults_clean['Longitude']-(-86.4347222))
faults_clean['longB_diff'] = abs(faults_clean['Longitude']-(-86.4438888))
faults_clean['longC_diff'] = abs(faults_clean['Longitude']-(-83.174722))

faults_clean #1187335 rows × 26 columns
256/328:
faullts_clean = faults
# faults_clean.shape
256/329:
faullts_clean = faults
faults_clean.shape
256/330:
faults_clean = faults.copy()
faults_clean.shape
256/331:
#finding the distance from the nearest service stations

faults_clean['latA_diff'] = abs(faults_clean['Latitude']-36.0666667)
faults_clean['latB_diff'] = abs(faults_clean['Latitude']-35.5883333)
faults_clean['latC_diff'] = abs(faults_clean['Latitude']-36.1950)
faults_clean['longA_diff'] = abs(faults_clean['Longitude']-(-86.4347222))
faults_clean['longB_diff'] = abs(faults_clean['Longitude']-(-86.4438888))
faults_clean['longC_diff'] = abs(faults_clean['Longitude']-(-83.174722))

faults_clean #1187335 rows × 26 columns
256/332:
#droping the record that happens to be in proximity to any three service stations (0.01 miles)
faults_clean = (faults_clean.loc[((faults_clean['latA_diff'] > 0.01) |(faults_clean['longA_diff'] > 0.01))&
                                 ((faults_clean['latB_diff'] > 0.01) |(faults_clean['longB_diff'] > 0.01))&
                                 ((faults_clean['latC_diff'] > 0.01) |(faults_clean['longC_diff'] > 0.01))]
            ).drop(['latA_diff', 'latB_diff', 'latC_diff', 'longA_diff', 
                    'longB_diff', 'longC_diff'], axis=1)
256/333: faults_clean.shape #Row left =(1032324, 20)
256/334:
#How many events happened close to service stations == **155011** events are eliminated
1187335-1032324
1187335-1055557
256/335:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    'RecordID': Lambda x: x[-1],
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

fault_diag_nan =(
    fault_diag
    .set_index('EventTimeStamp')
    .groupby('EquipmentID')
    
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/336:
agg_dict = {
#     'participantId': lambda x: x[-1],
#     'type_ITEM_PURCHASED': 'sum',
#     'type_LEVEL_UP': 'max'
    'RecordID': lambda x: x[-1],
    'AcceleratorPedal': 'mean',
 'BarometricPressure': 'mean',
#  'CruiseControlActive',
 'CruiseControlSetSpeed': 'mean',
 'DistanceLtd': 'mean',
 'EngineCoolantTemperature': 'mean',
 'EngineLoad': 'mean',
 'EngineOilPressure': 'mean',
 'EngineOilTemperature': 'mean',
 'EngineRpm': 'mean',
 'EngineTimeLtd': 'mean',
 'FuelLevel': 'mean',
 'FuelLtd': 'mean',
 'FuelRate': 'mean',
 'FuelTemperature': 'mean',
#  'IgnStatus',
 'IntakeManifoldTemperature': 'mean',
 'LampStatus': 'mean',
#  'ParkingBrake',
 'ServiceDistance': 'mean',
 'Speed': 'mean',
 'SwitchedBatteryVoltage': 'mean',
 'Throttle': 'mean',
 'TurboBoostPressure': 'mean'
}

fault_diag_nan =(
    fault_diag
    .set_index('EventTimeStamp')
    .groupby('EquipmentID')
    
    .rolling('360 min')
    .agg(agg_dict)
    .reset_index()
)
256/337: fault_diag_nan
256/338:
# pd.merge(left=faults_encoded, right=diagnostic_pivot, left_on="RecordID", right_on= "FaultId")

faults_encoded.shape #(533202, 993)
faults_diagnostic = pd.merge(right=faults_encoded, left=fault_diag_nan, on=['RecordID','EquipmentID', 'EventTimeStamp'])
256/339:
faults_diagnostic.shape #(648824, 1014)

faults_diagnostic
256/340: faults_encoded[faults_encoded['spn']==5246]['fmi'].value_counts()
256/341:
faults_encoded[faults_encoded['fmi']==0]['spn'].value_counts()
faults_encoded[faults_encoded['fmi']==16]['spn'].value_counts()
256/342:
# dropping all the nan value columns actionDescription and faultValue
faults_clean = faults_clean.drop(['actionDescription','faultValue'], axis=1)
256/343: faults_clean.shape #it now 1032324 rows and 18 columns
256/344:
#now removing the event dates before 2011

faults_clean.loc[faults['EventTimeStamp'].dt.year > 2014] #1031930 rows × 18 columns
# faults_clean = faults_clean.loc[faults['EventTimeStamp'].dt.year > 2011] #1031930 rows × 18 columns

#no difference either with 2011 or 2014. we will keep 2014.
256/345:
#now removing the event dates before 2011

faults_clean.loc[faults['EventTimeStamp'].dt.year > 2014] #1031930 rows × 18 columns 1055163 rows × 18 columns
faults_clean.loc[faults['EventTimeStamp'].dt.year > 2011] #1031930 rows × 18 columns

#no difference either with 2011 or 2014. we will keep 2014.
256/346:
#now removing the event dates before 2011

faults_clean.loc[faults['EventTimeStamp'].dt.year > 2014] # 1055163 rows × 18 columns
faults_clean = faults_clean.loc[faults['EventTimeStamp'].dt.year > 2011] #1055163 rows × 18 columns

#no difference either with 2011 or 2014. we will keep 2014.
256/347:
#Howmay rows eliminated due to caused by an integer error: **394 observations eliminated**

1032324-1031930
1055557-1055163
256/348:
#removing the equipment string more than 5

faults_clean[faults_clean['EquipmentID'].str.len() <= 5] #(1030202, 18)
# faults_clean[faults_clean['EquipmentID'].str.len() < 5]
256/349:
#removing the equipment string more than 5

faults_clean = faults_clean[faults_clean['EquipmentID'].str.len() <= 5] #(1030202, 18)
# faults_clean[faults_clean['EquipmentID'].str.len() < 5]
256/350:
#removing the equipment string more than 5

faults_clean = faults_clean[faults_clean['EquipmentID'].str.len() <= 5] #(1030202, 18)
# faults_clean[faults_clean['EquipmentID'].str.len() < 5] 
faults_clean
256/351:
#How many observations did we loose: **1728**
1031930-1030202
1055163-1053364
256/352:
#How many observations did we loose: **1799**
1031930-1030202
1055163-1053364
256/353: faults_clean.loc[faults['active'] == True] #533202 rows × 18 columns
256/354: faults_clean = faults_clean.loc[faults['active'] == True] #533202 rows × 18 columns
256/355:
faults_clean = faults_clean.loc[faults['active'] == True] #533202 rows × 18 columns
faults_clean
256/356:
#How many rows lost by this filter: **497000**

1053364-546674
256/357: diagnostic.head(5)
256/358: diagnostic.info()
256/359:
diagnostic['FaultId'].nunique() #1187335
diagnostic['Name'].nunique() #24
diagnostic['Value'].nunique() #1009465
256/360: diagnostic.groupby('Name')['Value'].value_counts().to_frame()
256/361:
# counting ',' in the value columns
diagnostic['Value'].str.contains(',').value_counts()

#there are 304 values across the df[Name] where we have ','.
256/362: diagnostic_core = diagnostic
256/363: diagnostic_core.info()
256/364:
# replacing the ',' 
# diagnostic['Value'].astype(str).str.replace(',', '').str.contains(',').value_counts()

diagnostic_core['Value'].astype(str).str.replace(',', '')

#want to replace the , in te value before pivoting so that it can convert to int.
#but it is coming as a series how to fix series into df. 
#Used to_frame() but working?????
256/365: diagnostic_core.info
256/366:
# diagnostic.pivot(columns='FaultId', index='Id', values=['Name', 'Value'])

diagnostic_pivot = diagnostic_core.pivot(columns='Name', index='FaultId', values= 'Value')
256/367:
diagnostic_pivot.shape #(1187335, 24)
diagnostic_pivot.head()
diagnostic_pivot.info() #all are object

diagnostic_pivot
256/368:
diagnostic_pivot.describe()
diagnostic_pivot.columns.to_list()
256/369: diagnostic_pivot.reset_index()
256/370:

diagnostic_pivot.reset_index()
# diagnostic_pivot['BarometricPressure'].str.contains(',').value_counts()
256/371:
# Remove commas from all 25 columns
for col in diagnostic_pivot.columns[:25]:
   diagnostic_pivot[col] = diagnostic_pivot[col].astype(str).str.replace(',', '')

# Convert all columns to numeric
# Diagnostics = Diagnostics.apply(pd.to_numeric, errors='coerce')

diagnostic_pivot.reset_index()
256/372:
# Convert all columns to numeric
diagnostic_pivot = diagnostic_pivot.apply(pd.to_numeric, errors='coerce')
256/373:
faults_clean['spn_fmi'] = ['_'.join(i) for i in zip(faults_clean['spn'].astype(str), faults_clean['fmi'].astype(str))]
faults_clean
256/374:
# Plotting the time series of given dataframe
plt.plot(faults_clean.EventTimeStamp, faults_clean.loc[faults_clean['spn']==5246].value_counts())
 
# Giving title to the chart using plt.title
# plt.title('Classes by Date')
 
# rotating the x-axis tick labels at 30degree
# towards right
# plt.xticks(rotation=30, ha='right')
 
# Providing x and y label to the chart
# plt.xlabel('Date')
# plt.ylabel('Classes')
256/375:
# Plotting the time series of given dataframe
plt.plot(faults_clean.loc[faults_clean['spn']==5246][EventTimeStamp], 
         faults_clean.loc[faults_clean['spn']==5246].value_counts())
 
# Giving title to the chart using plt.title
# plt.title('Classes by Date')
 
# rotating the x-axis tick labels at 30degree
# towards right
# plt.xticks(rotation=30, ha='right')
 
# Providing x and y label to the chart
# plt.xlabel('Date')
# plt.ylabel('Classes')
256/376:
# Plotting the time series of given dataframe
plt.plot(faults_clean.loc[faults_clean['spn']==5246]['EventTimeStamp'], 
         faults_clean.loc[faults_clean['spn']==5246].value_counts())
 
# Giving title to the chart using plt.title
# plt.title('Classes by Date')
 
# rotating the x-axis tick labels at 30degree
# towards right
# plt.xticks(rotation=30, ha='right')
 
# Providing x and y label to the chart
# plt.xlabel('Date')
# plt.ylabel('Classes')
256/377:
cols = ['activeTransitionCount','MCTNumber',
        'AcceleratorPedal', 'BarometricPressure', 'CruiseControlSetSpeed',
        'DistanceLtd', 'EngineCoolantTemperature', 'EngineLoad', 'EngineOilPressure', 
        'EngineOilTemperature', 'EngineRpm', 'EngineTimeLtd', 'FuelLevel', 'FuelLtd', 
        'FuelRate', 'FuelTemperature', 'IntakeManifoldTemperature', 'LampStatus', 
        'ServiceDistance', 'Speed', 'SwitchedBatteryVoltage', 'Throttle', 'TurboBoostPressure']

imputer = SimpleImputer(strategy='mean')


for column in cols:
    
    equipment_imputed = fault_diag.groupby('EquipmentID')[column].apply(lambda x: imputer.fit_transform(x.values.reshape(-1, 1)))

    for Id in equipment_imputed.index :
        
        fault_diag.loc[fault_diag['EquipmentID']== Id, column]= equipment_imputed.loc[Id].flatten()
256/378:
cols = ['activeTransitionCount','MCTNumber',
        'AcceleratorPedal', 'BarometricPressure', 'CruiseControlSetSpeed',
        'DistanceLtd', 'EngineCoolantTemperature', 'EngineLoad', 'EngineOilPressure', 
        'EngineOilTemperature', 'EngineRpm', 'EngineTimeLtd', 'FuelLevel', 'FuelLtd', 
        'FuelRate', 'FuelTemperature', 'IntakeManifoldTemperature', 'LampStatus', 
        'ServiceDistance', 'Speed', 'SwitchedBatteryVoltage', 'Throttle', 'TurboBoostPressure']

imputer = SimpleImputer(strategy='mean')


for column in cols:
    
    equipment_imputed = fault_diag.groupby('EquipmentID')[column].apply(lambda x: imputer.fit_transform(x.values.reshape(-1, 1)))
256/379:
cols = ['activeTransitionCount','MCTNumber',
        'AcceleratorPedal', 'BarometricPressure', 'CruiseControlSetSpeed',
        'DistanceLtd', 'EngineCoolantTemperature', 'EngineLoad', 'EngineOilPressure', 
        'EngineOilTemperature', 'EngineRpm', 'EngineTimeLtd', 'FuelLevel', 'FuelLtd', 
        'FuelRate', 'FuelTemperature', 'IntakeManifoldTemperature', 'LampStatus', 
        'ServiceDistance', 'Speed', 'SwitchedBatteryVoltage', 'Throttle', 'TurboBoostPressure']

imputer = SimpleImputer(strategy='mean')


for column in cols:
    
    equipment_imputed = fault_diag.groupby('EquipmentID')[column].apply(lambda x: imputer.fit_transform(x.values.reshape(-1, 1)))
    euipment_imputed.apply(lambda x: x.flatten()).explode()
    
    for Id in equipment_imputed.index :
        
        fault_diag.loc[fault_diag['EquipmentID']== Id, column]= equipment_imputed.loc[Id].flatten()
256/380:
cols = ['activeTransitionCount','MCTNumber',
        'AcceleratorPedal', 'BarometricPressure', 'CruiseControlSetSpeed',
        'DistanceLtd', 'EngineCoolantTemperature', 'EngineLoad', 'EngineOilPressure', 
        'EngineOilTemperature', 'EngineRpm', 'EngineTimeLtd', 'FuelLevel', 'FuelLtd', 
        'FuelRate', 'FuelTemperature', 'IntakeManifoldTemperature', 'LampStatus', 
        'ServiceDistance', 'Speed', 'SwitchedBatteryVoltage', 'Throttle', 'TurboBoostPressure']

imputer = SimpleImputer(strategy='mean')


for column in cols:
    
    equipment_imputed = fault_diag.groupby('EquipmentID')[column].apply(lambda x: imputer.fit_transform(x.values.reshape(-1, 1)))
    equipment_imputed = euipment_imputed.apply(lambda x: x.flatten()).explode()
    
    for Id in equipment_imputed.index :
        
        fault_diag.loc[fault_diag['EquipmentID']== Id, column]= equipment_imputed.loc[Id].flatten()
256/381:
cols = ['activeTransitionCount','MCTNumber',
        'AcceleratorPedal', 'BarometricPressure', 'CruiseControlSetSpeed',
        'DistanceLtd', 'EngineCoolantTemperature', 'EngineLoad', 'EngineOilPressure', 
        'EngineOilTemperature', 'EngineRpm', 'EngineTimeLtd', 'FuelLevel', 'FuelLtd', 
        'FuelRate', 'FuelTemperature', 'IntakeManifoldTemperature', 'LampStatus', 
        'ServiceDistance', 'Speed', 'SwitchedBatteryVoltage', 'Throttle', 'TurboBoostPressure']

imputer = SimpleImputer(strategy='mean')


for column in cols:
    
    equipment_imputed = fault_diag.groupby('EquipmentID')[column].apply(lambda x: imputer.fit_transform(x.values.reshape(-1, 1)))
#     equipment_imputed = euipment_imputed.apply(lambda x: x.flatten()).explode()
    
    for Id in equipment_imputed.index :
        
        fault_diag.loc[fault_diag['EquipmentID']== Id, column]= equipment_imputed.loc[Id].flatten()
256/382:
cols = ['activeTransitionCount','MCTNumber',
        'AcceleratorPedal', 'BarometricPressure', 'CruiseControlSetSpeed',
        'DistanceLtd', 'EngineCoolantTemperature', 'EngineLoad', 'EngineOilPressure', 
        'EngineOilTemperature', 'EngineRpm', 'EngineTimeLtd', 'FuelLevel', 'FuelLtd', 
        'FuelRate', 'FuelTemperature', 'IntakeManifoldTemperature', 'LampStatus', 
        'ServiceDistance', 'Speed', 'SwitchedBatteryVoltage', 'Throttle', 'TurboBoostPressure']

imputer = SimpleImputer(strategy='mean')


for column in cols:
    
    equipment_imputed = fault_diag.groupby('EquipmentID')[column].apply(lambda x: imputer.fit_transform(x.values.reshape(-1, 1)))
    equipment_imputed = equipment_imputed.apply(lambda x: x.flatten()).explode()
    
    for Id in equipment_imputed.index :
        
        fault_diag.loc[fault_diag['EquipmentID']== Id, column]= equipment_imputed.loc[Id].flatten()
262/1:
from pycaret.classification import *
from pycaret.regression import *
from pycaret.time_series import *
265/1:
#Libraries need for work

import pandas as pd
from datetime import datetime



from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

#we need to create a conda environment for pycaret and then pip install the packages after activating and rerun the notebook.
from pycaret.classification import *
from pycaret.regression import *
from pycaret.time_series import *

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
266/1:
#Libraries need for work

import pandas as pd
from datetime import datetime



from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

#we need to create a conda environment for pycaret and then pip install the packages after activating and rerun the notebook.
from pycaret.classification import *
from pycaret.regression import *
from pycaret.time_series import *

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
266/2:
#Reading the data files

faults = pd.read_csv("../data/J1939Faults.csv",low_memory=False,
                     parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
    #initial reading we saw it has date time object so we will keep them as date time. 
    #Also we saw the waning for memmory because of which we were loosing observations, so we will have low_memmory =False

diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
266/3: conda install openpyxl
266/4:
#Reading the data files

faults = pd.read_csv("../data/J1939Faults.csv",low_memory=False,
                     parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
    #initial reading we saw it has date time object so we will keep them as date time. 
    #Also we saw the waning for memmory because of which we were loosing observations, so we will have low_memmory =False

diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
   1:
#Libraries need for work

import pandas as pd
from datetime import datetime



from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

#we need to create a conda environment for pycaret and then pip install the packages after activating and rerun the notebook.
from pycaret.classification import *
from pycaret.regression import *
from pycaret.time_series import *

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
   2: conda install openpyxl
   3:
#Reading the data files

faults = pd.read_csv("../data/J1939Faults.csv",low_memory=False,
                     parse_dates=['EventTimeStamp', 'LocationTimeStamp'])
    #initial reading we saw it has date time object so we will keep them as date time. 
    #Also we saw the waning for memmory because of which we were loosing observations, so we will have low_memmory =False

diagnostic = pd.read_csv("../data/VehicleDiagnosticOnboardData.csv")
service = pd.read_excel("../data/Service Fault Codes_1_0_0_167.xlsx")
   4:
faults.shape #(1187335, 20)
diagnostic.shape #(12821626, 4)
service.shape #service.shape #
   5: faults.head(2)
   6: faults.info()
   7:
faults.describe()
faults.isna().sum()
   8:
faults.columns
faults['EventTimeStamp'].value_counts()
(faults['EventTimeStamp']
 .dt.year
 .value_counts()
 .sort_values(ascending = False)
 .plot(kind="bar", 
       title = 'Observations_per_year',
       xlabel='', ylabel='Number of Observations'))
#date time has year dt.year extracts year from it
   9:
(faults['LocationTimeStamp']
 .dt.year
 .value_counts()
 .sort_values(ascending = False)
 .plot(kind="bar", 
       title = 'Observations_per_year',
       xlabel='',
       ylabel='Number of Observations'))
#date time has year dt.year extracts year from it
  10:
faults['EventTimeStamp'].dt.year.unique()
faults['EventTimeStamp'].dt.year.value_counts()
faults['LocationTimeStamp'].dt.year.value_counts()
  11:
faults['EquipmentID'].nunique() #1122
(faults['EquipmentID'].value_counts().sort_values()
 .plot(kind="bar", 
       title = 'Equipment_Id',
       xlabel= (''),
       ylabel='Number of Observations'))
  12:
#the EuipmentId is a string character and has length varrying from character lnrth 3 4, 5, and 9 and 10
faults['EquipmentID'].str.len().nunique() #5
faults['EquipmentID'].str.len().value_counts().sort_values(ascending=False).to_frame()
  13: faults['active'].value_counts()
  14:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
faults[(faults['spn'] == 5246)].shape #1195 observations

faults.groupby('EquipmentID')[(faults['spn'] == 1569)]
  15:
faults['spn'].nunique() #450
# faults['fmi'].nunique() #26
# faults[(faults['spn'] == 1569)].shape #10927 observations
# faults[(faults['spn'] == 5246)].shape #1195 observations

# faults.groupby('EquipmentID')[(faults['spn'] == 1569)]
  16:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
# faults[(faults['spn'] == 1569)].shape #10927 observations
# faults[(faults['spn'] == 5246)].shape #1195 observations

# faults.groupby('EquipmentID')[(faults['spn'] == 1569)]
  17:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
# faults[(faults['spn'] == 1569)].shape #10927 observations
# faults[(faults['spn'] == 5246)].shape #1195 observations

# faults.groupby('EquipmentID')[(faults['spn'] == 1569)]
  18:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations
# faults[(faults['spn'] == 5246)].shape #1195 observations

# faults.groupby('EquipmentID')[(faults['spn'] == 1569)]
  19:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations o
faults[(faults['spn'] == 5246)].shape #1195 observations

# faults.groupby('EquipmentID')[(faults['spn'] == 1569)]
  20:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations o
faults[(faults['spn'] == 5246)].shape #1195 observations

faults.groupby('EquipmentID')[(faults['spn'] == 1569)]
  21:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations o
faults[(faults['spn'] == 5246)].shape #1195 observations

faults['EquipmentID'].value_counts()
  22:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations o
faults[(faults['spn'] == 5246)].shape #1195 observations

faults['EquipmentID'].value_counts()
faults.groupby('EquipmentID')[(faults['spn']==1569)].value_counts()
  23:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations o
faults[(faults['spn'] == 5246)].shape #1195 observations

faults['EquipmentID'].value_counts()
faults.groupby('EquipmentID')[(faults['spn']==1569)].sum()
  24:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations o
faults[(faults['spn'] == 5246)].shape #1195 observations

faults['EquipmentID'].value_counts()
# faults.groupby('EquipmentID')[(faults['spn']==1569)].sum()
faults['EquipmentID'].nunique()
  25:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations o
faults[(faults['spn'] == 5246)].shape #1195 observations

faults['EquipmentID'].value_counts()
# faults.groupby('EquipmentID')[(faults['spn']==1569)].sum()
faults['EquipmentID'].nunique() #1122
faults.groupby('EquipmentID')['spn']==1569)].sum()
  26:
faults['spn'].nunique() #450
faults['fmi'].nunique() #26
faults[(faults['spn'] == 1569)].shape #10927 observations o
faults[(faults['spn'] == 5246)].shape #1195 observations

faults['EquipmentID'].value_counts()
# faults.groupby('EquipmentID')[(faults['spn']==1569)].sum()
faults['EquipmentID'].nunique() #1122
faults.groupby('EquipmentID')['spn'].sum()
  27: %history -g -f history.txt
